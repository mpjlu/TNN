D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] init tnn success! 
TNNTorchNetwork::Init
PengNodeKind:prim::Constant Kind:14
PengNodeKind:prim::Constant Kind:14
PengNodeKind:prim::Constant Kind:14
PengNodeKind:prim::Constant Kind:14
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::Constant Kind:14
PengNodeKind:prim::Constant Kind:14
PengNodeKind:prim::Constant Kind:14
PengNodeKind:prim::Constant Kind:14
PengNodeKind:prim::Constant Kind:14
PengNodeKind:prim::Constant Kind:14
PengNodeKind:prim::Constant Kind:14
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::GetAttr Kind:140
PengNodeKind:prim::Constant Kind:14
PengNodeKind:prim::Constant Kind:14
PengNodeKind:prim::Constant Kind:14
PengNodeKind:prim::Constant Kind:14
PengMode QAT
Graph after ConstantPropagation
graph(%self.1 : __torch__.torch.fx.graph_module.GraphModule,
      %x.1 : Tensor):
  %1419 : bool = prim::Constant[value=0]()
  %1453 : int[] = prim::Constant[value=[1, 1]]()
  %1452 : int[] = prim::Constant[value=[3, 3]]()
  %944 : int = prim::Constant[value=1]()
  %3 : __torch__.torch.nn.modules.module.___torch_mangle_340.Module = prim::GetAttr[name="reg"](%self.1)
  %4 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_339.QuantConv2d = prim::GetAttr[name="2"](%3)
  %6 : __torch__.torch.nn.modules.module.___torch_mangle_340.Module = prim::GetAttr[name="reg"](%self.1)
  %7 : __torch__.torch.nn.modules.activation.___torch_mangle_336.ReLU = prim::GetAttr[name="1"](%6)
  %9 : __torch__.torch.nn.modules.module.___torch_mangle_340.Module = prim::GetAttr[name="reg"](%self.1)
  %10 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_335.QuantConv2d = prim::GetAttr[name="0"](%9)
  %12 : __torch__.torch.nn.modules.module.___torch_mangle_332.Module = prim::GetAttr[name="wh"](%self.1)
  %13 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_331.QuantConv2d = prim::GetAttr[name="2"](%12)
  %15 : __torch__.torch.nn.modules.module.___torch_mangle_332.Module = prim::GetAttr[name="wh"](%self.1)
  %16 : __torch__.torch.nn.modules.activation.___torch_mangle_328.ReLU = prim::GetAttr[name="1"](%15)
  %18 : __torch__.torch.nn.modules.module.___torch_mangle_332.Module = prim::GetAttr[name="wh"](%self.1)
  %19 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_327.QuantConv2d = prim::GetAttr[name="0"](%18)
  %21 : __torch__.torch.nn.modules.module.___torch_mangle_324.Module = prim::GetAttr[name="hm"](%self.1)
  %22 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_323.QuantConv2d = prim::GetAttr[name="2"](%21)
  %24 : __torch__.torch.nn.modules.module.___torch_mangle_324.Module = prim::GetAttr[name="hm"](%self.1)
  %25 : __torch__.torch.nn.modules.activation.___torch_mangle_320.ReLU = prim::GetAttr[name="1"](%24)
  %27 : __torch__.torch.nn.modules.module.___torch_mangle_324.Module = prim::GetAttr[name="hm"](%self.1)
  %28 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_319.QuantConv2d = prim::GetAttr[name="0"](%27)
  %30 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %31 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%30)
  %32 : __torch__.torch.nn.modules.module.___torch_mangle_314.Module = prim::GetAttr[name="node_3"](%31)
  %33 : __torch__.torch.nn.modules.activation.___torch_mangle_313.ReLU = prim::GetAttr[name="2"](%32)
  %35 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %36 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%35)
  %37 : __torch__.torch.nn.modules.module.___torch_mangle_314.Module = prim::GetAttr[name="node_3"](%36)
  %38 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_312.BatchNorm2d = prim::GetAttr[name="1"](%37)
  %40 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %41 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%40)
  %42 : __torch__.torch.nn.modules.module.___torch_mangle_314.Module = prim::GetAttr[name="node_3"](%41)
  %43 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_311.QuantConv2d = prim::GetAttr[name="0"](%42)
  %45 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %46 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%45)
  %47 : __torch__.torch.nn.modules.module.___torch_mangle_308.Module = prim::GetAttr[name="node_2"](%46)
  %48 : __torch__.torch.nn.modules.activation.___torch_mangle_307.ReLU = prim::GetAttr[name="2"](%47)
  %50 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %51 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%50)
  %52 : __torch__.torch.nn.modules.module.___torch_mangle_308.Module = prim::GetAttr[name="node_2"](%51)
  %53 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_306.BatchNorm2d = prim::GetAttr[name="1"](%52)
  %55 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %56 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%55)
  %57 : __torch__.torch.nn.modules.module.___torch_mangle_308.Module = prim::GetAttr[name="node_2"](%56)
  %58 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_305.QuantConv2d = prim::GetAttr[name="0"](%57)
  %60 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %61 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%60)
  %62 : __torch__.torch.nn.modules.module.___torch_mangle_302.Module = prim::GetAttr[name="node_1"](%61)
  %63 : __torch__.torch.nn.modules.activation.___torch_mangle_301.ReLU = prim::GetAttr[name="2"](%62)
  %65 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %66 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%65)
  %67 : __torch__.torch.nn.modules.module.___torch_mangle_302.Module = prim::GetAttr[name="node_1"](%66)
  %68 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_300.BatchNorm2d = prim::GetAttr[name="1"](%67)
  %70 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %71 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%70)
  %72 : __torch__.torch.nn.modules.module.___torch_mangle_302.Module = prim::GetAttr[name="node_1"](%71)
  %73 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_299.QuantConv2d = prim::GetAttr[name="0"](%72)
  %75 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %76 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%75)
  %77 : __torch__.torch.nn.modules.conv.___torch_mangle_296.ConvTranspose2d = prim::GetAttr[name="up_3"](%76)
  %79 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %80 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%79)
  %81 : __torch__.torch.nn.modules.module.___torch_mangle_295.Module = prim::GetAttr[name="proj_3"](%80)
  %82 : __torch__.torch.nn.modules.activation.___torch_mangle_294.ReLU = prim::GetAttr[name="2"](%81)
  %84 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %85 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%84)
  %86 : __torch__.torch.nn.modules.module.___torch_mangle_295.Module = prim::GetAttr[name="proj_3"](%85)
  %87 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_293.BatchNorm2d = prim::GetAttr[name="1"](%86)
  %89 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %90 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%89)
  %91 : __torch__.torch.nn.modules.module.___torch_mangle_295.Module = prim::GetAttr[name="proj_3"](%90)
  %92 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_292.QuantConv2d = prim::GetAttr[name="0"](%91)
  %94 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %95 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%94)
  %96 : __torch__.torch.nn.modules.conv.___torch_mangle_289.ConvTranspose2d = prim::GetAttr[name="up_2"](%95)
  %98 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %99 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%98)
  %100 : __torch__.torch.nn.modules.module.___torch_mangle_288.Module = prim::GetAttr[name="proj_2"](%99)
  %101 : __torch__.torch.nn.modules.activation.___torch_mangle_287.ReLU = prim::GetAttr[name="2"](%100)
  %103 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %104 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%103)
  %105 : __torch__.torch.nn.modules.module.___torch_mangle_288.Module = prim::GetAttr[name="proj_2"](%104)
  %106 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_286.BatchNorm2d = prim::GetAttr[name="1"](%105)
  %108 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %109 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%108)
  %110 : __torch__.torch.nn.modules.module.___torch_mangle_288.Module = prim::GetAttr[name="proj_2"](%109)
  %111 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_285.QuantConv2d = prim::GetAttr[name="0"](%110)
  %113 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %114 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%113)
  %115 : __torch__.torch.nn.modules.conv.___torch_mangle_282.ConvTranspose2d = prim::GetAttr[name="up_1"](%114)
  %117 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %118 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%117)
  %119 : __torch__.torch.nn.modules.module.___torch_mangle_281.Module = prim::GetAttr[name="proj_1"](%118)
  %120 : __torch__.torch.nn.modules.activation.___torch_mangle_280.ReLU = prim::GetAttr[name="2"](%119)
  %122 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %123 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%122)
  %124 : __torch__.torch.nn.modules.module.___torch_mangle_281.Module = prim::GetAttr[name="proj_1"](%123)
  %125 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_279.BatchNorm2d = prim::GetAttr[name="1"](%124)
  %127 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %128 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%127)
  %129 : __torch__.torch.nn.modules.module.___torch_mangle_281.Module = prim::GetAttr[name="proj_1"](%128)
  %130 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_278.QuantConv2d = prim::GetAttr[name="0"](%129)
  %132 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %133 : __torch__.torch.nn.modules.module.___torch_mangle_275.Module = prim::GetAttr[name="ida_1"](%132)
  %134 : __torch__.torch.nn.modules.module.___torch_mangle_274.Module = prim::GetAttr[name="node_2"](%133)
  %135 : __torch__.torch.nn.modules.activation.___torch_mangle_273.ReLU = prim::GetAttr[name="2"](%134)
  %137 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %138 : __torch__.torch.nn.modules.module.___torch_mangle_275.Module = prim::GetAttr[name="ida_1"](%137)
  %139 : __torch__.torch.nn.modules.module.___torch_mangle_274.Module = prim::GetAttr[name="node_2"](%138)
  %140 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_272.BatchNorm2d = prim::GetAttr[name="1"](%139)
  %142 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %143 : __torch__.torch.nn.modules.module.___torch_mangle_275.Module = prim::GetAttr[name="ida_1"](%142)
  %144 : __torch__.torch.nn.modules.module.___torch_mangle_274.Module = prim::GetAttr[name="node_2"](%143)
  %145 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_271.QuantConv2d = prim::GetAttr[name="0"](%144)
  %147 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %148 : __torch__.torch.nn.modules.module.___torch_mangle_275.Module = prim::GetAttr[name="ida_1"](%147)
  %149 : __torch__.torch.nn.modules.module.___torch_mangle_268.Module = prim::GetAttr[name="node_1"](%148)
  %150 : __torch__.torch.nn.modules.activation.___torch_mangle_267.ReLU = prim::GetAttr[name="2"](%149)
  %152 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %153 : __torch__.torch.nn.modules.module.___torch_mangle_275.Module = prim::GetAttr[name="ida_1"](%152)
  %154 : __torch__.torch.nn.modules.module.___torch_mangle_268.Module = prim::GetAttr[name="node_1"](%153)
  %155 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_266.BatchNorm2d = prim::GetAttr[name="1"](%154)
  %157 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %158 : __torch__.torch.nn.modules.module.___torch_mangle_275.Module = prim::GetAttr[name="ida_1"](%157)
  %159 : __torch__.torch.nn.modules.module.___torch_mangle_268.Module = prim::GetAttr[name="node_1"](%158)
  %160 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_265.QuantConv2d = prim::GetAttr[name="0"](%159)
  %162 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %163 : __torch__.torch.nn.modules.module.___torch_mangle_275.Module = prim::GetAttr[name="ida_1"](%162)
  %164 : __torch__.torch.nn.modules.conv.___torch_mangle_262.ConvTranspose2d = prim::GetAttr[name="up_2"](%163)
  %166 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %167 : __torch__.torch.nn.modules.module.___torch_mangle_275.Module = prim::GetAttr[name="ida_1"](%166)
  %168 : __torch__.torch.nn.modules.module.___torch_mangle_261.Module = prim::GetAttr[name="proj_2"](%167)
  %169 : __torch__.torch.nn.modules.activation.___torch_mangle_260.ReLU = prim::GetAttr[name="2"](%168)
  %171 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %172 : __torch__.torch.nn.modules.module.___torch_mangle_275.Module = prim::GetAttr[name="ida_1"](%171)
  %173 : __torch__.torch.nn.modules.module.___torch_mangle_261.Module = prim::GetAttr[name="proj_2"](%172)
  %174 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_259.BatchNorm2d = prim::GetAttr[name="1"](%173)
  %176 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %177 : __torch__.torch.nn.modules.module.___torch_mangle_275.Module = prim::GetAttr[name="ida_1"](%176)
  %178 : __torch__.torch.nn.modules.module.___torch_mangle_261.Module = prim::GetAttr[name="proj_2"](%177)
  %179 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_258.QuantConv2d = prim::GetAttr[name="0"](%178)
  %181 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %182 : __torch__.torch.nn.modules.module.___torch_mangle_275.Module = prim::GetAttr[name="ida_1"](%181)
  %183 : __torch__.torch.nn.modules.conv.___torch_mangle_255.ConvTranspose2d = prim::GetAttr[name="up_1"](%182)
  %185 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %186 : __torch__.torch.nn.modules.module.___torch_mangle_275.Module = prim::GetAttr[name="ida_1"](%185)
  %187 : __torch__.torch.nn.modules.module.___torch_mangle_254.Module = prim::GetAttr[name="proj_1"](%186)
  %188 : __torch__.torch.nn.modules.activation.___torch_mangle_253.ReLU = prim::GetAttr[name="2"](%187)
  %190 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %191 : __torch__.torch.nn.modules.module.___torch_mangle_275.Module = prim::GetAttr[name="ida_1"](%190)
  %192 : __torch__.torch.nn.modules.module.___torch_mangle_254.Module = prim::GetAttr[name="proj_1"](%191)
  %193 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_252.BatchNorm2d = prim::GetAttr[name="1"](%192)
  %195 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %196 : __torch__.torch.nn.modules.module.___torch_mangle_275.Module = prim::GetAttr[name="ida_1"](%195)
  %197 : __torch__.torch.nn.modules.module.___torch_mangle_254.Module = prim::GetAttr[name="proj_1"](%196)
  %198 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_251.QuantConv2d = prim::GetAttr[name="0"](%197)
  %200 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %201 : __torch__.torch.nn.modules.module.___torch_mangle_248.Module = prim::GetAttr[name="ida_0"](%200)
  %202 : __torch__.torch.nn.modules.module.___torch_mangle_247.Module = prim::GetAttr[name="node_1"](%201)
  %203 : __torch__.torch.nn.modules.activation.___torch_mangle_246.ReLU = prim::GetAttr[name="2"](%202)
  %205 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %206 : __torch__.torch.nn.modules.module.___torch_mangle_248.Module = prim::GetAttr[name="ida_0"](%205)
  %207 : __torch__.torch.nn.modules.module.___torch_mangle_247.Module = prim::GetAttr[name="node_1"](%206)
  %208 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_245.BatchNorm2d = prim::GetAttr[name="1"](%207)
  %210 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %211 : __torch__.torch.nn.modules.module.___torch_mangle_248.Module = prim::GetAttr[name="ida_0"](%210)
  %212 : __torch__.torch.nn.modules.module.___torch_mangle_247.Module = prim::GetAttr[name="node_1"](%211)
  %213 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_244.QuantConv2d = prim::GetAttr[name="0"](%212)
  %215 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %216 : __torch__.torch.nn.modules.module.___torch_mangle_248.Module = prim::GetAttr[name="ida_0"](%215)
  %217 : __torch__.torch.nn.modules.conv.ConvTranspose2d = prim::GetAttr[name="up_1"](%216)
  %219 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %220 : __torch__.torch.nn.modules.module.___torch_mangle_248.Module = prim::GetAttr[name="ida_0"](%219)
  %221 : __torch__.torch.nn.modules.module.___torch_mangle_241.Module = prim::GetAttr[name="proj_1"](%220)
  %222 : __torch__.torch.nn.modules.activation.___torch_mangle_240.ReLU = prim::GetAttr[name="2"](%221)
  %224 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %225 : __torch__.torch.nn.modules.module.___torch_mangle_248.Module = prim::GetAttr[name="ida_0"](%224)
  %226 : __torch__.torch.nn.modules.module.___torch_mangle_241.Module = prim::GetAttr[name="proj_1"](%225)
  %227 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_239.BatchNorm2d = prim::GetAttr[name="1"](%226)
  %229 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %230 : __torch__.torch.nn.modules.module.___torch_mangle_248.Module = prim::GetAttr[name="ida_0"](%229)
  %231 : __torch__.torch.nn.modules.module.___torch_mangle_241.Module = prim::GetAttr[name="proj_1"](%230)
  %232 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_238.QuantConv2d = prim::GetAttr[name="0"](%231)
  %234 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %235 : __torch__.torch.nn.modules.module.___torch_mangle_234.Module = prim::GetAttr[name="level5"](%234)
  %236 : __torch__.torch.nn.modules.module.___torch_mangle_233.Module = prim::GetAttr[name="root"](%235)
  %237 : __torch__.torch.nn.modules.activation.___torch_mangle_232.ReLU = prim::GetAttr[name="relu"](%236)
  %239 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %240 : __torch__.torch.nn.modules.module.___torch_mangle_234.Module = prim::GetAttr[name="level5"](%239)
  %241 : __torch__.torch.nn.modules.module.___torch_mangle_233.Module = prim::GetAttr[name="root"](%240)
  %242 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_231.BatchNorm2d = prim::GetAttr[name="bn"](%241)
  %244 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %245 : __torch__.torch.nn.modules.module.___torch_mangle_234.Module = prim::GetAttr[name="level5"](%244)
  %246 : __torch__.torch.nn.modules.module.___torch_mangle_233.Module = prim::GetAttr[name="root"](%245)
  %247 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_230.QuantConv2d = prim::GetAttr[name="conv"](%246)
  %249 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %250 : __torch__.torch.nn.modules.module.___torch_mangle_234.Module = prim::GetAttr[name="level5"](%249)
  %251 : __torch__.torch.nn.modules.module.___torch_mangle_227.Module = prim::GetAttr[name="tree2"](%250)
  %252 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_226.BatchNorm2d = prim::GetAttr[name="bn2"](%251)
  %254 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %255 : __torch__.torch.nn.modules.module.___torch_mangle_234.Module = prim::GetAttr[name="level5"](%254)
  %256 : __torch__.torch.nn.modules.module.___torch_mangle_227.Module = prim::GetAttr[name="tree2"](%255)
  %257 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_225.QuantConv2d = prim::GetAttr[name="conv2"](%256)
  %259 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %260 : __torch__.torch.nn.modules.module.___torch_mangle_234.Module = prim::GetAttr[name="level5"](%259)
  %261 : __torch__.torch.nn.modules.module.___torch_mangle_227.Module = prim::GetAttr[name="tree2"](%260)
  %262 : __torch__.torch.nn.modules.activation.___torch_mangle_222.ReLU = prim::GetAttr[name="relu"](%261)
  %264 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %265 : __torch__.torch.nn.modules.module.___torch_mangle_234.Module = prim::GetAttr[name="level5"](%264)
  %266 : __torch__.torch.nn.modules.module.___torch_mangle_227.Module = prim::GetAttr[name="tree2"](%265)
  %267 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_221.BatchNorm2d = prim::GetAttr[name="bn1"](%266)
  %269 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %270 : __torch__.torch.nn.modules.module.___torch_mangle_234.Module = prim::GetAttr[name="level5"](%269)
  %271 : __torch__.torch.nn.modules.module.___torch_mangle_227.Module = prim::GetAttr[name="tree2"](%270)
  %272 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_220.QuantConv2d = prim::GetAttr[name="conv1"](%271)
  %274 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %275 : __torch__.torch.nn.modules.module.___torch_mangle_234.Module = prim::GetAttr[name="level5"](%274)
  %276 : __torch__.torch.nn.modules.module.___torch_mangle_217.Module = prim::GetAttr[name="tree1"](%275)
  %277 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_identify.___torch_mangle_216.QuantIdentity = prim::GetAttr[name="relu_output_quant"](%276)
  %279 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %280 : __torch__.torch.nn.modules.module.___torch_mangle_234.Module = prim::GetAttr[name="level5"](%279)
  %281 : __torch__.torch.nn.modules.module.___torch_mangle_217.Module = prim::GetAttr[name="tree1"](%280)
  %282 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_214.BatchNorm2d = prim::GetAttr[name="bn2"](%281)
  %284 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %285 : __torch__.torch.nn.modules.module.___torch_mangle_234.Module = prim::GetAttr[name="level5"](%284)
  %286 : __torch__.torch.nn.modules.module.___torch_mangle_217.Module = prim::GetAttr[name="tree1"](%285)
  %287 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_213.QuantConv2d = prim::GetAttr[name="conv2"](%286)
  %289 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %290 : __torch__.torch.nn.modules.module.___torch_mangle_234.Module = prim::GetAttr[name="level5"](%289)
  %291 : __torch__.torch.nn.modules.module.___torch_mangle_217.Module = prim::GetAttr[name="tree1"](%290)
  %292 : __torch__.torch.nn.modules.activation.___torch_mangle_210.ReLU = prim::GetAttr[name="relu"](%291)
  %294 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %295 : __torch__.torch.nn.modules.module.___torch_mangle_234.Module = prim::GetAttr[name="level5"](%294)
  %296 : __torch__.torch.nn.modules.module.___torch_mangle_217.Module = prim::GetAttr[name="tree1"](%295)
  %297 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_209.BatchNorm2d = prim::GetAttr[name="bn1"](%296)
  %299 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %300 : __torch__.torch.nn.modules.module.___torch_mangle_234.Module = prim::GetAttr[name="level5"](%299)
  %301 : __torch__.torch.nn.modules.module.___torch_mangle_217.Module = prim::GetAttr[name="tree1"](%300)
  %302 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_208.QuantConv2d = prim::GetAttr[name="conv1"](%301)
  %304 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %305 : __torch__.torch.nn.modules.module.___torch_mangle_234.Module = prim::GetAttr[name="level5"](%304)
  %306 : __torch__.torch.nn.modules.module.___torch_mangle_205.Module = prim::GetAttr[name="project"](%305)
  %307 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_identify.___torch_mangle_204.QuantIdentity = prim::GetAttr[name="1_output_quant"](%306)
  %309 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %310 : __torch__.torch.nn.modules.module.___torch_mangle_234.Module = prim::GetAttr[name="level5"](%309)
  %311 : __torch__.torch.nn.modules.module.___torch_mangle_205.Module = prim::GetAttr[name="project"](%310)
  %312 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_202.BatchNorm2d = prim::GetAttr[name="1"](%311)
  %314 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %315 : __torch__.torch.nn.modules.module.___torch_mangle_234.Module = prim::GetAttr[name="level5"](%314)
  %316 : __torch__.torch.nn.modules.module.___torch_mangle_205.Module = prim::GetAttr[name="project"](%315)
  %317 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_201.QuantConv2d = prim::GetAttr[name="0"](%316)
  %319 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %320 : __torch__.torch.nn.modules.module.___torch_mangle_234.Module = prim::GetAttr[name="level5"](%319)
  %321 : __torch__.torch.nn.modules.pooling.___torch_mangle_198.MaxPool2d = prim::GetAttr[name="downsample"](%320)
  %323 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %324 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%323)
  %325 : __torch__.torch.nn.modules.module.___torch_mangle_196.Module = prim::GetAttr[name="tree2"](%324)
  %326 : __torch__.torch.nn.modules.module.___torch_mangle_195.Module = prim::GetAttr[name="root"](%325)
  %327 : __torch__.torch.nn.modules.activation.___torch_mangle_194.ReLU = prim::GetAttr[name="relu"](%326)
  %329 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %330 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%329)
  %331 : __torch__.torch.nn.modules.module.___torch_mangle_196.Module = prim::GetAttr[name="tree2"](%330)
  %332 : __torch__.torch.nn.modules.module.___torch_mangle_195.Module = prim::GetAttr[name="root"](%331)
  %333 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_193.BatchNorm2d = prim::GetAttr[name="bn"](%332)
  %335 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %336 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%335)
  %337 : __torch__.torch.nn.modules.module.___torch_mangle_196.Module = prim::GetAttr[name="tree2"](%336)
  %338 : __torch__.torch.nn.modules.module.___torch_mangle_195.Module = prim::GetAttr[name="root"](%337)
  %339 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_192.QuantConv2d = prim::GetAttr[name="conv"](%338)
  %341 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %342 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%341)
  %343 : __torch__.torch.nn.modules.module.___torch_mangle_196.Module = prim::GetAttr[name="tree2"](%342)
  %344 : __torch__.torch.nn.modules.module.___torch_mangle_189.Module = prim::GetAttr[name="tree2"](%343)
  %345 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_188.BatchNorm2d = prim::GetAttr[name="bn2"](%344)
  %347 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %348 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%347)
  %349 : __torch__.torch.nn.modules.module.___torch_mangle_196.Module = prim::GetAttr[name="tree2"](%348)
  %350 : __torch__.torch.nn.modules.module.___torch_mangle_189.Module = prim::GetAttr[name="tree2"](%349)
  %351 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_187.QuantConv2d = prim::GetAttr[name="conv2"](%350)
  %353 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %354 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%353)
  %355 : __torch__.torch.nn.modules.module.___torch_mangle_196.Module = prim::GetAttr[name="tree2"](%354)
  %356 : __torch__.torch.nn.modules.module.___torch_mangle_189.Module = prim::GetAttr[name="tree2"](%355)
  %357 : __torch__.torch.nn.modules.activation.___torch_mangle_184.ReLU = prim::GetAttr[name="relu"](%356)
  %359 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %360 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%359)
  %361 : __torch__.torch.nn.modules.module.___torch_mangle_196.Module = prim::GetAttr[name="tree2"](%360)
  %362 : __torch__.torch.nn.modules.module.___torch_mangle_189.Module = prim::GetAttr[name="tree2"](%361)
  %363 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_183.BatchNorm2d = prim::GetAttr[name="bn1"](%362)
  %365 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %366 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%365)
  %367 : __torch__.torch.nn.modules.module.___torch_mangle_196.Module = prim::GetAttr[name="tree2"](%366)
  %368 : __torch__.torch.nn.modules.module.___torch_mangle_189.Module = prim::GetAttr[name="tree2"](%367)
  %369 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_182.QuantConv2d = prim::GetAttr[name="conv1"](%368)
  %371 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %372 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%371)
  %373 : __torch__.torch.nn.modules.module.___torch_mangle_196.Module = prim::GetAttr[name="tree2"](%372)
  %374 : __torch__.torch.nn.modules.module.___torch_mangle_179.Module = prim::GetAttr[name="tree1"](%373)
  %375 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_identify.___torch_mangle_178.QuantIdentity = prim::GetAttr[name="relu_output_quant"](%374)
  %377 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %378 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%377)
  %379 : __torch__.torch.nn.modules.module.___torch_mangle_196.Module = prim::GetAttr[name="tree2"](%378)
  %380 : __torch__.torch.nn.modules.module.___torch_mangle_179.Module = prim::GetAttr[name="tree1"](%379)
  %381 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_176.BatchNorm2d = prim::GetAttr[name="bn2"](%380)
  %383 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %384 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%383)
  %385 : __torch__.torch.nn.modules.module.___torch_mangle_196.Module = prim::GetAttr[name="tree2"](%384)
  %386 : __torch__.torch.nn.modules.module.___torch_mangle_179.Module = prim::GetAttr[name="tree1"](%385)
  %387 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_175.QuantConv2d = prim::GetAttr[name="conv2"](%386)
  %389 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %390 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%389)
  %391 : __torch__.torch.nn.modules.module.___torch_mangle_196.Module = prim::GetAttr[name="tree2"](%390)
  %392 : __torch__.torch.nn.modules.module.___torch_mangle_179.Module = prim::GetAttr[name="tree1"](%391)
  %393 : __torch__.torch.nn.modules.activation.___torch_mangle_172.ReLU = prim::GetAttr[name="relu"](%392)
  %395 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %396 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%395)
  %397 : __torch__.torch.nn.modules.module.___torch_mangle_196.Module = prim::GetAttr[name="tree2"](%396)
  %398 : __torch__.torch.nn.modules.module.___torch_mangle_179.Module = prim::GetAttr[name="tree1"](%397)
  %399 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_171.BatchNorm2d = prim::GetAttr[name="bn1"](%398)
  %401 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %402 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%401)
  %403 : __torch__.torch.nn.modules.module.___torch_mangle_196.Module = prim::GetAttr[name="tree2"](%402)
  %404 : __torch__.torch.nn.modules.module.___torch_mangle_179.Module = prim::GetAttr[name="tree1"](%403)
  %405 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_170.QuantConv2d = prim::GetAttr[name="conv1"](%404)
  %407 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %408 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%407)
  %409 : __torch__.torch.nn.modules.module.___torch_mangle_167.Module = prim::GetAttr[name="tree1"](%408)
  %410 : __torch__.torch.nn.modules.module.___torch_mangle_166.Module = prim::GetAttr[name="root"](%409)
  %411 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_identify.___torch_mangle_165.QuantIdentity = prim::GetAttr[name="relu_output_quant"](%410)
  %413 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %414 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%413)
  %415 : __torch__.torch.nn.modules.module.___torch_mangle_167.Module = prim::GetAttr[name="tree1"](%414)
  %416 : __torch__.torch.nn.modules.module.___torch_mangle_166.Module = prim::GetAttr[name="root"](%415)
  %417 : __torch__.torch.nn.modules.activation.___torch_mangle_163.ReLU = prim::GetAttr[name="relu"](%416)
  %419 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %420 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%419)
  %421 : __torch__.torch.nn.modules.module.___torch_mangle_167.Module = prim::GetAttr[name="tree1"](%420)
  %422 : __torch__.torch.nn.modules.module.___torch_mangle_166.Module = prim::GetAttr[name="root"](%421)
  %423 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_162.BatchNorm2d = prim::GetAttr[name="bn"](%422)
  %425 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %426 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%425)
  %427 : __torch__.torch.nn.modules.module.___torch_mangle_167.Module = prim::GetAttr[name="tree1"](%426)
  %428 : __torch__.torch.nn.modules.module.___torch_mangle_166.Module = prim::GetAttr[name="root"](%427)
  %429 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_161.QuantConv2d = prim::GetAttr[name="conv"](%428)
  %431 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %432 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%431)
  %433 : __torch__.torch.nn.modules.module.___torch_mangle_167.Module = prim::GetAttr[name="tree1"](%432)
  %434 : __torch__.torch.nn.modules.module.___torch_mangle_158.Module = prim::GetAttr[name="tree2"](%433)
  %435 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_157.BatchNorm2d = prim::GetAttr[name="bn2"](%434)
  %437 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %438 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%437)
  %439 : __torch__.torch.nn.modules.module.___torch_mangle_167.Module = prim::GetAttr[name="tree1"](%438)
  %440 : __torch__.torch.nn.modules.module.___torch_mangle_158.Module = prim::GetAttr[name="tree2"](%439)
  %441 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_156.QuantConv2d = prim::GetAttr[name="conv2"](%440)
  %443 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %444 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%443)
  %445 : __torch__.torch.nn.modules.module.___torch_mangle_167.Module = prim::GetAttr[name="tree1"](%444)
  %446 : __torch__.torch.nn.modules.module.___torch_mangle_158.Module = prim::GetAttr[name="tree2"](%445)
  %447 : __torch__.torch.nn.modules.activation.___torch_mangle_153.ReLU = prim::GetAttr[name="relu"](%446)
  %449 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %450 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%449)
  %451 : __torch__.torch.nn.modules.module.___torch_mangle_167.Module = prim::GetAttr[name="tree1"](%450)
  %452 : __torch__.torch.nn.modules.module.___torch_mangle_158.Module = prim::GetAttr[name="tree2"](%451)
  %453 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_152.BatchNorm2d = prim::GetAttr[name="bn1"](%452)
  %455 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %456 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%455)
  %457 : __torch__.torch.nn.modules.module.___torch_mangle_167.Module = prim::GetAttr[name="tree1"](%456)
  %458 : __torch__.torch.nn.modules.module.___torch_mangle_158.Module = prim::GetAttr[name="tree2"](%457)
  %459 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_151.QuantConv2d = prim::GetAttr[name="conv1"](%458)
  %461 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %462 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%461)
  %463 : __torch__.torch.nn.modules.module.___torch_mangle_167.Module = prim::GetAttr[name="tree1"](%462)
  %464 : __torch__.torch.nn.modules.module.___torch_mangle_148.Module = prim::GetAttr[name="tree1"](%463)
  %465 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_identify.___torch_mangle_147.QuantIdentity = prim::GetAttr[name="relu_output_quant"](%464)
  %467 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %468 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%467)
  %469 : __torch__.torch.nn.modules.module.___torch_mangle_167.Module = prim::GetAttr[name="tree1"](%468)
  %470 : __torch__.torch.nn.modules.module.___torch_mangle_148.Module = prim::GetAttr[name="tree1"](%469)
  %471 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_145.BatchNorm2d = prim::GetAttr[name="bn2"](%470)
  %473 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %474 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%473)
  %475 : __torch__.torch.nn.modules.module.___torch_mangle_167.Module = prim::GetAttr[name="tree1"](%474)
  %476 : __torch__.torch.nn.modules.module.___torch_mangle_148.Module = prim::GetAttr[name="tree1"](%475)
  %477 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_144.QuantConv2d = prim::GetAttr[name="conv2"](%476)
  %479 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %480 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%479)
  %481 : __torch__.torch.nn.modules.module.___torch_mangle_167.Module = prim::GetAttr[name="tree1"](%480)
  %482 : __torch__.torch.nn.modules.module.___torch_mangle_148.Module = prim::GetAttr[name="tree1"](%481)
  %483 : __torch__.torch.nn.modules.activation.___torch_mangle_141.ReLU = prim::GetAttr[name="relu"](%482)
  %485 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %486 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%485)
  %487 : __torch__.torch.nn.modules.module.___torch_mangle_167.Module = prim::GetAttr[name="tree1"](%486)
  %488 : __torch__.torch.nn.modules.module.___torch_mangle_148.Module = prim::GetAttr[name="tree1"](%487)
  %489 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_140.BatchNorm2d = prim::GetAttr[name="bn1"](%488)
  %491 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %492 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%491)
  %493 : __torch__.torch.nn.modules.module.___torch_mangle_167.Module = prim::GetAttr[name="tree1"](%492)
  %494 : __torch__.torch.nn.modules.module.___torch_mangle_148.Module = prim::GetAttr[name="tree1"](%493)
  %495 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_139.QuantConv2d = prim::GetAttr[name="conv1"](%494)
  %497 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %498 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%497)
  %499 : __torch__.torch.nn.modules.module.___torch_mangle_167.Module = prim::GetAttr[name="tree1"](%498)
  %500 : __torch__.torch.nn.modules.module.___torch_mangle_136.Module = prim::GetAttr[name="project"](%499)
  %501 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_identify.___torch_mangle_135.QuantIdentity = prim::GetAttr[name="1_output_quant"](%500)
  %503 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %504 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%503)
  %505 : __torch__.torch.nn.modules.module.___torch_mangle_167.Module = prim::GetAttr[name="tree1"](%504)
  %506 : __torch__.torch.nn.modules.module.___torch_mangle_136.Module = prim::GetAttr[name="project"](%505)
  %507 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_133.BatchNorm2d = prim::GetAttr[name="1"](%506)
  %509 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %510 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%509)
  %511 : __torch__.torch.nn.modules.module.___torch_mangle_167.Module = prim::GetAttr[name="tree1"](%510)
  %512 : __torch__.torch.nn.modules.module.___torch_mangle_136.Module = prim::GetAttr[name="project"](%511)
  %513 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_132.QuantConv2d = prim::GetAttr[name="0"](%512)
  %515 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %516 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%515)
  %517 : __torch__.torch.nn.modules.module.___torch_mangle_167.Module = prim::GetAttr[name="tree1"](%516)
  %518 : __torch__.torch.nn.modules.pooling.___torch_mangle_129.MaxPool2d = prim::GetAttr[name="downsample"](%517)
  %520 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %521 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%520)
  %522 : __torch__.torch.nn.modules.module.___torch_mangle_128.Module = prim::GetAttr[name="project"](%521)
  %523 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_127.BatchNorm2d = prim::GetAttr[name="1"](%522)
  %525 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %526 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%525)
  %527 : __torch__.torch.nn.modules.module.___torch_mangle_128.Module = prim::GetAttr[name="project"](%526)
  %528 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_126.QuantConv2d = prim::GetAttr[name="0"](%527)
  %530 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %531 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%530)
  %532 : __torch__.torch.nn.modules.pooling.___torch_mangle_123.MaxPool2d = prim::GetAttr[name="downsample"](%531)
  %534 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %535 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%534)
  %536 : __torch__.torch.nn.modules.module.___torch_mangle_121.Module = prim::GetAttr[name="tree2"](%535)
  %537 : __torch__.torch.nn.modules.module.___torch_mangle_120.Module = prim::GetAttr[name="root"](%536)
  %538 : __torch__.torch.nn.modules.activation.___torch_mangle_119.ReLU = prim::GetAttr[name="relu"](%537)
  %540 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %541 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%540)
  %542 : __torch__.torch.nn.modules.module.___torch_mangle_121.Module = prim::GetAttr[name="tree2"](%541)
  %543 : __torch__.torch.nn.modules.module.___torch_mangle_120.Module = prim::GetAttr[name="root"](%542)
  %544 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_118.BatchNorm2d = prim::GetAttr[name="bn"](%543)
  %546 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %547 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%546)
  %548 : __torch__.torch.nn.modules.module.___torch_mangle_121.Module = prim::GetAttr[name="tree2"](%547)
  %549 : __torch__.torch.nn.modules.module.___torch_mangle_120.Module = prim::GetAttr[name="root"](%548)
  %550 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_117.QuantConv2d = prim::GetAttr[name="conv"](%549)
  %552 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %553 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%552)
  %554 : __torch__.torch.nn.modules.module.___torch_mangle_121.Module = prim::GetAttr[name="tree2"](%553)
  %555 : __torch__.torch.nn.modules.module.___torch_mangle_114.Module = prim::GetAttr[name="tree2"](%554)
  %556 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_113.BatchNorm2d = prim::GetAttr[name="bn2"](%555)
  %558 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %559 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%558)
  %560 : __torch__.torch.nn.modules.module.___torch_mangle_121.Module = prim::GetAttr[name="tree2"](%559)
  %561 : __torch__.torch.nn.modules.module.___torch_mangle_114.Module = prim::GetAttr[name="tree2"](%560)
  %562 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_112.QuantConv2d = prim::GetAttr[name="conv2"](%561)
  %564 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %565 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%564)
  %566 : __torch__.torch.nn.modules.module.___torch_mangle_121.Module = prim::GetAttr[name="tree2"](%565)
  %567 : __torch__.torch.nn.modules.module.___torch_mangle_114.Module = prim::GetAttr[name="tree2"](%566)
  %568 : __torch__.torch.nn.modules.activation.___torch_mangle_109.ReLU = prim::GetAttr[name="relu"](%567)
  %570 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %571 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%570)
  %572 : __torch__.torch.nn.modules.module.___torch_mangle_121.Module = prim::GetAttr[name="tree2"](%571)
  %573 : __torch__.torch.nn.modules.module.___torch_mangle_114.Module = prim::GetAttr[name="tree2"](%572)
  %574 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_108.BatchNorm2d = prim::GetAttr[name="bn1"](%573)
  %576 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %577 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%576)
  %578 : __torch__.torch.nn.modules.module.___torch_mangle_121.Module = prim::GetAttr[name="tree2"](%577)
  %579 : __torch__.torch.nn.modules.module.___torch_mangle_114.Module = prim::GetAttr[name="tree2"](%578)
  %580 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_107.QuantConv2d = prim::GetAttr[name="conv1"](%579)
  %582 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %583 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%582)
  %584 : __torch__.torch.nn.modules.module.___torch_mangle_121.Module = prim::GetAttr[name="tree2"](%583)
  %585 : __torch__.torch.nn.modules.module.___torch_mangle_104.Module = prim::GetAttr[name="tree1"](%584)
  %586 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_identify.___torch_mangle_103.QuantIdentity = prim::GetAttr[name="relu_output_quant"](%585)
  %588 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %589 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%588)
  %590 : __torch__.torch.nn.modules.module.___torch_mangle_121.Module = prim::GetAttr[name="tree2"](%589)
  %591 : __torch__.torch.nn.modules.module.___torch_mangle_104.Module = prim::GetAttr[name="tree1"](%590)
  %592 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_101.BatchNorm2d = prim::GetAttr[name="bn2"](%591)
  %594 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %595 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%594)
  %596 : __torch__.torch.nn.modules.module.___torch_mangle_121.Module = prim::GetAttr[name="tree2"](%595)
  %597 : __torch__.torch.nn.modules.module.___torch_mangle_104.Module = prim::GetAttr[name="tree1"](%596)
  %598 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_100.QuantConv2d = prim::GetAttr[name="conv2"](%597)
  %600 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %601 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%600)
  %602 : __torch__.torch.nn.modules.module.___torch_mangle_121.Module = prim::GetAttr[name="tree2"](%601)
  %603 : __torch__.torch.nn.modules.module.___torch_mangle_104.Module = prim::GetAttr[name="tree1"](%602)
  %604 : __torch__.torch.nn.modules.activation.___torch_mangle_97.ReLU = prim::GetAttr[name="relu"](%603)
  %606 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %607 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%606)
  %608 : __torch__.torch.nn.modules.module.___torch_mangle_121.Module = prim::GetAttr[name="tree2"](%607)
  %609 : __torch__.torch.nn.modules.module.___torch_mangle_104.Module = prim::GetAttr[name="tree1"](%608)
  %610 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_96.BatchNorm2d = prim::GetAttr[name="bn1"](%609)
  %612 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %613 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%612)
  %614 : __torch__.torch.nn.modules.module.___torch_mangle_121.Module = prim::GetAttr[name="tree2"](%613)
  %615 : __torch__.torch.nn.modules.module.___torch_mangle_104.Module = prim::GetAttr[name="tree1"](%614)
  %616 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_95.QuantConv2d = prim::GetAttr[name="conv1"](%615)
  %618 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %619 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%618)
  %620 : __torch__.torch.nn.modules.module.___torch_mangle_92.Module = prim::GetAttr[name="tree1"](%619)
  %621 : __torch__.torch.nn.modules.module.___torch_mangle_91.Module = prim::GetAttr[name="root"](%620)
  %622 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_identify.___torch_mangle_90.QuantIdentity = prim::GetAttr[name="relu_output_quant"](%621)
  %624 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %625 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%624)
  %626 : __torch__.torch.nn.modules.module.___torch_mangle_92.Module = prim::GetAttr[name="tree1"](%625)
  %627 : __torch__.torch.nn.modules.module.___torch_mangle_91.Module = prim::GetAttr[name="root"](%626)
  %628 : __torch__.torch.nn.modules.activation.___torch_mangle_88.ReLU = prim::GetAttr[name="relu"](%627)
  %630 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %631 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%630)
  %632 : __torch__.torch.nn.modules.module.___torch_mangle_92.Module = prim::GetAttr[name="tree1"](%631)
  %633 : __torch__.torch.nn.modules.module.___torch_mangle_91.Module = prim::GetAttr[name="root"](%632)
  %634 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_87.BatchNorm2d = prim::GetAttr[name="bn"](%633)
  %636 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %637 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%636)
  %638 : __torch__.torch.nn.modules.module.___torch_mangle_92.Module = prim::GetAttr[name="tree1"](%637)
  %639 : __torch__.torch.nn.modules.module.___torch_mangle_91.Module = prim::GetAttr[name="root"](%638)
  %640 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_86.QuantConv2d = prim::GetAttr[name="conv"](%639)
  %642 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %643 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%642)
  %644 : __torch__.torch.nn.modules.module.___torch_mangle_92.Module = prim::GetAttr[name="tree1"](%643)
  %645 : __torch__.torch.nn.modules.module.___torch_mangle_83.Module = prim::GetAttr[name="tree2"](%644)
  %646 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_82.BatchNorm2d = prim::GetAttr[name="bn2"](%645)
  %648 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %649 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%648)
  %650 : __torch__.torch.nn.modules.module.___torch_mangle_92.Module = prim::GetAttr[name="tree1"](%649)
  %651 : __torch__.torch.nn.modules.module.___torch_mangle_83.Module = prim::GetAttr[name="tree2"](%650)
  %652 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_81.QuantConv2d = prim::GetAttr[name="conv2"](%651)
  %654 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %655 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%654)
  %656 : __torch__.torch.nn.modules.module.___torch_mangle_92.Module = prim::GetAttr[name="tree1"](%655)
  %657 : __torch__.torch.nn.modules.module.___torch_mangle_83.Module = prim::GetAttr[name="tree2"](%656)
  %658 : __torch__.torch.nn.modules.activation.___torch_mangle_78.ReLU = prim::GetAttr[name="relu"](%657)
  %660 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %661 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%660)
  %662 : __torch__.torch.nn.modules.module.___torch_mangle_92.Module = prim::GetAttr[name="tree1"](%661)
  %663 : __torch__.torch.nn.modules.module.___torch_mangle_83.Module = prim::GetAttr[name="tree2"](%662)
  %664 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_77.BatchNorm2d = prim::GetAttr[name="bn1"](%663)
  %666 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %667 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%666)
  %668 : __torch__.torch.nn.modules.module.___torch_mangle_92.Module = prim::GetAttr[name="tree1"](%667)
  %669 : __torch__.torch.nn.modules.module.___torch_mangle_83.Module = prim::GetAttr[name="tree2"](%668)
  %670 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_76.QuantConv2d = prim::GetAttr[name="conv1"](%669)
  %672 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %673 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%672)
  %674 : __torch__.torch.nn.modules.module.___torch_mangle_92.Module = prim::GetAttr[name="tree1"](%673)
  %675 : __torch__.torch.nn.modules.module.___torch_mangle_73.Module = prim::GetAttr[name="tree1"](%674)
  %676 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_identify.___torch_mangle_72.QuantIdentity = prim::GetAttr[name="relu_output_quant"](%675)
  %678 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %679 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%678)
  %680 : __torch__.torch.nn.modules.module.___torch_mangle_92.Module = prim::GetAttr[name="tree1"](%679)
  %681 : __torch__.torch.nn.modules.module.___torch_mangle_73.Module = prim::GetAttr[name="tree1"](%680)
  %682 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_70.BatchNorm2d = prim::GetAttr[name="bn2"](%681)
  %684 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %685 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%684)
  %686 : __torch__.torch.nn.modules.module.___torch_mangle_92.Module = prim::GetAttr[name="tree1"](%685)
  %687 : __torch__.torch.nn.modules.module.___torch_mangle_73.Module = prim::GetAttr[name="tree1"](%686)
  %688 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_69.QuantConv2d = prim::GetAttr[name="conv2"](%687)
  %690 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %691 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%690)
  %692 : __torch__.torch.nn.modules.module.___torch_mangle_92.Module = prim::GetAttr[name="tree1"](%691)
  %693 : __torch__.torch.nn.modules.module.___torch_mangle_73.Module = prim::GetAttr[name="tree1"](%692)
  %694 : __torch__.torch.nn.modules.activation.___torch_mangle_66.ReLU = prim::GetAttr[name="relu"](%693)
  %696 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %697 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%696)
  %698 : __torch__.torch.nn.modules.module.___torch_mangle_92.Module = prim::GetAttr[name="tree1"](%697)
  %699 : __torch__.torch.nn.modules.module.___torch_mangle_73.Module = prim::GetAttr[name="tree1"](%698)
  %700 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_65.BatchNorm2d = prim::GetAttr[name="bn1"](%699)
  %702 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %703 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%702)
  %704 : __torch__.torch.nn.modules.module.___torch_mangle_92.Module = prim::GetAttr[name="tree1"](%703)
  %705 : __torch__.torch.nn.modules.module.___torch_mangle_73.Module = prim::GetAttr[name="tree1"](%704)
  %706 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_64.QuantConv2d = prim::GetAttr[name="conv1"](%705)
  %708 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %709 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%708)
  %710 : __torch__.torch.nn.modules.module.___torch_mangle_92.Module = prim::GetAttr[name="tree1"](%709)
  %711 : __torch__.torch.nn.modules.module.___torch_mangle_61.Module = prim::GetAttr[name="project"](%710)
  %712 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_identify.___torch_mangle_60.QuantIdentity = prim::GetAttr[name="1_output_quant"](%711)
  %714 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %715 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%714)
  %716 : __torch__.torch.nn.modules.module.___torch_mangle_92.Module = prim::GetAttr[name="tree1"](%715)
  %717 : __torch__.torch.nn.modules.module.___torch_mangle_61.Module = prim::GetAttr[name="project"](%716)
  %718 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_58.BatchNorm2d = prim::GetAttr[name="1"](%717)
  %720 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %721 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%720)
  %722 : __torch__.torch.nn.modules.module.___torch_mangle_92.Module = prim::GetAttr[name="tree1"](%721)
  %723 : __torch__.torch.nn.modules.module.___torch_mangle_61.Module = prim::GetAttr[name="project"](%722)
  %724 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_57.QuantConv2d = prim::GetAttr[name="0"](%723)
  %726 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %727 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%726)
  %728 : __torch__.torch.nn.modules.module.___torch_mangle_92.Module = prim::GetAttr[name="tree1"](%727)
  %729 : __torch__.torch.nn.modules.pooling.___torch_mangle_54.MaxPool2d = prim::GetAttr[name="downsample"](%728)
  %731 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %732 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%731)
  %733 : __torch__.torch.nn.modules.module.___torch_mangle_53.Module = prim::GetAttr[name="project"](%732)
  %734 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_52.BatchNorm2d = prim::GetAttr[name="1"](%733)
  %736 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %737 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%736)
  %738 : __torch__.torch.nn.modules.module.___torch_mangle_53.Module = prim::GetAttr[name="project"](%737)
  %739 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_51.QuantConv2d = prim::GetAttr[name="0"](%738)
  %741 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %742 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%741)
  %743 : __torch__.torch.nn.modules.pooling.___torch_mangle_48.MaxPool2d = prim::GetAttr[name="downsample"](%742)
  %745 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %746 : __torch__.torch.nn.modules.module.___torch_mangle_47.Module = prim::GetAttr[name="level2"](%745)
  %747 : __torch__.torch.nn.modules.module.___torch_mangle_46.Module = prim::GetAttr[name="root"](%746)
  %748 : __torch__.torch.nn.modules.activation.___torch_mangle_45.ReLU = prim::GetAttr[name="relu"](%747)
  %750 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %751 : __torch__.torch.nn.modules.module.___torch_mangle_47.Module = prim::GetAttr[name="level2"](%750)
  %752 : __torch__.torch.nn.modules.module.___torch_mangle_46.Module = prim::GetAttr[name="root"](%751)
  %753 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_44.BatchNorm2d = prim::GetAttr[name="bn"](%752)
  %755 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %756 : __torch__.torch.nn.modules.module.___torch_mangle_47.Module = prim::GetAttr[name="level2"](%755)
  %757 : __torch__.torch.nn.modules.module.___torch_mangle_46.Module = prim::GetAttr[name="root"](%756)
  %758 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_43.QuantConv2d = prim::GetAttr[name="conv"](%757)
  %760 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %761 : __torch__.torch.nn.modules.module.___torch_mangle_47.Module = prim::GetAttr[name="level2"](%760)
  %762 : __torch__.torch.nn.modules.module.___torch_mangle_40.Module = prim::GetAttr[name="tree2"](%761)
  %763 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_39.BatchNorm2d = prim::GetAttr[name="bn2"](%762)
  %765 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %766 : __torch__.torch.nn.modules.module.___torch_mangle_47.Module = prim::GetAttr[name="level2"](%765)
  %767 : __torch__.torch.nn.modules.module.___torch_mangle_40.Module = prim::GetAttr[name="tree2"](%766)
  %768 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_38.QuantConv2d = prim::GetAttr[name="conv2"](%767)
  %770 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %771 : __torch__.torch.nn.modules.module.___torch_mangle_47.Module = prim::GetAttr[name="level2"](%770)
  %772 : __torch__.torch.nn.modules.module.___torch_mangle_40.Module = prim::GetAttr[name="tree2"](%771)
  %773 : __torch__.torch.nn.modules.activation.___torch_mangle_35.ReLU = prim::GetAttr[name="relu"](%772)
  %775 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %776 : __torch__.torch.nn.modules.module.___torch_mangle_47.Module = prim::GetAttr[name="level2"](%775)
  %777 : __torch__.torch.nn.modules.module.___torch_mangle_40.Module = prim::GetAttr[name="tree2"](%776)
  %778 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_34.BatchNorm2d = prim::GetAttr[name="bn1"](%777)
  %780 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %781 : __torch__.torch.nn.modules.module.___torch_mangle_47.Module = prim::GetAttr[name="level2"](%780)
  %782 : __torch__.torch.nn.modules.module.___torch_mangle_40.Module = prim::GetAttr[name="tree2"](%781)
  %783 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_33.QuantConv2d = prim::GetAttr[name="conv1"](%782)
  %785 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %786 : __torch__.torch.nn.modules.module.___torch_mangle_47.Module = prim::GetAttr[name="level2"](%785)
  %787 : __torch__.torch.nn.modules.module.___torch_mangle_30.Module = prim::GetAttr[name="tree1"](%786)
  %788 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_identify.___torch_mangle_29.QuantIdentity = prim::GetAttr[name="relu_output_quant"](%787)
  %790 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %791 : __torch__.torch.nn.modules.module.___torch_mangle_47.Module = prim::GetAttr[name="level2"](%790)
  %792 : __torch__.torch.nn.modules.module.___torch_mangle_30.Module = prim::GetAttr[name="tree1"](%791)
  %793 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_27.BatchNorm2d = prim::GetAttr[name="bn2"](%792)
  %795 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %796 : __torch__.torch.nn.modules.module.___torch_mangle_47.Module = prim::GetAttr[name="level2"](%795)
  %797 : __torch__.torch.nn.modules.module.___torch_mangle_30.Module = prim::GetAttr[name="tree1"](%796)
  %798 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_26.QuantConv2d = prim::GetAttr[name="conv2"](%797)
  %800 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %801 : __torch__.torch.nn.modules.module.___torch_mangle_47.Module = prim::GetAttr[name="level2"](%800)
  %802 : __torch__.torch.nn.modules.module.___torch_mangle_30.Module = prim::GetAttr[name="tree1"](%801)
  %803 : __torch__.torch.nn.modules.activation.___torch_mangle_23.ReLU = prim::GetAttr[name="relu"](%802)
  %805 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %806 : __torch__.torch.nn.modules.module.___torch_mangle_47.Module = prim::GetAttr[name="level2"](%805)
  %807 : __torch__.torch.nn.modules.module.___torch_mangle_30.Module = prim::GetAttr[name="tree1"](%806)
  %808 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_22.BatchNorm2d = prim::GetAttr[name="bn1"](%807)
  %810 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %811 : __torch__.torch.nn.modules.module.___torch_mangle_47.Module = prim::GetAttr[name="level2"](%810)
  %812 : __torch__.torch.nn.modules.module.___torch_mangle_30.Module = prim::GetAttr[name="tree1"](%811)
  %813 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_21.QuantConv2d = prim::GetAttr[name="conv1"](%812)
  %815 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %816 : __torch__.torch.nn.modules.module.___torch_mangle_47.Module = prim::GetAttr[name="level2"](%815)
  %817 : __torch__.torch.nn.modules.module.___torch_mangle_18.Module = prim::GetAttr[name="project"](%816)
  %818 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_identify.QuantIdentity = prim::GetAttr[name="1_output_quant"](%817)
  %820 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %821 : __torch__.torch.nn.modules.module.___torch_mangle_47.Module = prim::GetAttr[name="level2"](%820)
  %822 : __torch__.torch.nn.modules.module.___torch_mangle_18.Module = prim::GetAttr[name="project"](%821)
  %823 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_16.BatchNorm2d = prim::GetAttr[name="1"](%822)
  %825 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %826 : __torch__.torch.nn.modules.module.___torch_mangle_47.Module = prim::GetAttr[name="level2"](%825)
  %827 : __torch__.torch.nn.modules.module.___torch_mangle_18.Module = prim::GetAttr[name="project"](%826)
  %828 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_15.QuantConv2d = prim::GetAttr[name="0"](%827)
  %830 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %831 : __torch__.torch.nn.modules.module.___torch_mangle_47.Module = prim::GetAttr[name="level2"](%830)
  %832 : __torch__.torch.nn.modules.pooling.MaxPool2d = prim::GetAttr[name="downsample"](%831)
  %834 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %835 : __torch__.torch.nn.modules.module.___torch_mangle_12.Module = prim::GetAttr[name="level1"](%834)
  %836 : __torch__.torch.nn.modules.activation.___torch_mangle_11.ReLU = prim::GetAttr[name="2"](%835)
  %838 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %839 : __torch__.torch.nn.modules.module.___torch_mangle_12.Module = prim::GetAttr[name="level1"](%838)
  %840 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_10.BatchNorm2d = prim::GetAttr[name="1"](%839)
  %842 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %843 : __torch__.torch.nn.modules.module.___torch_mangle_12.Module = prim::GetAttr[name="level1"](%842)
  %844 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_9.QuantConv2d = prim::GetAttr[name="0"](%843)
  %846 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %847 : __torch__.torch.nn.modules.module.___torch_mangle_6.Module = prim::GetAttr[name="level0"](%846)
  %848 : __torch__.torch.nn.modules.activation.___torch_mangle_5.ReLU = prim::GetAttr[name="2"](%847)
  %850 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %851 : __torch__.torch.nn.modules.module.___torch_mangle_6.Module = prim::GetAttr[name="level0"](%850)
  %852 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_4.BatchNorm2d = prim::GetAttr[name="1"](%851)
  %854 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %855 : __torch__.torch.nn.modules.module.___torch_mangle_6.Module = prim::GetAttr[name="level0"](%854)
  %856 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_3.QuantConv2d = prim::GetAttr[name="0"](%855)
  %858 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %859 : __torch__.torch.nn.modules.module.Module = prim::GetAttr[name="base_layer"](%858)
  %860 : __torch__.torch.nn.modules.activation.ReLU = prim::GetAttr[name="2"](%859)
  %862 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %863 : __torch__.torch.nn.modules.module.Module = prim::GetAttr[name="base_layer"](%862)
  %864 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name="1"](%863)
  %866 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %867 : __torch__.torch.nn.modules.module.Module = prim::GetAttr[name="base_layer"](%866)
  %868 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.QuantConv2d = prim::GetAttr[name="0"](%867)
  %1456 : Tensor = prim::Constant[value=<Tensor>]()
  %1457 : Tensor = prim::Constant[value=<Tensor>]()
  %1458 : float = prim::Constant[value=0.033028970553180367]()
  %1459 : int = prim::Constant[value=0]()
  %1460 : int = prim::Constant[value=-128]()
  %1461 : int = prim::Constant[value=127]()
  %1462 : int = prim::Constant[value=1]()
  %1463 : NoneType = prim::Constant()
  %1464 : int[] = prim::Constant[value=[1, 1]]()
  %1465 : int[] = prim::Constant[value=[3, 3]]()
  %1466 : bool = prim::Constant[value=0]()
  %1467 : int[] = prim::Constant[value=[0, 0]]()
  %1468 : bool = prim::Constant[value=1]()
  %1469 : Tensor = prim::GetAttr[name="weight"](%868)
  %1470 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_0.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%868)
  %1471 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%868)
  %quant_input.2 : Tensor = aten::fake_quantize_per_tensor_affine(%x.1, %1458, %1459, %1460, %1461)
  %quant_weight.2 : Tensor = aten::fake_quantize_per_channel_affine(%1469, %1457, %1456, %1459, %1460, %1461)
  %input.5 : Tensor = aten::_convolution(%quant_input.2, %quant_weight.2, %1463, %1464, %1465, %1464, %1466, %1467, %1462, %1466, %1466, %1468, %1468)
  %1475 : float = prim::Constant[value=1.0000000000000001e-05]()
  %1476 : float = prim::Constant[value=0.10000000000000001]()
  %1477 : bool = prim::Constant[value=0]()
  %1478 : bool = prim::Constant[value=1]()
  %1479 : Tensor = prim::GetAttr[name="running_var"](%864)
  %1480 : Tensor = prim::GetAttr[name="running_mean"](%864)
  %1481 : Tensor = prim::GetAttr[name="bias"](%864)
  %1482 : Tensor = prim::GetAttr[name="weight"](%864)
  %input.7 : Tensor = aten::batch_norm(%input.5, %1482, %1481, %1480, %1479, %1477, %1476, %1475, %1478)
  %1484 : Tensor = aten::relu_(%input.7)
  %1485 : Tensor = prim::Constant[value=<Tensor>]()
  %1486 : Tensor = prim::Constant[value=<Tensor>]()
  %1487 : float = prim::Constant[value=0.12232814999077264]()
  %1488 : int = prim::Constant[value=0]()
  %1489 : int = prim::Constant[value=-128]()
  %1490 : int = prim::Constant[value=127]()
  %1491 : int = prim::Constant[value=1]()
  %1492 : NoneType = prim::Constant()
  %1493 : int[] = prim::Constant[value=[1, 1]]()
  %1494 : bool = prim::Constant[value=0]()
  %1495 : int[] = prim::Constant[value=[0, 0]]()
  %1496 : bool = prim::Constant[value=1]()
  %1497 : Tensor = prim::GetAttr[name="weight"](%856)
  %1498 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_2.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%856)
  %1499 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_1.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%856)
  %quant_input.4 : Tensor = aten::fake_quantize_per_tensor_affine(%1484, %1487, %1488, %1489, %1490)
  %quant_weight.4 : Tensor = aten::fake_quantize_per_channel_affine(%1497, %1486, %1485, %1488, %1489, %1490)
  %input.9 : Tensor = aten::_convolution(%quant_input.4, %quant_weight.4, %1492, %1493, %1493, %1493, %1494, %1495, %1491, %1494, %1494, %1496, %1496)
  %1503 : float = prim::Constant[value=1.0000000000000001e-05]()
  %1504 : float = prim::Constant[value=0.10000000000000001]()
  %1505 : bool = prim::Constant[value=0]()
  %1506 : bool = prim::Constant[value=1]()
  %1507 : Tensor = prim::GetAttr[name="running_var"](%852)
  %1508 : Tensor = prim::GetAttr[name="running_mean"](%852)
  %1509 : Tensor = prim::GetAttr[name="bias"](%852)
  %1510 : Tensor = prim::GetAttr[name="weight"](%852)
  %input.11 : Tensor = aten::batch_norm(%input.9, %1510, %1509, %1508, %1507, %1505, %1504, %1503, %1506)
  %1512 : Tensor = aten::relu_(%input.11)
  %1513 : Tensor = prim::Constant[value=<Tensor>]()
  %1514 : Tensor = prim::Constant[value=<Tensor>]()
  %1515 : float = prim::Constant[value=0.24879081605926273]()
  %1516 : int = prim::Constant[value=0]()
  %1517 : int = prim::Constant[value=-128]()
  %1518 : int = prim::Constant[value=127]()
  %1519 : int = prim::Constant[value=1]()
  %1520 : NoneType = prim::Constant()
  %1521 : int[] = prim::Constant[value=[2, 2]]()
  %1522 : int[] = prim::Constant[value=[1, 1]]()
  %1523 : bool = prim::Constant[value=0]()
  %1524 : int[] = prim::Constant[value=[0, 0]]()
  %1525 : bool = prim::Constant[value=1]()
  %1526 : Tensor = prim::GetAttr[name="weight"](%844)
  %1527 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_8.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%844)
  %1528 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_7.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%844)
  %quant_input.6 : Tensor = aten::fake_quantize_per_tensor_affine(%1512, %1515, %1516, %1517, %1518)
  %quant_weight.6 : Tensor = aten::fake_quantize_per_channel_affine(%1526, %1514, %1513, %1516, %1517, %1518)
  %input.13 : Tensor = aten::_convolution(%quant_input.6, %quant_weight.6, %1520, %1521, %1522, %1522, %1523, %1524, %1519, %1523, %1523, %1525, %1525)
  %1532 : float = prim::Constant[value=1.0000000000000001e-05]()
  %1533 : float = prim::Constant[value=0.10000000000000001]()
  %1534 : bool = prim::Constant[value=0]()
  %1535 : bool = prim::Constant[value=1]()
  %1536 : Tensor = prim::GetAttr[name="running_var"](%840)
  %1537 : Tensor = prim::GetAttr[name="running_mean"](%840)
  %1538 : Tensor = prim::GetAttr[name="bias"](%840)
  %1539 : Tensor = prim::GetAttr[name="weight"](%840)
  %input.15 : Tensor = aten::batch_norm(%input.13, %1539, %1538, %1537, %1536, %1534, %1533, %1532, %1535)
  %1541 : Tensor = aten::relu_(%input.15)
  %1542 : int[] = prim::Constant[value=[2, 2]]()
  %1543 : int[] = prim::Constant[value=[0, 0]]()
  %1544 : int[] = prim::Constant[value=[1, 1]]()
  %1545 : bool = prim::Constant[value=0]()
  %inputs.5 : Tensor = aten::max_pool2d(%1541, %1542, %1542, %1543, %1544, %1545)
  %1547 : Tensor = prim::Constant[value=<Tensor>]()
  %1548 : Tensor = prim::Constant[value=<Tensor>]()
  %1549 : float = prim::Constant[value=0.20050177987166276]()
  %1550 : int = prim::Constant[value=0]()
  %1551 : int = prim::Constant[value=-128]()
  %1552 : int = prim::Constant[value=127]()
  %1553 : int = prim::Constant[value=1]()
  %1554 : NoneType = prim::Constant()
  %1555 : int[] = prim::Constant[value=[1, 1]]()
  %1556 : int[] = prim::Constant[value=[0, 0]]()
  %1557 : bool = prim::Constant[value=0]()
  %1558 : bool = prim::Constant[value=1]()
  %1559 : Tensor = prim::GetAttr[name="weight"](%828)
  %1560 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_14.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%828)
  %1561 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_13.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%828)
  %quant_input.8 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs.5, %1549, %1550, %1551, %1552)
  %quant_weight.8 : Tensor = aten::fake_quantize_per_channel_affine(%1559, %1548, %1547, %1550, %1551, %1552)
  %input.17 : Tensor = aten::_convolution(%quant_input.8, %quant_weight.8, %1554, %1555, %1556, %1555, %1557, %1556, %1553, %1557, %1557, %1558, %1558)
  %1565 : float = prim::Constant[value=1.0000000000000001e-05]()
  %1566 : float = prim::Constant[value=0.10000000000000001]()
  %1567 : bool = prim::Constant[value=0]()
  %1568 : bool = prim::Constant[value=1]()
  %1569 : Tensor = prim::GetAttr[name="running_var"](%823)
  %1570 : Tensor = prim::GetAttr[name="running_mean"](%823)
  %1571 : Tensor = prim::GetAttr[name="bias"](%823)
  %1572 : Tensor = prim::GetAttr[name="weight"](%823)
  %inputs.7 : Tensor = aten::batch_norm(%input.17, %1572, %1571, %1570, %1569, %1567, %1566, %1565, %1568)
  %1574 : float = prim::Constant[value=0.56899291511595718]()
  %1575 : int = prim::Constant[value=0]()
  %1576 : int = prim::Constant[value=-128]()
  %1577 : int = prim::Constant[value=127]()
  %1578 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_17.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%818)
  %base_level2_project_1_output_quant.1 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs.7, %1574, %1575, %1576, %1577)
  %1580 : Tensor = prim::Constant[value=<Tensor>]()
  %1581 : Tensor = prim::Constant[value=<Tensor>]()
  %1582 : float = prim::Constant[value=0.20050177987166276]()
  %1583 : int = prim::Constant[value=0]()
  %1584 : int = prim::Constant[value=-128]()
  %1585 : int = prim::Constant[value=127]()
  %1586 : int = prim::Constant[value=1]()
  %1587 : NoneType = prim::Constant()
  %1588 : int[] = prim::Constant[value=[2, 2]]()
  %1589 : int[] = prim::Constant[value=[1, 1]]()
  %1590 : bool = prim::Constant[value=0]()
  %1591 : int[] = prim::Constant[value=[0, 0]]()
  %1592 : bool = prim::Constant[value=1]()
  %1593 : Tensor = prim::GetAttr[name="weight"](%813)
  %1594 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_20.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%813)
  %1595 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_19.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%813)
  %quant_input.10 : Tensor = aten::fake_quantize_per_tensor_affine(%1541, %1582, %1583, %1584, %1585)
  %quant_weight.10 : Tensor = aten::fake_quantize_per_channel_affine(%1593, %1581, %1580, %1583, %1584, %1585)
  %input.19 : Tensor = aten::_convolution(%quant_input.10, %quant_weight.10, %1587, %1588, %1589, %1589, %1590, %1591, %1586, %1590, %1590, %1592, %1592)
  %1599 : float = prim::Constant[value=1.0000000000000001e-05]()
  %1600 : float = prim::Constant[value=0.10000000000000001]()
  %1601 : bool = prim::Constant[value=0]()
  %1602 : bool = prim::Constant[value=1]()
  %1603 : Tensor = prim::GetAttr[name="running_var"](%808)
  %1604 : Tensor = prim::GetAttr[name="running_mean"](%808)
  %1605 : Tensor = prim::GetAttr[name="bias"](%808)
  %1606 : Tensor = prim::GetAttr[name="weight"](%808)
  %input.21 : Tensor = aten::batch_norm(%input.19, %1606, %1605, %1604, %1603, %1601, %1600, %1599, %1602)
  %1608 : Tensor = aten::relu_(%input.21)
  %1609 : Tensor = prim::Constant[value=<Tensor>]()
  %1610 : Tensor = prim::Constant[value=<Tensor>]()
  %1611 : float = prim::Constant[value=0.087626915278397208]()
  %1612 : int = prim::Constant[value=0]()
  %1613 : int = prim::Constant[value=-128]()
  %1614 : int = prim::Constant[value=127]()
  %1615 : int = prim::Constant[value=1]()
  %1616 : NoneType = prim::Constant()
  %1617 : int[] = prim::Constant[value=[1, 1]]()
  %1618 : bool = prim::Constant[value=0]()
  %1619 : int[] = prim::Constant[value=[0, 0]]()
  %1620 : bool = prim::Constant[value=1]()
  %1621 : Tensor = prim::GetAttr[name="weight"](%798)
  %1622 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_25.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%798)
  %1623 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_24.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%798)
  %quant_input.12 : Tensor = aten::fake_quantize_per_tensor_affine(%1608, %1611, %1612, %1613, %1614)
  %quant_weight.12 : Tensor = aten::fake_quantize_per_channel_affine(%1621, %1610, %1609, %1612, %1613, %1614)
  %input.23 : Tensor = aten::_convolution(%quant_input.12, %quant_weight.12, %1616, %1617, %1617, %1617, %1618, %1619, %1615, %1618, %1618, %1620, %1620)
  %1627 : float = prim::Constant[value=1.0000000000000001e-05]()
  %1628 : float = prim::Constant[value=0.10000000000000001]()
  %1629 : bool = prim::Constant[value=0]()
  %1630 : bool = prim::Constant[value=1]()
  %1631 : Tensor = prim::GetAttr[name="running_var"](%793)
  %1632 : Tensor = prim::GetAttr[name="running_mean"](%793)
  %1633 : Tensor = prim::GetAttr[name="bias"](%793)
  %1634 : Tensor = prim::GetAttr[name="weight"](%793)
  %base_level2_tree1_bn2.1 : Tensor = aten::batch_norm(%input.23, %1634, %1633, %1632, %1631, %1629, %1628, %1627, %1630)
  %input.3 : Tensor = aten::add(%base_level2_tree1_bn2.1, %base_level2_project_1_output_quant.1, %944)
  %1636 : Tensor = aten::relu_(%input.3)
  %1637 : float = prim::Constant[value=0.064039365513118229]()
  %1638 : int = prim::Constant[value=0]()
  %1639 : int = prim::Constant[value=-128]()
  %1640 : int = prim::Constant[value=127]()
  %1641 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_28.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%788)
  %base_level2_tree1_relu_output_quant.1 : Tensor = aten::fake_quantize_per_tensor_affine(%1636, %1637, %1638, %1639, %1640)
  %1643 : Tensor = prim::Constant[value=<Tensor>]()
  %1644 : Tensor = prim::Constant[value=<Tensor>]()
  %1645 : float = prim::Constant[value=0.064039365513118229]()
  %1646 : int = prim::Constant[value=0]()
  %1647 : int = prim::Constant[value=-128]()
  %1648 : int = prim::Constant[value=127]()
  %1649 : int = prim::Constant[value=1]()
  %1650 : NoneType = prim::Constant()
  %1651 : int[] = prim::Constant[value=[1, 1]]()
  %1652 : bool = prim::Constant[value=0]()
  %1653 : int[] = prim::Constant[value=[0, 0]]()
  %1654 : bool = prim::Constant[value=1]()
  %1655 : Tensor = prim::GetAttr[name="weight"](%783)
  %1656 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_32.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%783)
  %1657 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_31.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%783)
  %quant_input.14 : Tensor = aten::fake_quantize_per_tensor_affine(%1636, %1645, %1646, %1647, %1648)
  %quant_weight.14 : Tensor = aten::fake_quantize_per_channel_affine(%1655, %1644, %1643, %1646, %1647, %1648)
  %input.25 : Tensor = aten::_convolution(%quant_input.14, %quant_weight.14, %1650, %1651, %1651, %1651, %1652, %1653, %1649, %1652, %1652, %1654, %1654)
  %1661 : float = prim::Constant[value=1.0000000000000001e-05]()
  %1662 : float = prim::Constant[value=0.10000000000000001]()
  %1663 : bool = prim::Constant[value=0]()
  %1664 : bool = prim::Constant[value=1]()
  %1665 : Tensor = prim::GetAttr[name="running_var"](%778)
  %1666 : Tensor = prim::GetAttr[name="running_mean"](%778)
  %1667 : Tensor = prim::GetAttr[name="bias"](%778)
  %1668 : Tensor = prim::GetAttr[name="weight"](%778)
  %input.27 : Tensor = aten::batch_norm(%input.25, %1668, %1667, %1666, %1665, %1663, %1662, %1661, %1664)
  %1670 : Tensor = aten::relu_(%input.27)
  %1671 : Tensor = prim::Constant[value=<Tensor>]()
  %1672 : Tensor = prim::Constant[value=<Tensor>]()
  %1673 : float = prim::Constant[value=0.047357044820710431]()
  %1674 : int = prim::Constant[value=0]()
  %1675 : int = prim::Constant[value=-128]()
  %1676 : int = prim::Constant[value=127]()
  %1677 : int = prim::Constant[value=1]()
  %1678 : NoneType = prim::Constant()
  %1679 : int[] = prim::Constant[value=[1, 1]]()
  %1680 : bool = prim::Constant[value=0]()
  %1681 : int[] = prim::Constant[value=[0, 0]]()
  %1682 : bool = prim::Constant[value=1]()
  %1683 : Tensor = prim::GetAttr[name="weight"](%768)
  %1684 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_37.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%768)
  %1685 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_36.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%768)
  %quant_input.16 : Tensor = aten::fake_quantize_per_tensor_affine(%1670, %1673, %1674, %1675, %1676)
  %quant_weight.16 : Tensor = aten::fake_quantize_per_channel_affine(%1683, %1672, %1671, %1674, %1675, %1676)
  %input.29 : Tensor = aten::_convolution(%quant_input.16, %quant_weight.16, %1678, %1679, %1679, %1679, %1680, %1681, %1677, %1680, %1680, %1682, %1682)
  %1689 : float = prim::Constant[value=1.0000000000000001e-05]()
  %1690 : float = prim::Constant[value=0.10000000000000001]()
  %1691 : bool = prim::Constant[value=0]()
  %1692 : bool = prim::Constant[value=1]()
  %1693 : Tensor = prim::GetAttr[name="running_var"](%763)
  %1694 : Tensor = prim::GetAttr[name="running_mean"](%763)
  %1695 : Tensor = prim::GetAttr[name="bias"](%763)
  %1696 : Tensor = prim::GetAttr[name="weight"](%763)
  %base_level2_tree2_bn2.1 : Tensor = aten::batch_norm(%input.29, %1696, %1695, %1694, %1693, %1691, %1690, %1689, %1692)
  %input0.1 : Tensor = aten::add(%base_level2_tree2_bn2.1, %base_level2_tree1_relu_output_quant.1, %944)
  %1698 : Tensor = aten::relu_(%input0.1)
  %943 : Tensor[] = prim::ListConstruct(%1698, %1636)
  %inputs.3 : Tensor = aten::cat(%943, %944)
  %1699 : Tensor = prim::Constant[value=<Tensor>]()
  %1700 : Tensor = prim::Constant[value=<Tensor>]()
  %1701 : float = prim::Constant[value=0.10902035330224225]()
  %1702 : int = prim::Constant[value=0]()
  %1703 : int = prim::Constant[value=-128]()
  %1704 : int = prim::Constant[value=127]()
  %1705 : int = prim::Constant[value=1]()
  %1706 : NoneType = prim::Constant()
  %1707 : int[] = prim::Constant[value=[1, 1]]()
  %1708 : int[] = prim::Constant[value=[0, 0]]()
  %1709 : bool = prim::Constant[value=0]()
  %1710 : bool = prim::Constant[value=1]()
  %1711 : Tensor = prim::GetAttr[name="weight"](%758)
  %1712 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_42.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%758)
  %1713 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_41.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%758)
  %quant_input.18 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs.3, %1701, %1702, %1703, %1704)
  %quant_weight.18 : Tensor = aten::fake_quantize_per_channel_affine(%1711, %1700, %1699, %1702, %1703, %1704)
  %input.31 : Tensor = aten::_convolution(%quant_input.18, %quant_weight.18, %1706, %1707, %1708, %1707, %1709, %1708, %1705, %1709, %1709, %1710, %1710)
  %1717 : float = prim::Constant[value=1.0000000000000001e-05]()
  %1718 : float = prim::Constant[value=0.10000000000000001]()
  %1719 : bool = prim::Constant[value=0]()
  %1720 : bool = prim::Constant[value=1]()
  %1721 : Tensor = prim::GetAttr[name="running_var"](%753)
  %1722 : Tensor = prim::GetAttr[name="running_mean"](%753)
  %1723 : Tensor = prim::GetAttr[name="bias"](%753)
  %1724 : Tensor = prim::GetAttr[name="weight"](%753)
  %input.33 : Tensor = aten::batch_norm(%input.31, %1724, %1723, %1722, %1721, %1719, %1718, %1717, %1720)
  %1726 : Tensor = aten::relu_(%input.33)
  %1727 : int[] = prim::Constant[value=[2, 2]]()
  %1728 : int[] = prim::Constant[value=[0, 0]]()
  %1729 : int[] = prim::Constant[value=[1, 1]]()
  %1730 : bool = prim::Constant[value=0]()
  %inputs.9 : Tensor = aten::max_pool2d(%1726, %1727, %1727, %1728, %1729, %1730)
  %1732 : Tensor = prim::Constant[value=<Tensor>]()
  %1733 : Tensor = prim::Constant[value=<Tensor>]()
  %1734 : float = prim::Constant[value=0.068756268719049884]()
  %1735 : int = prim::Constant[value=0]()
  %1736 : int = prim::Constant[value=-128]()
  %1737 : int = prim::Constant[value=127]()
  %1738 : int = prim::Constant[value=1]()
  %1739 : NoneType = prim::Constant()
  %1740 : int[] = prim::Constant[value=[1, 1]]()
  %1741 : int[] = prim::Constant[value=[0, 0]]()
  %1742 : bool = prim::Constant[value=0]()
  %1743 : bool = prim::Constant[value=1]()
  %1744 : Tensor = prim::GetAttr[name="weight"](%739)
  %1745 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_50.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%739)
  %1746 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_49.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%739)
  %quant_input.20 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs.9, %1734, %1735, %1736, %1737)
  %quant_weight.20 : Tensor = aten::fake_quantize_per_channel_affine(%1744, %1733, %1732, %1735, %1736, %1737)
  %input.35 : Tensor = aten::_convolution(%quant_input.20, %quant_weight.20, %1739, %1740, %1741, %1740, %1742, %1741, %1738, %1742, %1742, %1743, %1743)
  %1750 : NoneType = prim::Constant()
  %1751 : int[] = prim::Constant[value=[2, 2]]()
  %1752 : int[] = prim::Constant[value=[0, 0]]()
  %1753 : int[] = prim::Constant[value=[1, 1]]()
  %1754 : bool = prim::Constant[value=0]()
  %inputs.11 : Tensor = aten::max_pool2d(%1726, %1751, %1751, %1752, %1753, %1754)
  %1756 : Tensor = prim::Constant[value=<Tensor>]()
  %1757 : Tensor = prim::Constant[value=<Tensor>]()
  %1758 : float = prim::Constant[value=0.068756268719049884]()
  %1759 : int = prim::Constant[value=0]()
  %1760 : int = prim::Constant[value=-128]()
  %1761 : int = prim::Constant[value=127]()
  %1762 : int = prim::Constant[value=1]()
  %1763 : NoneType = prim::Constant()
  %1764 : int[] = prim::Constant[value=[1, 1]]()
  %1765 : int[] = prim::Constant[value=[0, 0]]()
  %1766 : bool = prim::Constant[value=0]()
  %1767 : bool = prim::Constant[value=1]()
  %1768 : Tensor = prim::GetAttr[name="weight"](%724)
  %1769 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_56.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%724)
  %1770 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_55.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%724)
  %quant_input.22 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs.11, %1758, %1759, %1760, %1761)
  %quant_weight.22 : Tensor = aten::fake_quantize_per_channel_affine(%1768, %1757, %1756, %1759, %1760, %1761)
  %input.37 : Tensor = aten::_convolution(%quant_input.22, %quant_weight.22, %1763, %1764, %1765, %1764, %1766, %1765, %1762, %1766, %1766, %1767, %1767)
  %1774 : float = prim::Constant[value=1.0000000000000001e-05]()
  %1775 : float = prim::Constant[value=0.10000000000000001]()
  %1776 : bool = prim::Constant[value=0]()
  %1777 : bool = prim::Constant[value=1]()
  %1778 : Tensor = prim::GetAttr[name="running_var"](%718)
  %1779 : Tensor = prim::GetAttr[name="running_mean"](%718)
  %1780 : Tensor = prim::GetAttr[name="bias"](%718)
  %1781 : Tensor = prim::GetAttr[name="weight"](%718)
  %inputs.13 : Tensor = aten::batch_norm(%input.37, %1781, %1780, %1779, %1778, %1776, %1775, %1774, %1777)
  %1783 : float = prim::Constant[value=0.035285611790934888]()
  %1784 : int = prim::Constant[value=0]()
  %1785 : int = prim::Constant[value=-128]()
  %1786 : int = prim::Constant[value=127]()
  %1787 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_59.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%712)
  %base_level3_tree1_project_1_output_quant.1 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs.13, %1783, %1784, %1785, %1786)
  %1789 : Tensor = prim::Constant[value=<Tensor>]()
  %1790 : Tensor = prim::Constant[value=<Tensor>]()
  %1791 : float = prim::Constant[value=0.068756268719049884]()
  %1792 : int = prim::Constant[value=0]()
  %1793 : int = prim::Constant[value=-128]()
  %1794 : int = prim::Constant[value=127]()
  %1795 : int = prim::Constant[value=1]()
  %1796 : NoneType = prim::Constant()
  %1797 : int[] = prim::Constant[value=[2, 2]]()
  %1798 : int[] = prim::Constant[value=[1, 1]]()
  %1799 : bool = prim::Constant[value=0]()
  %1800 : int[] = prim::Constant[value=[0, 0]]()
  %1801 : bool = prim::Constant[value=1]()
  %1802 : Tensor = prim::GetAttr[name="weight"](%706)
  %1803 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_63.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%706)
  %1804 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_62.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%706)
  %quant_input.24 : Tensor = aten::fake_quantize_per_tensor_affine(%1726, %1791, %1792, %1793, %1794)
  %quant_weight.24 : Tensor = aten::fake_quantize_per_channel_affine(%1802, %1790, %1789, %1792, %1793, %1794)
  %input.39 : Tensor = aten::_convolution(%quant_input.24, %quant_weight.24, %1796, %1797, %1798, %1798, %1799, %1800, %1795, %1799, %1799, %1801, %1801)
  %1808 : float = prim::Constant[value=1.0000000000000001e-05]()
  %1809 : float = prim::Constant[value=0.10000000000000001]()
  %1810 : bool = prim::Constant[value=0]()
  %1811 : bool = prim::Constant[value=1]()
  %1812 : Tensor = prim::GetAttr[name="running_var"](%700)
  %1813 : Tensor = prim::GetAttr[name="running_mean"](%700)
  %1814 : Tensor = prim::GetAttr[name="bias"](%700)
  %1815 : Tensor = prim::GetAttr[name="weight"](%700)
  %input.41 : Tensor = aten::batch_norm(%input.39, %1815, %1814, %1813, %1812, %1810, %1809, %1808, %1811)
  %1817 : Tensor = aten::relu_(%input.41)
  %1818 : Tensor = prim::Constant[value=<Tensor>]()
  %1819 : Tensor = prim::Constant[value=<Tensor>]()
  %1820 : float = prim::Constant[value=0.037216479384054348]()
  %1821 : int = prim::Constant[value=0]()
  %1822 : int = prim::Constant[value=-128]()
  %1823 : int = prim::Constant[value=127]()
  %1824 : int = prim::Constant[value=1]()
  %1825 : NoneType = prim::Constant()
  %1826 : int[] = prim::Constant[value=[1, 1]]()
  %1827 : bool = prim::Constant[value=0]()
  %1828 : int[] = prim::Constant[value=[0, 0]]()
  %1829 : bool = prim::Constant[value=1]()
  %1830 : Tensor = prim::GetAttr[name="weight"](%688)
  %1831 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_68.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%688)
  %1832 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_67.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%688)
  %quant_input.26 : Tensor = aten::fake_quantize_per_tensor_affine(%1817, %1820, %1821, %1822, %1823)
  %quant_weight.26 : Tensor = aten::fake_quantize_per_channel_affine(%1830, %1819, %1818, %1821, %1822, %1823)
  %input.43 : Tensor = aten::_convolution(%quant_input.26, %quant_weight.26, %1825, %1826, %1826, %1826, %1827, %1828, %1824, %1827, %1827, %1829, %1829)
  %1836 : float = prim::Constant[value=1.0000000000000001e-05]()
  %1837 : float = prim::Constant[value=0.10000000000000001]()
  %1838 : bool = prim::Constant[value=0]()
  %1839 : bool = prim::Constant[value=1]()
  %1840 : Tensor = prim::GetAttr[name="running_var"](%682)
  %1841 : Tensor = prim::GetAttr[name="running_mean"](%682)
  %1842 : Tensor = prim::GetAttr[name="bias"](%682)
  %1843 : Tensor = prim::GetAttr[name="weight"](%682)
  %base_level3_tree1_tree1_bn2.1 : Tensor = aten::batch_norm(%input.43, %1843, %1842, %1841, %1840, %1838, %1837, %1836, %1839)
  %input1.1 : Tensor = aten::add(%base_level3_tree1_tree1_bn2.1, %base_level3_tree1_project_1_output_quant.1, %944)
  %1845 : Tensor = aten::relu_(%input1.1)
  %1846 : float = prim::Constant[value=0.04412779470128337]()
  %1847 : int = prim::Constant[value=0]()
  %1848 : int = prim::Constant[value=-128]()
  %1849 : int = prim::Constant[value=127]()
  %1850 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_71.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%676)
  %base_level3_tree1_tree1_relu_output_quant.1 : Tensor = aten::fake_quantize_per_tensor_affine(%1845, %1846, %1847, %1848, %1849)
  %1852 : Tensor = prim::Constant[value=<Tensor>]()
  %1853 : Tensor = prim::Constant[value=<Tensor>]()
  %1854 : float = prim::Constant[value=0.04412779470128337]()
  %1855 : int = prim::Constant[value=0]()
  %1856 : int = prim::Constant[value=-128]()
  %1857 : int = prim::Constant[value=127]()
  %1858 : int = prim::Constant[value=1]()
  %1859 : NoneType = prim::Constant()
  %1860 : int[] = prim::Constant[value=[1, 1]]()
  %1861 : bool = prim::Constant[value=0]()
  %1862 : int[] = prim::Constant[value=[0, 0]]()
  %1863 : bool = prim::Constant[value=1]()
  %1864 : Tensor = prim::GetAttr[name="weight"](%670)
  %1865 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_75.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%670)
  %1866 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_74.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%670)
  %quant_input.28 : Tensor = aten::fake_quantize_per_tensor_affine(%1845, %1854, %1855, %1856, %1857)
  %quant_weight.28 : Tensor = aten::fake_quantize_per_channel_affine(%1864, %1853, %1852, %1855, %1856, %1857)
  %input.45 : Tensor = aten::_convolution(%quant_input.28, %quant_weight.28, %1859, %1860, %1860, %1860, %1861, %1862, %1858, %1861, %1861, %1863, %1863)
  %1870 : float = prim::Constant[value=1.0000000000000001e-05]()
  %1871 : float = prim::Constant[value=0.10000000000000001]()
  %1872 : bool = prim::Constant[value=0]()
  %1873 : bool = prim::Constant[value=1]()
  %1874 : Tensor = prim::GetAttr[name="running_var"](%664)
  %1875 : Tensor = prim::GetAttr[name="running_mean"](%664)
  %1876 : Tensor = prim::GetAttr[name="bias"](%664)
  %1877 : Tensor = prim::GetAttr[name="weight"](%664)
  %input.47 : Tensor = aten::batch_norm(%input.45, %1877, %1876, %1875, %1874, %1872, %1871, %1870, %1873)
  %1879 : Tensor = aten::relu_(%input.47)
  %1880 : Tensor = prim::Constant[value=<Tensor>]()
  %1881 : Tensor = prim::Constant[value=<Tensor>]()
  %1882 : float = prim::Constant[value=0.024860139906875731]()
  %1883 : int = prim::Constant[value=0]()
  %1884 : int = prim::Constant[value=-128]()
  %1885 : int = prim::Constant[value=127]()
  %1886 : int = prim::Constant[value=1]()
  %1887 : NoneType = prim::Constant()
  %1888 : int[] = prim::Constant[value=[1, 1]]()
  %1889 : bool = prim::Constant[value=0]()
  %1890 : int[] = prim::Constant[value=[0, 0]]()
  %1891 : bool = prim::Constant[value=1]()
  %1892 : Tensor = prim::GetAttr[name="weight"](%652)
  %1893 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_80.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%652)
  %1894 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_79.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%652)
  %quant_input.30 : Tensor = aten::fake_quantize_per_tensor_affine(%1879, %1882, %1883, %1884, %1885)
  %quant_weight.30 : Tensor = aten::fake_quantize_per_channel_affine(%1892, %1881, %1880, %1883, %1884, %1885)
  %input.49 : Tensor = aten::_convolution(%quant_input.30, %quant_weight.30, %1887, %1888, %1888, %1888, %1889, %1890, %1886, %1889, %1889, %1891, %1891)
  %1898 : float = prim::Constant[value=1.0000000000000001e-05]()
  %1899 : float = prim::Constant[value=0.10000000000000001]()
  %1900 : bool = prim::Constant[value=0]()
  %1901 : bool = prim::Constant[value=1]()
  %1902 : Tensor = prim::GetAttr[name="running_var"](%646)
  %1903 : Tensor = prim::GetAttr[name="running_mean"](%646)
  %1904 : Tensor = prim::GetAttr[name="bias"](%646)
  %1905 : Tensor = prim::GetAttr[name="weight"](%646)
  %base_level3_tree1_tree2_bn2.1 : Tensor = aten::batch_norm(%input.49, %1905, %1904, %1903, %1902, %1900, %1899, %1898, %1901)
  %input2.1 : Tensor = aten::add(%base_level3_tree1_tree2_bn2.1, %base_level3_tree1_tree1_relu_output_quant.1, %944)
  %1907 : Tensor = aten::relu_(%input2.1)
  %1014 : Tensor[] = prim::ListConstruct(%1907, %1845)
  %inputs0.1 : Tensor = aten::cat(%1014, %944)
  %1908 : Tensor = prim::Constant[value=<Tensor>]()
  %1909 : Tensor = prim::Constant[value=<Tensor>]()
  %1910 : float = prim::Constant[value=0.055899556227556366]()
  %1911 : int = prim::Constant[value=0]()
  %1912 : int = prim::Constant[value=-128]()
  %1913 : int = prim::Constant[value=127]()
  %1914 : int = prim::Constant[value=1]()
  %1915 : NoneType = prim::Constant()
  %1916 : int[] = prim::Constant[value=[1, 1]]()
  %1917 : int[] = prim::Constant[value=[0, 0]]()
  %1918 : bool = prim::Constant[value=0]()
  %1919 : bool = prim::Constant[value=1]()
  %1920 : Tensor = prim::GetAttr[name="weight"](%640)
  %1921 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_85.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%640)
  %1922 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_84.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%640)
  %quant_input.32 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs0.1, %1910, %1911, %1912, %1913)
  %quant_weight.32 : Tensor = aten::fake_quantize_per_channel_affine(%1920, %1909, %1908, %1911, %1912, %1913)
  %input.51 : Tensor = aten::_convolution(%quant_input.32, %quant_weight.32, %1915, %1916, %1917, %1916, %1918, %1917, %1914, %1918, %1918, %1919, %1919)
  %1926 : float = prim::Constant[value=1.0000000000000001e-05]()
  %1927 : float = prim::Constant[value=0.10000000000000001]()
  %1928 : bool = prim::Constant[value=0]()
  %1929 : bool = prim::Constant[value=1]()
  %1930 : Tensor = prim::GetAttr[name="running_var"](%634)
  %1931 : Tensor = prim::GetAttr[name="running_mean"](%634)
  %1932 : Tensor = prim::GetAttr[name="bias"](%634)
  %1933 : Tensor = prim::GetAttr[name="weight"](%634)
  %input.53 : Tensor = aten::batch_norm(%input.51, %1933, %1932, %1931, %1930, %1928, %1927, %1926, %1929)
  %1935 : Tensor = aten::relu_(%input.53)
  %1936 : float = prim::Constant[value=0.034769336069662739]()
  %1937 : int = prim::Constant[value=0]()
  %1938 : int = prim::Constant[value=-128]()
  %1939 : int = prim::Constant[value=127]()
  %1940 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_89.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%622)
  %base_level3_tree1_root_relu_output_quant.1 : Tensor = aten::fake_quantize_per_tensor_affine(%1935, %1936, %1937, %1938, %1939)
  %1942 : Tensor = prim::Constant[value=<Tensor>]()
  %1943 : Tensor = prim::Constant[value=<Tensor>]()
  %1944 : float = prim::Constant[value=0.034769336069662739]()
  %1945 : int = prim::Constant[value=0]()
  %1946 : int = prim::Constant[value=-128]()
  %1947 : int = prim::Constant[value=127]()
  %1948 : int = prim::Constant[value=1]()
  %1949 : NoneType = prim::Constant()
  %1950 : int[] = prim::Constant[value=[1, 1]]()
  %1951 : bool = prim::Constant[value=0]()
  %1952 : int[] = prim::Constant[value=[0, 0]]()
  %1953 : bool = prim::Constant[value=1]()
  %1954 : Tensor = prim::GetAttr[name="weight"](%616)
  %1955 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_94.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%616)
  %1956 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_93.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%616)
  %quant_input.34 : Tensor = aten::fake_quantize_per_tensor_affine(%1935, %1944, %1945, %1946, %1947)
  %quant_weight.34 : Tensor = aten::fake_quantize_per_channel_affine(%1954, %1943, %1942, %1945, %1946, %1947)
  %input.55 : Tensor = aten::_convolution(%quant_input.34, %quant_weight.34, %1949, %1950, %1950, %1950, %1951, %1952, %1948, %1951, %1951, %1953, %1953)
  %1960 : float = prim::Constant[value=1.0000000000000001e-05]()
  %1961 : float = prim::Constant[value=0.10000000000000001]()
  %1962 : bool = prim::Constant[value=0]()
  %1963 : bool = prim::Constant[value=1]()
  %1964 : Tensor = prim::GetAttr[name="running_var"](%610)
  %1965 : Tensor = prim::GetAttr[name="running_mean"](%610)
  %1966 : Tensor = prim::GetAttr[name="bias"](%610)
  %1967 : Tensor = prim::GetAttr[name="weight"](%610)
  %input.57 : Tensor = aten::batch_norm(%input.55, %1967, %1966, %1965, %1964, %1962, %1961, %1960, %1963)
  %1969 : Tensor = aten::relu_(%input.57)
  %1970 : Tensor = prim::Constant[value=<Tensor>]()
  %1971 : Tensor = prim::Constant[value=<Tensor>]()
  %1972 : float = prim::Constant[value=0.036814873612771824]()
  %1973 : int = prim::Constant[value=0]()
  %1974 : int = prim::Constant[value=-128]()
  %1975 : int = prim::Constant[value=127]()
  %1976 : int = prim::Constant[value=1]()
  %1977 : NoneType = prim::Constant()
  %1978 : int[] = prim::Constant[value=[1, 1]]()
  %1979 : bool = prim::Constant[value=0]()
  %1980 : int[] = prim::Constant[value=[0, 0]]()
  %1981 : bool = prim::Constant[value=1]()
  %1982 : Tensor = prim::GetAttr[name="weight"](%598)
  %1983 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_99.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%598)
  %1984 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_98.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%598)
  %quant_input.36 : Tensor = aten::fake_quantize_per_tensor_affine(%1969, %1972, %1973, %1974, %1975)
  %quant_weight.36 : Tensor = aten::fake_quantize_per_channel_affine(%1982, %1971, %1970, %1973, %1974, %1975)
  %input.59 : Tensor = aten::_convolution(%quant_input.36, %quant_weight.36, %1977, %1978, %1978, %1978, %1979, %1980, %1976, %1979, %1979, %1981, %1981)
  %1988 : float = prim::Constant[value=1.0000000000000001e-05]()
  %1989 : float = prim::Constant[value=0.10000000000000001]()
  %1990 : bool = prim::Constant[value=0]()
  %1991 : bool = prim::Constant[value=1]()
  %1992 : Tensor = prim::GetAttr[name="running_var"](%592)
  %1993 : Tensor = prim::GetAttr[name="running_mean"](%592)
  %1994 : Tensor = prim::GetAttr[name="bias"](%592)
  %1995 : Tensor = prim::GetAttr[name="weight"](%592)
  %base_level3_tree2_tree1_bn2.1 : Tensor = aten::batch_norm(%input.59, %1995, %1994, %1993, %1992, %1990, %1989, %1988, %1991)
  %input3.1 : Tensor = aten::add(%base_level3_tree2_tree1_bn2.1, %base_level3_tree1_root_relu_output_quant.1, %944)
  %1997 : Tensor = aten::relu_(%input3.1)
  %1998 : float = prim::Constant[value=0.04428145265954686]()
  %1999 : int = prim::Constant[value=0]()
  %2000 : int = prim::Constant[value=-128]()
  %2001 : int = prim::Constant[value=127]()
  %2002 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_102.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%586)
  %base_level3_tree2_tree1_relu_output_quant.1 : Tensor = aten::fake_quantize_per_tensor_affine(%1997, %1998, %1999, %2000, %2001)
  %2004 : Tensor = prim::Constant[value=<Tensor>]()
  %2005 : Tensor = prim::Constant[value=<Tensor>]()
  %2006 : float = prim::Constant[value=0.04428145265954686]()
  %2007 : int = prim::Constant[value=0]()
  %2008 : int = prim::Constant[value=-128]()
  %2009 : int = prim::Constant[value=127]()
  %2010 : int = prim::Constant[value=1]()
  %2011 : NoneType = prim::Constant()
  %2012 : int[] = prim::Constant[value=[1, 1]]()
  %2013 : bool = prim::Constant[value=0]()
  %2014 : int[] = prim::Constant[value=[0, 0]]()
  %2015 : bool = prim::Constant[value=1]()
  %2016 : Tensor = prim::GetAttr[name="weight"](%580)
  %2017 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_106.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%580)
  %2018 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_105.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%580)
  %quant_input.38 : Tensor = aten::fake_quantize_per_tensor_affine(%1997, %2006, %2007, %2008, %2009)
  %quant_weight.38 : Tensor = aten::fake_quantize_per_channel_affine(%2016, %2005, %2004, %2007, %2008, %2009)
  %input.61 : Tensor = aten::_convolution(%quant_input.38, %quant_weight.38, %2011, %2012, %2012, %2012, %2013, %2014, %2010, %2013, %2013, %2015, %2015)
  %2022 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2023 : float = prim::Constant[value=0.10000000000000001]()
  %2024 : bool = prim::Constant[value=0]()
  %2025 : bool = prim::Constant[value=1]()
  %2026 : Tensor = prim::GetAttr[name="running_var"](%574)
  %2027 : Tensor = prim::GetAttr[name="running_mean"](%574)
  %2028 : Tensor = prim::GetAttr[name="bias"](%574)
  %2029 : Tensor = prim::GetAttr[name="weight"](%574)
  %input.63 : Tensor = aten::batch_norm(%input.61, %2029, %2028, %2027, %2026, %2024, %2023, %2022, %2025)
  %2031 : Tensor = aten::relu_(%input.63)
  %2032 : Tensor = prim::Constant[value=<Tensor>]()
  %2033 : Tensor = prim::Constant[value=<Tensor>]()
  %2034 : float = prim::Constant[value=0.038084904978594444]()
  %2035 : int = prim::Constant[value=0]()
  %2036 : int = prim::Constant[value=-128]()
  %2037 : int = prim::Constant[value=127]()
  %2038 : int = prim::Constant[value=1]()
  %2039 : NoneType = prim::Constant()
  %2040 : int[] = prim::Constant[value=[1, 1]]()
  %2041 : bool = prim::Constant[value=0]()
  %2042 : int[] = prim::Constant[value=[0, 0]]()
  %2043 : bool = prim::Constant[value=1]()
  %2044 : Tensor = prim::GetAttr[name="weight"](%562)
  %2045 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_111.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%562)
  %2046 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_110.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%562)
  %quant_input.40 : Tensor = aten::fake_quantize_per_tensor_affine(%2031, %2034, %2035, %2036, %2037)
  %quant_weight.40 : Tensor = aten::fake_quantize_per_channel_affine(%2044, %2033, %2032, %2035, %2036, %2037)
  %input.65 : Tensor = aten::_convolution(%quant_input.40, %quant_weight.40, %2039, %2040, %2040, %2040, %2041, %2042, %2038, %2041, %2041, %2043, %2043)
  %2050 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2051 : float = prim::Constant[value=0.10000000000000001]()
  %2052 : bool = prim::Constant[value=0]()
  %2053 : bool = prim::Constant[value=1]()
  %2054 : Tensor = prim::GetAttr[name="running_var"](%556)
  %2055 : Tensor = prim::GetAttr[name="running_mean"](%556)
  %2056 : Tensor = prim::GetAttr[name="bias"](%556)
  %2057 : Tensor = prim::GetAttr[name="weight"](%556)
  %base_level3_tree2_tree2_bn2.1 : Tensor = aten::batch_norm(%input.65, %2057, %2056, %2055, %2054, %2052, %2051, %2050, %2053)
  %input4.1 : Tensor = aten::add(%base_level3_tree2_tree2_bn2.1, %base_level3_tree2_tree1_relu_output_quant.1, %944)
  %2059 : Tensor = aten::relu_(%input4.1)
  %1071 : Tensor[] = prim::ListConstruct(%2059, %1997, %inputs.9, %1935)
  %inputs1.1 : Tensor = aten::cat(%1071, %944)
  %2060 : Tensor = prim::Constant[value=<Tensor>]()
  %2061 : Tensor = prim::Constant[value=<Tensor>]()
  %2062 : float = prim::Constant[value=0.076764474703570992]()
  %2063 : int = prim::Constant[value=0]()
  %2064 : int = prim::Constant[value=-128]()
  %2065 : int = prim::Constant[value=127]()
  %2066 : int = prim::Constant[value=1]()
  %2067 : NoneType = prim::Constant()
  %2068 : int[] = prim::Constant[value=[1, 1]]()
  %2069 : int[] = prim::Constant[value=[0, 0]]()
  %2070 : bool = prim::Constant[value=0]()
  %2071 : bool = prim::Constant[value=1]()
  %2072 : Tensor = prim::GetAttr[name="weight"](%550)
  %2073 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_116.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%550)
  %2074 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_115.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%550)
  %quant_input.42 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs1.1, %2062, %2063, %2064, %2065)
  %quant_weight.42 : Tensor = aten::fake_quantize_per_channel_affine(%2072, %2061, %2060, %2063, %2064, %2065)
  %input.67 : Tensor = aten::_convolution(%quant_input.42, %quant_weight.42, %2067, %2068, %2069, %2068, %2070, %2069, %2066, %2070, %2070, %2071, %2071)
  %2078 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2079 : float = prim::Constant[value=0.10000000000000001]()
  %2080 : bool = prim::Constant[value=0]()
  %2081 : bool = prim::Constant[value=1]()
  %2082 : Tensor = prim::GetAttr[name="running_var"](%544)
  %2083 : Tensor = prim::GetAttr[name="running_mean"](%544)
  %2084 : Tensor = prim::GetAttr[name="bias"](%544)
  %2085 : Tensor = prim::GetAttr[name="weight"](%544)
  %input.69 : Tensor = aten::batch_norm(%input.67, %2085, %2084, %2083, %2082, %2080, %2079, %2078, %2081)
  %2087 : Tensor = aten::relu_(%input.69)
  %2088 : int[] = prim::Constant[value=[2, 2]]()
  %2089 : int[] = prim::Constant[value=[0, 0]]()
  %2090 : int[] = prim::Constant[value=[1, 1]]()
  %2091 : bool = prim::Constant[value=0]()
  %inputs.15 : Tensor = aten::max_pool2d(%2087, %2088, %2088, %2089, %2090, %2091)
  %2093 : Tensor = prim::Constant[value=<Tensor>]()
  %2094 : Tensor = prim::Constant[value=<Tensor>]()
  %2095 : float = prim::Constant[value=0.046685489143912247]()
  %2096 : int = prim::Constant[value=0]()
  %2097 : int = prim::Constant[value=-128]()
  %2098 : int = prim::Constant[value=127]()
  %2099 : int = prim::Constant[value=1]()
  %2100 : NoneType = prim::Constant()
  %2101 : int[] = prim::Constant[value=[1, 1]]()
  %2102 : int[] = prim::Constant[value=[0, 0]]()
  %2103 : bool = prim::Constant[value=0]()
  %2104 : bool = prim::Constant[value=1]()
  %2105 : Tensor = prim::GetAttr[name="weight"](%528)
  %2106 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_125.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%528)
  %2107 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_124.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%528)
  %quant_input.44 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs.15, %2095, %2096, %2097, %2098)
  %quant_weight.44 : Tensor = aten::fake_quantize_per_channel_affine(%2105, %2094, %2093, %2096, %2097, %2098)
  %input.71 : Tensor = aten::_convolution(%quant_input.44, %quant_weight.44, %2100, %2101, %2102, %2101, %2103, %2102, %2099, %2103, %2103, %2104, %2104)
  %2111 : NoneType = prim::Constant()
  %2112 : int[] = prim::Constant[value=[2, 2]]()
  %2113 : int[] = prim::Constant[value=[0, 0]]()
  %2114 : int[] = prim::Constant[value=[1, 1]]()
  %2115 : bool = prim::Constant[value=0]()
  %inputs.17 : Tensor = aten::max_pool2d(%2087, %2112, %2112, %2113, %2114, %2115)
  %2117 : Tensor = prim::Constant[value=<Tensor>]()
  %2118 : Tensor = prim::Constant[value=<Tensor>]()
  %2119 : float = prim::Constant[value=0.046685489143912247]()
  %2120 : int = prim::Constant[value=0]()
  %2121 : int = prim::Constant[value=-128]()
  %2122 : int = prim::Constant[value=127]()
  %2123 : int = prim::Constant[value=1]()
  %2124 : NoneType = prim::Constant()
  %2125 : int[] = prim::Constant[value=[1, 1]]()
  %2126 : int[] = prim::Constant[value=[0, 0]]()
  %2127 : bool = prim::Constant[value=0]()
  %2128 : bool = prim::Constant[value=1]()
  %2129 : Tensor = prim::GetAttr[name="weight"](%513)
  %2130 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_131.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%513)
  %2131 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_130.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%513)
  %quant_input.46 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs.17, %2119, %2120, %2121, %2122)
  %quant_weight.46 : Tensor = aten::fake_quantize_per_channel_affine(%2129, %2118, %2117, %2120, %2121, %2122)
  %input.73 : Tensor = aten::_convolution(%quant_input.46, %quant_weight.46, %2124, %2125, %2126, %2125, %2127, %2126, %2123, %2127, %2127, %2128, %2128)
  %2135 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2136 : float = prim::Constant[value=0.10000000000000001]()
  %2137 : bool = prim::Constant[value=0]()
  %2138 : bool = prim::Constant[value=1]()
  %2139 : Tensor = prim::GetAttr[name="running_var"](%507)
  %2140 : Tensor = prim::GetAttr[name="running_mean"](%507)
  %2141 : Tensor = prim::GetAttr[name="bias"](%507)
  %2142 : Tensor = prim::GetAttr[name="weight"](%507)
  %inputs.19 : Tensor = aten::batch_norm(%input.73, %2142, %2141, %2140, %2139, %2137, %2136, %2135, %2138)
  %2144 : float = prim::Constant[value=0.038747915132777901]()
  %2145 : int = prim::Constant[value=0]()
  %2146 : int = prim::Constant[value=-128]()
  %2147 : int = prim::Constant[value=127]()
  %2148 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_134.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%501)
  %base_level4_tree1_project_1_output_quant.1 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs.19, %2144, %2145, %2146, %2147)
  %2150 : Tensor = prim::Constant[value=<Tensor>]()
  %2151 : Tensor = prim::Constant[value=<Tensor>]()
  %2152 : float = prim::Constant[value=0.046685489143912247]()
  %2153 : int = prim::Constant[value=0]()
  %2154 : int = prim::Constant[value=-128]()
  %2155 : int = prim::Constant[value=127]()
  %2156 : int = prim::Constant[value=1]()
  %2157 : NoneType = prim::Constant()
  %2158 : int[] = prim::Constant[value=[2, 2]]()
  %2159 : int[] = prim::Constant[value=[1, 1]]()
  %2160 : bool = prim::Constant[value=0]()
  %2161 : int[] = prim::Constant[value=[0, 0]]()
  %2162 : bool = prim::Constant[value=1]()
  %2163 : Tensor = prim::GetAttr[name="weight"](%495)
  %2164 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_138.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%495)
  %2165 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_137.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%495)
  %quant_input.48 : Tensor = aten::fake_quantize_per_tensor_affine(%2087, %2152, %2153, %2154, %2155)
  %quant_weight.48 : Tensor = aten::fake_quantize_per_channel_affine(%2163, %2151, %2150, %2153, %2154, %2155)
  %input.75 : Tensor = aten::_convolution(%quant_input.48, %quant_weight.48, %2157, %2158, %2159, %2159, %2160, %2161, %2156, %2160, %2160, %2162, %2162)
  %2169 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2170 : float = prim::Constant[value=0.10000000000000001]()
  %2171 : bool = prim::Constant[value=0]()
  %2172 : bool = prim::Constant[value=1]()
  %2173 : Tensor = prim::GetAttr[name="running_var"](%489)
  %2174 : Tensor = prim::GetAttr[name="running_mean"](%489)
  %2175 : Tensor = prim::GetAttr[name="bias"](%489)
  %2176 : Tensor = prim::GetAttr[name="weight"](%489)
  %input.77 : Tensor = aten::batch_norm(%input.75, %2176, %2175, %2174, %2173, %2171, %2170, %2169, %2172)
  %2178 : Tensor = aten::relu_(%input.77)
  %2179 : Tensor = prim::Constant[value=<Tensor>]()
  %2180 : Tensor = prim::Constant[value=<Tensor>]()
  %2181 : float = prim::Constant[value=0.032149284843384751]()
  %2182 : int = prim::Constant[value=0]()
  %2183 : int = prim::Constant[value=-128]()
  %2184 : int = prim::Constant[value=127]()
  %2185 : int = prim::Constant[value=1]()
  %2186 : NoneType = prim::Constant()
  %2187 : int[] = prim::Constant[value=[1, 1]]()
  %2188 : bool = prim::Constant[value=0]()
  %2189 : int[] = prim::Constant[value=[0, 0]]()
  %2190 : bool = prim::Constant[value=1]()
  %2191 : Tensor = prim::GetAttr[name="weight"](%477)
  %2192 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_143.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%477)
  %2193 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_142.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%477)
  %quant_input.50 : Tensor = aten::fake_quantize_per_tensor_affine(%2178, %2181, %2182, %2183, %2184)
  %quant_weight.50 : Tensor = aten::fake_quantize_per_channel_affine(%2191, %2180, %2179, %2182, %2183, %2184)
  %input.79 : Tensor = aten::_convolution(%quant_input.50, %quant_weight.50, %2186, %2187, %2187, %2187, %2188, %2189, %2185, %2188, %2188, %2190, %2190)
  %2197 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2198 : float = prim::Constant[value=0.10000000000000001]()
  %2199 : bool = prim::Constant[value=0]()
  %2200 : bool = prim::Constant[value=1]()
  %2201 : Tensor = prim::GetAttr[name="running_var"](%471)
  %2202 : Tensor = prim::GetAttr[name="running_mean"](%471)
  %2203 : Tensor = prim::GetAttr[name="bias"](%471)
  %2204 : Tensor = prim::GetAttr[name="weight"](%471)
  %base_level4_tree1_tree1_bn2.1 : Tensor = aten::batch_norm(%input.79, %2204, %2203, %2202, %2201, %2199, %2198, %2197, %2200)
  %input5.1 : Tensor = aten::add(%base_level4_tree1_tree1_bn2.1, %base_level4_tree1_project_1_output_quant.1, %944)
  %2206 : Tensor = aten::relu_(%input5.1)
  %2207 : float = prim::Constant[value=0.041200863094780389]()
  %2208 : int = prim::Constant[value=0]()
  %2209 : int = prim::Constant[value=-128]()
  %2210 : int = prim::Constant[value=127]()
  %2211 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_146.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%465)
  %base_level4_tree1_tree1_relu_output_quant.1 : Tensor = aten::fake_quantize_per_tensor_affine(%2206, %2207, %2208, %2209, %2210)
  %2213 : Tensor = prim::Constant[value=<Tensor>]()
  %2214 : Tensor = prim::Constant[value=<Tensor>]()
  %2215 : float = prim::Constant[value=0.041200863094780389]()
  %2216 : int = prim::Constant[value=0]()
  %2217 : int = prim::Constant[value=-128]()
  %2218 : int = prim::Constant[value=127]()
  %2219 : int = prim::Constant[value=1]()
  %2220 : NoneType = prim::Constant()
  %2221 : int[] = prim::Constant[value=[1, 1]]()
  %2222 : bool = prim::Constant[value=0]()
  %2223 : int[] = prim::Constant[value=[0, 0]]()
  %2224 : bool = prim::Constant[value=1]()
  %2225 : Tensor = prim::GetAttr[name="weight"](%459)
  %2226 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_150.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%459)
  %2227 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_149.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%459)
  %quant_input.52 : Tensor = aten::fake_quantize_per_tensor_affine(%2206, %2215, %2216, %2217, %2218)
  %quant_weight.52 : Tensor = aten::fake_quantize_per_channel_affine(%2225, %2214, %2213, %2216, %2217, %2218)
  %input.81 : Tensor = aten::_convolution(%quant_input.52, %quant_weight.52, %2220, %2221, %2221, %2221, %2222, %2223, %2219, %2222, %2222, %2224, %2224)
  %2231 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2232 : float = prim::Constant[value=0.10000000000000001]()
  %2233 : bool = prim::Constant[value=0]()
  %2234 : bool = prim::Constant[value=1]()
  %2235 : Tensor = prim::GetAttr[name="running_var"](%453)
  %2236 : Tensor = prim::GetAttr[name="running_mean"](%453)
  %2237 : Tensor = prim::GetAttr[name="bias"](%453)
  %2238 : Tensor = prim::GetAttr[name="weight"](%453)
  %input.83 : Tensor = aten::batch_norm(%input.81, %2238, %2237, %2236, %2235, %2233, %2232, %2231, %2234)
  %2240 : Tensor = aten::relu_(%input.83)
  %2241 : Tensor = prim::Constant[value=<Tensor>]()
  %2242 : Tensor = prim::Constant[value=<Tensor>]()
  %2243 : float = prim::Constant[value=0.041013927910271593]()
  %2244 : int = prim::Constant[value=0]()
  %2245 : int = prim::Constant[value=-128]()
  %2246 : int = prim::Constant[value=127]()
  %2247 : int = prim::Constant[value=1]()
  %2248 : NoneType = prim::Constant()
  %2249 : int[] = prim::Constant[value=[1, 1]]()
  %2250 : bool = prim::Constant[value=0]()
  %2251 : int[] = prim::Constant[value=[0, 0]]()
  %2252 : bool = prim::Constant[value=1]()
  %2253 : Tensor = prim::GetAttr[name="weight"](%441)
  %2254 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_155.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%441)
  %2255 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_154.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%441)
  %quant_input.54 : Tensor = aten::fake_quantize_per_tensor_affine(%2240, %2243, %2244, %2245, %2246)
  %quant_weight.54 : Tensor = aten::fake_quantize_per_channel_affine(%2253, %2242, %2241, %2244, %2245, %2246)
  %input.85 : Tensor = aten::_convolution(%quant_input.54, %quant_weight.54, %2248, %2249, %2249, %2249, %2250, %2251, %2247, %2250, %2250, %2252, %2252)
  %2259 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2260 : float = prim::Constant[value=0.10000000000000001]()
  %2261 : bool = prim::Constant[value=0]()
  %2262 : bool = prim::Constant[value=1]()
  %2263 : Tensor = prim::GetAttr[name="running_var"](%435)
  %2264 : Tensor = prim::GetAttr[name="running_mean"](%435)
  %2265 : Tensor = prim::GetAttr[name="bias"](%435)
  %2266 : Tensor = prim::GetAttr[name="weight"](%435)
  %base_level4_tree1_tree2_bn2.1 : Tensor = aten::batch_norm(%input.85, %2266, %2265, %2264, %2263, %2261, %2260, %2259, %2262)
  %input6.1 : Tensor = aten::add(%base_level4_tree1_tree2_bn2.1, %base_level4_tree1_tree1_relu_output_quant.1, %944)
  %2268 : Tensor = aten::relu_(%input6.1)
  %1142 : Tensor[] = prim::ListConstruct(%2268, %2206)
  %inputs2.1 : Tensor = aten::cat(%1142, %944)
  %2269 : Tensor = prim::Constant[value=<Tensor>]()
  %2270 : Tensor = prim::Constant[value=<Tensor>]()
  %2271 : float = prim::Constant[value=0.050051929443839967]()
  %2272 : int = prim::Constant[value=0]()
  %2273 : int = prim::Constant[value=-128]()
  %2274 : int = prim::Constant[value=127]()
  %2275 : int = prim::Constant[value=1]()
  %2276 : NoneType = prim::Constant()
  %2277 : int[] = prim::Constant[value=[1, 1]]()
  %2278 : int[] = prim::Constant[value=[0, 0]]()
  %2279 : bool = prim::Constant[value=0]()
  %2280 : bool = prim::Constant[value=1]()
  %2281 : Tensor = prim::GetAttr[name="weight"](%429)
  %2282 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_160.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%429)
  %2283 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_159.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%429)
  %quant_input.56 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs2.1, %2271, %2272, %2273, %2274)
  %quant_weight.56 : Tensor = aten::fake_quantize_per_channel_affine(%2281, %2270, %2269, %2272, %2273, %2274)
  %input.87 : Tensor = aten::_convolution(%quant_input.56, %quant_weight.56, %2276, %2277, %2278, %2277, %2279, %2278, %2275, %2279, %2279, %2280, %2280)
  %2287 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2288 : float = prim::Constant[value=0.10000000000000001]()
  %2289 : bool = prim::Constant[value=0]()
  %2290 : bool = prim::Constant[value=1]()
  %2291 : Tensor = prim::GetAttr[name="running_var"](%423)
  %2292 : Tensor = prim::GetAttr[name="running_mean"](%423)
  %2293 : Tensor = prim::GetAttr[name="bias"](%423)
  %2294 : Tensor = prim::GetAttr[name="weight"](%423)
  %input.89 : Tensor = aten::batch_norm(%input.87, %2294, %2293, %2292, %2291, %2289, %2288, %2287, %2290)
  %2296 : Tensor = aten::relu_(%input.89)
  %2297 : float = prim::Constant[value=0.034652248142272468]()
  %2298 : int = prim::Constant[value=0]()
  %2299 : int = prim::Constant[value=-128]()
  %2300 : int = prim::Constant[value=127]()
  %2301 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_164.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%411)
  %base_level4_tree1_root_relu_output_quant.1 : Tensor = aten::fake_quantize_per_tensor_affine(%2296, %2297, %2298, %2299, %2300)
  %2303 : Tensor = prim::Constant[value=<Tensor>]()
  %2304 : Tensor = prim::Constant[value=<Tensor>]()
  %2305 : float = prim::Constant[value=0.034652248142272468]()
  %2306 : int = prim::Constant[value=0]()
  %2307 : int = prim::Constant[value=-128]()
  %2308 : int = prim::Constant[value=127]()
  %2309 : int = prim::Constant[value=1]()
  %2310 : NoneType = prim::Constant()
  %2311 : int[] = prim::Constant[value=[1, 1]]()
  %2312 : bool = prim::Constant[value=0]()
  %2313 : int[] = prim::Constant[value=[0, 0]]()
  %2314 : bool = prim::Constant[value=1]()
  %2315 : Tensor = prim::GetAttr[name="weight"](%405)
  %2316 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_169.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%405)
  %2317 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_168.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%405)
  %quant_input.58 : Tensor = aten::fake_quantize_per_tensor_affine(%2296, %2305, %2306, %2307, %2308)
  %quant_weight.58 : Tensor = aten::fake_quantize_per_channel_affine(%2315, %2304, %2303, %2306, %2307, %2308)
  %input.91 : Tensor = aten::_convolution(%quant_input.58, %quant_weight.58, %2310, %2311, %2311, %2311, %2312, %2313, %2309, %2312, %2312, %2314, %2314)
  %2321 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2322 : float = prim::Constant[value=0.10000000000000001]()
  %2323 : bool = prim::Constant[value=0]()
  %2324 : bool = prim::Constant[value=1]()
  %2325 : Tensor = prim::GetAttr[name="running_var"](%399)
  %2326 : Tensor = prim::GetAttr[name="running_mean"](%399)
  %2327 : Tensor = prim::GetAttr[name="bias"](%399)
  %2328 : Tensor = prim::GetAttr[name="weight"](%399)
  %input.93 : Tensor = aten::batch_norm(%input.91, %2328, %2327, %2326, %2325, %2323, %2322, %2321, %2324)
  %2330 : Tensor = aten::relu_(%input.93)
  %2331 : Tensor = prim::Constant[value=<Tensor>]()
  %2332 : Tensor = prim::Constant[value=<Tensor>]()
  %2333 : float = prim::Constant[value=0.029836707227812037]()
  %2334 : int = prim::Constant[value=0]()
  %2335 : int = prim::Constant[value=-128]()
  %2336 : int = prim::Constant[value=127]()
  %2337 : int = prim::Constant[value=1]()
  %2338 : NoneType = prim::Constant()
  %2339 : int[] = prim::Constant[value=[1, 1]]()
  %2340 : bool = prim::Constant[value=0]()
  %2341 : int[] = prim::Constant[value=[0, 0]]()
  %2342 : bool = prim::Constant[value=1]()
  %2343 : Tensor = prim::GetAttr[name="weight"](%387)
  %2344 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_174.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%387)
  %2345 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_173.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%387)
  %quant_input.60 : Tensor = aten::fake_quantize_per_tensor_affine(%2330, %2333, %2334, %2335, %2336)
  %quant_weight.60 : Tensor = aten::fake_quantize_per_channel_affine(%2343, %2332, %2331, %2334, %2335, %2336)
  %input.95 : Tensor = aten::_convolution(%quant_input.60, %quant_weight.60, %2338, %2339, %2339, %2339, %2340, %2341, %2337, %2340, %2340, %2342, %2342)
  %2349 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2350 : float = prim::Constant[value=0.10000000000000001]()
  %2351 : bool = prim::Constant[value=0]()
  %2352 : bool = prim::Constant[value=1]()
  %2353 : Tensor = prim::GetAttr[name="running_var"](%381)
  %2354 : Tensor = prim::GetAttr[name="running_mean"](%381)
  %2355 : Tensor = prim::GetAttr[name="bias"](%381)
  %2356 : Tensor = prim::GetAttr[name="weight"](%381)
  %base_level4_tree2_tree1_bn2.1 : Tensor = aten::batch_norm(%input.95, %2356, %2355, %2354, %2353, %2351, %2350, %2349, %2352)
  %input7.1 : Tensor = aten::add(%base_level4_tree2_tree1_bn2.1, %base_level4_tree1_root_relu_output_quant.1, %944)
  %2358 : Tensor = aten::relu_(%input7.1)
  %2359 : float = prim::Constant[value=0.048360787038728009]()
  %2360 : int = prim::Constant[value=0]()
  %2361 : int = prim::Constant[value=-128]()
  %2362 : int = prim::Constant[value=127]()
  %2363 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_177.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%375)
  %base_level4_tree2_tree1_relu_output_quant.1 : Tensor = aten::fake_quantize_per_tensor_affine(%2358, %2359, %2360, %2361, %2362)
  %2365 : Tensor = prim::Constant[value=<Tensor>]()
  %2366 : Tensor = prim::Constant[value=<Tensor>]()
  %2367 : float = prim::Constant[value=0.048360787038728009]()
  %2368 : int = prim::Constant[value=0]()
  %2369 : int = prim::Constant[value=-128]()
  %2370 : int = prim::Constant[value=127]()
  %2371 : int = prim::Constant[value=1]()
  %2372 : NoneType = prim::Constant()
  %2373 : int[] = prim::Constant[value=[1, 1]]()
  %2374 : bool = prim::Constant[value=0]()
  %2375 : int[] = prim::Constant[value=[0, 0]]()
  %2376 : bool = prim::Constant[value=1]()
  %2377 : Tensor = prim::GetAttr[name="weight"](%369)
  %2378 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_181.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%369)
  %2379 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_180.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%369)
  %quant_input.62 : Tensor = aten::fake_quantize_per_tensor_affine(%2358, %2367, %2368, %2369, %2370)
  %quant_weight.62 : Tensor = aten::fake_quantize_per_channel_affine(%2377, %2366, %2365, %2368, %2369, %2370)
  %input.97 : Tensor = aten::_convolution(%quant_input.62, %quant_weight.62, %2372, %2373, %2373, %2373, %2374, %2375, %2371, %2374, %2374, %2376, %2376)
  %2383 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2384 : float = prim::Constant[value=0.10000000000000001]()
  %2385 : bool = prim::Constant[value=0]()
  %2386 : bool = prim::Constant[value=1]()
  %2387 : Tensor = prim::GetAttr[name="running_var"](%363)
  %2388 : Tensor = prim::GetAttr[name="running_mean"](%363)
  %2389 : Tensor = prim::GetAttr[name="bias"](%363)
  %2390 : Tensor = prim::GetAttr[name="weight"](%363)
  %input.99 : Tensor = aten::batch_norm(%input.97, %2390, %2389, %2388, %2387, %2385, %2384, %2383, %2386)
  %2392 : Tensor = aten::relu_(%input.99)
  %2393 : Tensor = prim::Constant[value=<Tensor>]()
  %2394 : Tensor = prim::Constant[value=<Tensor>]()
  %2395 : float = prim::Constant[value=0.038877584802822801]()
  %2396 : int = prim::Constant[value=0]()
  %2397 : int = prim::Constant[value=-128]()
  %2398 : int = prim::Constant[value=127]()
  %2399 : int = prim::Constant[value=1]()
  %2400 : NoneType = prim::Constant()
  %2401 : int[] = prim::Constant[value=[1, 1]]()
  %2402 : bool = prim::Constant[value=0]()
  %2403 : int[] = prim::Constant[value=[0, 0]]()
  %2404 : bool = prim::Constant[value=1]()
  %2405 : Tensor = prim::GetAttr[name="weight"](%351)
  %2406 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_186.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%351)
  %2407 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_185.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%351)
  %quant_input.64 : Tensor = aten::fake_quantize_per_tensor_affine(%2392, %2395, %2396, %2397, %2398)
  %quant_weight.64 : Tensor = aten::fake_quantize_per_channel_affine(%2405, %2394, %2393, %2396, %2397, %2398)
  %input.101 : Tensor = aten::_convolution(%quant_input.64, %quant_weight.64, %2400, %2401, %2401, %2401, %2402, %2403, %2399, %2402, %2402, %2404, %2404)
  %2411 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2412 : float = prim::Constant[value=0.10000000000000001]()
  %2413 : bool = prim::Constant[value=0]()
  %2414 : bool = prim::Constant[value=1]()
  %2415 : Tensor = prim::GetAttr[name="running_var"](%345)
  %2416 : Tensor = prim::GetAttr[name="running_mean"](%345)
  %2417 : Tensor = prim::GetAttr[name="bias"](%345)
  %2418 : Tensor = prim::GetAttr[name="weight"](%345)
  %base_level4_tree2_tree2_bn2.1 : Tensor = aten::batch_norm(%input.101, %2418, %2417, %2416, %2415, %2413, %2412, %2411, %2414)
  %input8.1 : Tensor = aten::add(%base_level4_tree2_tree2_bn2.1, %base_level4_tree2_tree1_relu_output_quant.1, %944)
  %2420 : Tensor = aten::relu_(%input8.1)
  %1199 : Tensor[] = prim::ListConstruct(%2420, %2358, %inputs.15, %2296)
  %inputs3.1 : Tensor = aten::cat(%1199, %944)
  %2421 : Tensor = prim::Constant[value=<Tensor>]()
  %2422 : Tensor = prim::Constant[value=<Tensor>]()
  %2423 : float = prim::Constant[value=0.059606807438407357]()
  %2424 : int = prim::Constant[value=0]()
  %2425 : int = prim::Constant[value=-128]()
  %2426 : int = prim::Constant[value=127]()
  %2427 : int = prim::Constant[value=1]()
  %2428 : NoneType = prim::Constant()
  %2429 : int[] = prim::Constant[value=[1, 1]]()
  %2430 : int[] = prim::Constant[value=[0, 0]]()
  %2431 : bool = prim::Constant[value=0]()
  %2432 : bool = prim::Constant[value=1]()
  %2433 : Tensor = prim::GetAttr[name="weight"](%339)
  %2434 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_191.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%339)
  %2435 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_190.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%339)
  %quant_input.66 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs3.1, %2423, %2424, %2425, %2426)
  %quant_weight.66 : Tensor = aten::fake_quantize_per_channel_affine(%2433, %2422, %2421, %2424, %2425, %2426)
  %input.103 : Tensor = aten::_convolution(%quant_input.66, %quant_weight.66, %2428, %2429, %2430, %2429, %2431, %2430, %2427, %2431, %2431, %2432, %2432)
  %2439 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2440 : float = prim::Constant[value=0.10000000000000001]()
  %2441 : bool = prim::Constant[value=0]()
  %2442 : bool = prim::Constant[value=1]()
  %2443 : Tensor = prim::GetAttr[name="running_var"](%333)
  %2444 : Tensor = prim::GetAttr[name="running_mean"](%333)
  %2445 : Tensor = prim::GetAttr[name="bias"](%333)
  %2446 : Tensor = prim::GetAttr[name="weight"](%333)
  %input.105 : Tensor = aten::batch_norm(%input.103, %2446, %2445, %2444, %2443, %2441, %2440, %2439, %2442)
  %2448 : Tensor = aten::relu_(%input.105)
  %2449 : int[] = prim::Constant[value=[2, 2]]()
  %2450 : int[] = prim::Constant[value=[0, 0]]()
  %2451 : int[] = prim::Constant[value=[1, 1]]()
  %2452 : bool = prim::Constant[value=0]()
  %inputs.21 : Tensor = aten::max_pool2d(%2448, %2449, %2449, %2450, %2451, %2452)
  %2454 : Tensor = prim::Constant[value=<Tensor>]()
  %2455 : Tensor = prim::Constant[value=<Tensor>]()
  %2456 : float = prim::Constant[value=0.041764908888208586]()
  %2457 : int = prim::Constant[value=0]()
  %2458 : int = prim::Constant[value=-128]()
  %2459 : int = prim::Constant[value=127]()
  %2460 : int = prim::Constant[value=1]()
  %2461 : NoneType = prim::Constant()
  %2462 : int[] = prim::Constant[value=[1, 1]]()
  %2463 : int[] = prim::Constant[value=[0, 0]]()
  %2464 : bool = prim::Constant[value=0]()
  %2465 : bool = prim::Constant[value=1]()
  %2466 : Tensor = prim::GetAttr[name="weight"](%317)
  %2467 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_200.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%317)
  %2468 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_199.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%317)
  %quant_input.68 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs.21, %2456, %2457, %2458, %2459)
  %quant_weight.68 : Tensor = aten::fake_quantize_per_channel_affine(%2466, %2455, %2454, %2457, %2458, %2459)
  %input.107 : Tensor = aten::_convolution(%quant_input.68, %quant_weight.68, %2461, %2462, %2463, %2462, %2464, %2463, %2460, %2464, %2464, %2465, %2465)
  %2472 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2473 : float = prim::Constant[value=0.10000000000000001]()
  %2474 : bool = prim::Constant[value=0]()
  %2475 : bool = prim::Constant[value=1]()
  %2476 : Tensor = prim::GetAttr[name="running_var"](%312)
  %2477 : Tensor = prim::GetAttr[name="running_mean"](%312)
  %2478 : Tensor = prim::GetAttr[name="bias"](%312)
  %2479 : Tensor = prim::GetAttr[name="weight"](%312)
  %inputs.1 : Tensor = aten::batch_norm(%input.107, %2479, %2478, %2477, %2476, %2474, %2473, %2472, %2475)
  %2481 : float = prim::Constant[value=0.040352085443932241]()
  %2482 : int = prim::Constant[value=0]()
  %2483 : int = prim::Constant[value=-128]()
  %2484 : int = prim::Constant[value=127]()
  %2485 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_203.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%307)
  %base_level5_project_1_output_quant.1 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs.1, %2481, %2482, %2483, %2484)
  %2487 : Tensor = prim::Constant[value=<Tensor>]()
  %2488 : Tensor = prim::Constant[value=<Tensor>]()
  %2489 : float = prim::Constant[value=0.041764908888208586]()
  %2490 : int = prim::Constant[value=0]()
  %2491 : int = prim::Constant[value=-128]()
  %2492 : int = prim::Constant[value=127]()
  %2493 : int = prim::Constant[value=1]()
  %2494 : NoneType = prim::Constant()
  %2495 : int[] = prim::Constant[value=[2, 2]]()
  %2496 : int[] = prim::Constant[value=[1, 1]]()
  %2497 : bool = prim::Constant[value=0]()
  %2498 : int[] = prim::Constant[value=[0, 0]]()
  %2499 : bool = prim::Constant[value=1]()
  %2500 : Tensor = prim::GetAttr[name="weight"](%302)
  %2501 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_207.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%302)
  %2502 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_206.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%302)
  %quant_input.70 : Tensor = aten::fake_quantize_per_tensor_affine(%2448, %2489, %2490, %2491, %2492)
  %quant_weight.70 : Tensor = aten::fake_quantize_per_channel_affine(%2500, %2488, %2487, %2490, %2491, %2492)
  %input.109 : Tensor = aten::_convolution(%quant_input.70, %quant_weight.70, %2494, %2495, %2496, %2496, %2497, %2498, %2493, %2497, %2497, %2499, %2499)
  %2506 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2507 : float = prim::Constant[value=0.10000000000000001]()
  %2508 : bool = prim::Constant[value=0]()
  %2509 : bool = prim::Constant[value=1]()
  %2510 : Tensor = prim::GetAttr[name="running_var"](%297)
  %2511 : Tensor = prim::GetAttr[name="running_mean"](%297)
  %2512 : Tensor = prim::GetAttr[name="bias"](%297)
  %2513 : Tensor = prim::GetAttr[name="weight"](%297)
  %input.111 : Tensor = aten::batch_norm(%input.109, %2513, %2512, %2511, %2510, %2508, %2507, %2506, %2509)
  %2515 : Tensor = aten::relu_(%input.111)
  %2516 : Tensor = prim::Constant[value=<Tensor>]()
  %2517 : Tensor = prim::Constant[value=<Tensor>]()
  %2518 : float = prim::Constant[value=0.024328402646883265]()
  %2519 : int = prim::Constant[value=0]()
  %2520 : int = prim::Constant[value=-128]()
  %2521 : int = prim::Constant[value=127]()
  %2522 : int = prim::Constant[value=1]()
  %2523 : NoneType = prim::Constant()
  %2524 : int[] = prim::Constant[value=[1, 1]]()
  %2525 : bool = prim::Constant[value=0]()
  %2526 : int[] = prim::Constant[value=[0, 0]]()
  %2527 : bool = prim::Constant[value=1]()
  %2528 : Tensor = prim::GetAttr[name="weight"](%287)
  %2529 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_212.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%287)
  %2530 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_211.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%287)
  %quant_input.72 : Tensor = aten::fake_quantize_per_tensor_affine(%2515, %2518, %2519, %2520, %2521)
  %quant_weight.72 : Tensor = aten::fake_quantize_per_channel_affine(%2528, %2517, %2516, %2519, %2520, %2521)
  %input.113 : Tensor = aten::_convolution(%quant_input.72, %quant_weight.72, %2523, %2524, %2524, %2524, %2525, %2526, %2522, %2525, %2525, %2527, %2527)
  %2534 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2535 : float = prim::Constant[value=0.10000000000000001]()
  %2536 : bool = prim::Constant[value=0]()
  %2537 : bool = prim::Constant[value=1]()
  %2538 : Tensor = prim::GetAttr[name="running_var"](%282)
  %2539 : Tensor = prim::GetAttr[name="running_mean"](%282)
  %2540 : Tensor = prim::GetAttr[name="bias"](%282)
  %2541 : Tensor = prim::GetAttr[name="weight"](%282)
  %base_level5_tree1_bn2.1 : Tensor = aten::batch_norm(%input.113, %2541, %2540, %2539, %2538, %2536, %2535, %2534, %2537)
  %input9.1 : Tensor = aten::add(%base_level5_tree1_bn2.1, %base_level5_project_1_output_quant.1, %944)
  %2543 : Tensor = aten::relu_(%input9.1)
  %2544 : float = prim::Constant[value=0.039226884917011408]()
  %2545 : int = prim::Constant[value=0]()
  %2546 : int = prim::Constant[value=-128]()
  %2547 : int = prim::Constant[value=127]()
  %2548 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_215.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%277)
  %base_level5_tree1_relu_output_quant.1 : Tensor = aten::fake_quantize_per_tensor_affine(%2543, %2544, %2545, %2546, %2547)
  %2550 : Tensor = prim::Constant[value=<Tensor>]()
  %2551 : Tensor = prim::Constant[value=<Tensor>]()
  %2552 : float = prim::Constant[value=0.039226884917011408]()
  %2553 : int = prim::Constant[value=0]()
  %2554 : int = prim::Constant[value=-128]()
  %2555 : int = prim::Constant[value=127]()
  %2556 : int = prim::Constant[value=1]()
  %2557 : NoneType = prim::Constant()
  %2558 : int[] = prim::Constant[value=[1, 1]]()
  %2559 : bool = prim::Constant[value=0]()
  %2560 : int[] = prim::Constant[value=[0, 0]]()
  %2561 : bool = prim::Constant[value=1]()
  %2562 : Tensor = prim::GetAttr[name="weight"](%272)
  %2563 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_219.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%272)
  %2564 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_218.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%272)
  %quant_input.74 : Tensor = aten::fake_quantize_per_tensor_affine(%2543, %2552, %2553, %2554, %2555)
  %quant_weight.74 : Tensor = aten::fake_quantize_per_channel_affine(%2562, %2551, %2550, %2553, %2554, %2555)
  %input.115 : Tensor = aten::_convolution(%quant_input.74, %quant_weight.74, %2557, %2558, %2558, %2558, %2559, %2560, %2556, %2559, %2559, %2561, %2561)
  %2568 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2569 : float = prim::Constant[value=0.10000000000000001]()
  %2570 : bool = prim::Constant[value=0]()
  %2571 : bool = prim::Constant[value=1]()
  %2572 : Tensor = prim::GetAttr[name="running_var"](%267)
  %2573 : Tensor = prim::GetAttr[name="running_mean"](%267)
  %2574 : Tensor = prim::GetAttr[name="bias"](%267)
  %2575 : Tensor = prim::GetAttr[name="weight"](%267)
  %input.117 : Tensor = aten::batch_norm(%input.115, %2575, %2574, %2573, %2572, %2570, %2569, %2568, %2571)
  %2577 : Tensor = aten::relu_(%input.117)
  %2578 : Tensor = prim::Constant[value=<Tensor>]()
  %2579 : Tensor = prim::Constant[value=<Tensor>]()
  %2580 : float = prim::Constant[value=0.032877809419406684]()
  %2581 : int = prim::Constant[value=0]()
  %2582 : int = prim::Constant[value=-128]()
  %2583 : int = prim::Constant[value=127]()
  %2584 : int = prim::Constant[value=1]()
  %2585 : NoneType = prim::Constant()
  %2586 : int[] = prim::Constant[value=[1, 1]]()
  %2587 : bool = prim::Constant[value=0]()
  %2588 : int[] = prim::Constant[value=[0, 0]]()
  %2589 : bool = prim::Constant[value=1]()
  %2590 : Tensor = prim::GetAttr[name="weight"](%257)
  %2591 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_224.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%257)
  %2592 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_223.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%257)
  %quant_input.76 : Tensor = aten::fake_quantize_per_tensor_affine(%2577, %2580, %2581, %2582, %2583)
  %quant_weight.76 : Tensor = aten::fake_quantize_per_channel_affine(%2590, %2579, %2578, %2581, %2582, %2583)
  %input.119 : Tensor = aten::_convolution(%quant_input.76, %quant_weight.76, %2585, %2586, %2586, %2586, %2587, %2588, %2584, %2587, %2587, %2589, %2589)
  %2596 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2597 : float = prim::Constant[value=0.10000000000000001]()
  %2598 : bool = prim::Constant[value=0]()
  %2599 : bool = prim::Constant[value=1]()
  %2600 : Tensor = prim::GetAttr[name="running_var"](%252)
  %2601 : Tensor = prim::GetAttr[name="running_mean"](%252)
  %2602 : Tensor = prim::GetAttr[name="bias"](%252)
  %2603 : Tensor = prim::GetAttr[name="weight"](%252)
  %base_level5_tree2_bn2.1 : Tensor = aten::batch_norm(%input.119, %2603, %2602, %2601, %2600, %2598, %2597, %2596, %2599)
  %input10.1 : Tensor = aten::add(%base_level5_tree2_bn2.1, %base_level5_tree1_relu_output_quant.1, %944)
  %2605 : Tensor = aten::relu_(%input10.1)
  %1264 : Tensor[] = prim::ListConstruct(%2605, %2543, %inputs.21)
  %inputs4.1 : Tensor = aten::cat(%1264, %944)
  %2606 : Tensor = prim::Constant[value=<Tensor>]()
  %2607 : Tensor = prim::Constant[value=<Tensor>]()
  %2608 : float = prim::Constant[value=0.081878324193278632]()
  %2609 : int = prim::Constant[value=0]()
  %2610 : int = prim::Constant[value=-128]()
  %2611 : int = prim::Constant[value=127]()
  %2612 : int = prim::Constant[value=1]()
  %2613 : NoneType = prim::Constant()
  %2614 : int[] = prim::Constant[value=[1, 1]]()
  %2615 : int[] = prim::Constant[value=[0, 0]]()
  %2616 : bool = prim::Constant[value=0]()
  %2617 : bool = prim::Constant[value=1]()
  %2618 : Tensor = prim::GetAttr[name="weight"](%247)
  %2619 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_229.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%247)
  %2620 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_228.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%247)
  %quant_input.78 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs4.1, %2608, %2609, %2610, %2611)
  %quant_weight.78 : Tensor = aten::fake_quantize_per_channel_affine(%2618, %2607, %2606, %2609, %2610, %2611)
  %input.121 : Tensor = aten::_convolution(%quant_input.78, %quant_weight.78, %2613, %2614, %2615, %2614, %2616, %2615, %2612, %2616, %2616, %2617, %2617)
  %2624 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2625 : float = prim::Constant[value=0.10000000000000001]()
  %2626 : bool = prim::Constant[value=0]()
  %2627 : bool = prim::Constant[value=1]()
  %2628 : Tensor = prim::GetAttr[name="running_var"](%242)
  %2629 : Tensor = prim::GetAttr[name="running_mean"](%242)
  %2630 : Tensor = prim::GetAttr[name="bias"](%242)
  %2631 : Tensor = prim::GetAttr[name="weight"](%242)
  %input.123 : Tensor = aten::batch_norm(%input.121, %2631, %2630, %2629, %2628, %2626, %2625, %2624, %2627)
  %2633 : Tensor = aten::relu_(%input.123)
  %2634 : Tensor = prim::Constant[value=<Tensor>]()
  %2635 : Tensor = prim::Constant[value=<Tensor>]()
  %2636 : float = prim::Constant[value=0.28054608322503999]()
  %2637 : int = prim::Constant[value=0]()
  %2638 : int = prim::Constant[value=-128]()
  %2639 : int = prim::Constant[value=127]()
  %2640 : int = prim::Constant[value=1]()
  %2641 : NoneType = prim::Constant()
  %2642 : int[] = prim::Constant[value=[1, 1]]()
  %2643 : int[] = prim::Constant[value=[0, 0]]()
  %2644 : bool = prim::Constant[value=0]()
  %2645 : bool = prim::Constant[value=1]()
  %2646 : Tensor = prim::GetAttr[name="weight"](%232)
  %2647 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_237.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%232)
  %2648 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_236.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%232)
  %quant_input.80 : Tensor = aten::fake_quantize_per_tensor_affine(%2633, %2636, %2637, %2638, %2639)
  %quant_weight.80 : Tensor = aten::fake_quantize_per_channel_affine(%2646, %2635, %2634, %2637, %2638, %2639)
  %input.125 : Tensor = aten::_convolution(%quant_input.80, %quant_weight.80, %2641, %2642, %2643, %2642, %2644, %2643, %2640, %2644, %2644, %2645, %2645)
  %2652 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2653 : float = prim::Constant[value=0.10000000000000001]()
  %2654 : bool = prim::Constant[value=0]()
  %2655 : bool = prim::Constant[value=1]()
  %2656 : Tensor = prim::GetAttr[name="running_var"](%227)
  %2657 : Tensor = prim::GetAttr[name="running_mean"](%227)
  %2658 : Tensor = prim::GetAttr[name="bias"](%227)
  %2659 : Tensor = prim::GetAttr[name="weight"](%227)
  %input.127 : Tensor = aten::batch_norm(%input.125, %2659, %2658, %2657, %2656, %2654, %2653, %2652, %2655)
  %2661 : Tensor = aten::relu_(%input.127)
  %2662 : int = prim::Constant[value=256]()
  %2663 : NoneType = prim::Constant()
  %2664 : int[] = prim::Constant[value=[2, 2]]()
  %2665 : int[] = prim::Constant[value=[1, 1]]()
  %2666 : bool = prim::Constant[value=1]()
  %2667 : int[] = prim::Constant[value=[0, 0]]()
  %2668 : bool = prim::Constant[value=0]()
  %2669 : Tensor = prim::GetAttr[name="weight"](%217)
  %dla_up_ida_0_up_1.1 : Tensor = aten::_convolution(%2661, %2669, %2663, %2664, %2665, %2665, %2666, %2667, %2662, %2668, %2668, %2666, %2666)
  %1286 : Tensor[] = prim::ListConstruct(%2448, %dla_up_ida_0_up_1.1)
  %inputs5.1 : Tensor = aten::cat(%1286, %944)
  %2671 : Tensor = prim::Constant[value=<Tensor>]()
  %2672 : Tensor = prim::Constant[value=<Tensor>]()
  %2673 : float = prim::Constant[value=0.17380840571846548]()
  %2674 : int = prim::Constant[value=0]()
  %2675 : int = prim::Constant[value=-128]()
  %2676 : int = prim::Constant[value=127]()
  %2677 : int = prim::Constant[value=1]()
  %2678 : NoneType = prim::Constant()
  %2679 : int[] = prim::Constant[value=[1, 1]]()
  %2680 : bool = prim::Constant[value=0]()
  %2681 : int[] = prim::Constant[value=[0, 0]]()
  %2682 : bool = prim::Constant[value=1]()
  %2683 : Tensor = prim::GetAttr[name="weight"](%213)
  %2684 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_243.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%213)
  %2685 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_242.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%213)
  %quant_input.82 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs5.1, %2673, %2674, %2675, %2676)
  %quant_weight.82 : Tensor = aten::fake_quantize_per_channel_affine(%2683, %2672, %2671, %2674, %2675, %2676)
  %input.129 : Tensor = aten::_convolution(%quant_input.82, %quant_weight.82, %2678, %2679, %2679, %2679, %2680, %2681, %2677, %2680, %2680, %2682, %2682)
  %2689 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2690 : float = prim::Constant[value=0.10000000000000001]()
  %2691 : bool = prim::Constant[value=0]()
  %2692 : bool = prim::Constant[value=1]()
  %2693 : Tensor = prim::GetAttr[name="running_var"](%208)
  %2694 : Tensor = prim::GetAttr[name="running_mean"](%208)
  %2695 : Tensor = prim::GetAttr[name="bias"](%208)
  %2696 : Tensor = prim::GetAttr[name="weight"](%208)
  %input.131 : Tensor = aten::batch_norm(%input.129, %2696, %2695, %2694, %2693, %2691, %2690, %2689, %2692)
  %2698 : Tensor = aten::relu_(%input.131)
  %2699 : Tensor = prim::Constant[value=<Tensor>]()
  %2700 : Tensor = prim::Constant[value=<Tensor>]()
  %2701 : float = prim::Constant[value=0.041764908888208586]()
  %2702 : int = prim::Constant[value=0]()
  %2703 : int = prim::Constant[value=-128]()
  %2704 : int = prim::Constant[value=127]()
  %2705 : int = prim::Constant[value=1]()
  %2706 : NoneType = prim::Constant()
  %2707 : int[] = prim::Constant[value=[1, 1]]()
  %2708 : int[] = prim::Constant[value=[0, 0]]()
  %2709 : bool = prim::Constant[value=0]()
  %2710 : bool = prim::Constant[value=1]()
  %2711 : Tensor = prim::GetAttr[name="weight"](%198)
  %2712 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_250.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%198)
  %2713 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_249.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%198)
  %quant_input.84 : Tensor = aten::fake_quantize_per_tensor_affine(%2448, %2701, %2702, %2703, %2704)
  %quant_weight.84 : Tensor = aten::fake_quantize_per_channel_affine(%2711, %2700, %2699, %2702, %2703, %2704)
  %input.133 : Tensor = aten::_convolution(%quant_input.84, %quant_weight.84, %2706, %2707, %2708, %2707, %2709, %2708, %2705, %2709, %2709, %2710, %2710)
  %2717 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2718 : float = prim::Constant[value=0.10000000000000001]()
  %2719 : bool = prim::Constant[value=0]()
  %2720 : bool = prim::Constant[value=1]()
  %2721 : Tensor = prim::GetAttr[name="running_var"](%193)
  %2722 : Tensor = prim::GetAttr[name="running_mean"](%193)
  %2723 : Tensor = prim::GetAttr[name="bias"](%193)
  %2724 : Tensor = prim::GetAttr[name="weight"](%193)
  %input.135 : Tensor = aten::batch_norm(%input.133, %2724, %2723, %2722, %2721, %2719, %2718, %2717, %2720)
  %2726 : Tensor = aten::relu_(%input.135)
  %2727 : int = prim::Constant[value=128]()
  %2728 : NoneType = prim::Constant()
  %2729 : int[] = prim::Constant[value=[2, 2]]()
  %2730 : int[] = prim::Constant[value=[1, 1]]()
  %2731 : bool = prim::Constant[value=1]()
  %2732 : int[] = prim::Constant[value=[0, 0]]()
  %2733 : bool = prim::Constant[value=0]()
  %2734 : Tensor = prim::GetAttr[name="weight"](%183)
  %dla_up_ida_1_up_1.1 : Tensor = aten::_convolution(%2726, %2734, %2728, %2729, %2730, %2730, %2731, %2732, %2727, %2733, %2733, %2731, %2731)
  %2736 : Tensor = prim::Constant[value=<Tensor>]()
  %2737 : Tensor = prim::Constant[value=<Tensor>]()
  %2738 : float = prim::Constant[value=0.12328381425752415]()
  %2739 : int = prim::Constant[value=0]()
  %2740 : int = prim::Constant[value=-128]()
  %2741 : int = prim::Constant[value=127]()
  %2742 : int = prim::Constant[value=1]()
  %2743 : NoneType = prim::Constant()
  %2744 : int[] = prim::Constant[value=[1, 1]]()
  %2745 : int[] = prim::Constant[value=[0, 0]]()
  %2746 : bool = prim::Constant[value=0]()
  %2747 : bool = prim::Constant[value=1]()
  %2748 : Tensor = prim::GetAttr[name="weight"](%179)
  %2749 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_257.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%179)
  %2750 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_256.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%179)
  %quant_input.86 : Tensor = aten::fake_quantize_per_tensor_affine(%2698, %2738, %2739, %2740, %2741)
  %quant_weight.86 : Tensor = aten::fake_quantize_per_channel_affine(%2748, %2737, %2736, %2739, %2740, %2741)
  %input.137 : Tensor = aten::_convolution(%quant_input.86, %quant_weight.86, %2743, %2744, %2745, %2744, %2746, %2745, %2742, %2746, %2746, %2747, %2747)
  %2754 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2755 : float = prim::Constant[value=0.10000000000000001]()
  %2756 : bool = prim::Constant[value=0]()
  %2757 : bool = prim::Constant[value=1]()
  %2758 : Tensor = prim::GetAttr[name="running_var"](%174)
  %2759 : Tensor = prim::GetAttr[name="running_mean"](%174)
  %2760 : Tensor = prim::GetAttr[name="bias"](%174)
  %2761 : Tensor = prim::GetAttr[name="weight"](%174)
  %input.139 : Tensor = aten::batch_norm(%input.137, %2761, %2760, %2759, %2758, %2756, %2755, %2754, %2757)
  %2763 : Tensor = aten::relu_(%input.139)
  %2764 : int = prim::Constant[value=128]()
  %2765 : NoneType = prim::Constant()
  %2766 : int[] = prim::Constant[value=[2, 2]]()
  %2767 : int[] = prim::Constant[value=[1, 1]]()
  %2768 : bool = prim::Constant[value=1]()
  %2769 : int[] = prim::Constant[value=[0, 0]]()
  %2770 : bool = prim::Constant[value=0]()
  %2771 : Tensor = prim::GetAttr[name="weight"](%164)
  %dla_up_ida_1_up_2.1 : Tensor = aten::_convolution(%2763, %2771, %2765, %2766, %2767, %2767, %2768, %2769, %2764, %2770, %2770, %2768, %2768)
  %1318 : Tensor[] = prim::ListConstruct(%2087, %dla_up_ida_1_up_1.1)
  %inputs6.1 : Tensor = aten::cat(%1318, %944)
  %2773 : Tensor = prim::Constant[value=<Tensor>]()
  %2774 : Tensor = prim::Constant[value=<Tensor>]()
  %2775 : float = prim::Constant[value=0.13152006855161172]()
  %2776 : int = prim::Constant[value=0]()
  %2777 : int = prim::Constant[value=-128]()
  %2778 : int = prim::Constant[value=127]()
  %2779 : int = prim::Constant[value=1]()
  %2780 : NoneType = prim::Constant()
  %2781 : int[] = prim::Constant[value=[1, 1]]()
  %2782 : bool = prim::Constant[value=0]()
  %2783 : int[] = prim::Constant[value=[0, 0]]()
  %2784 : bool = prim::Constant[value=1]()
  %2785 : Tensor = prim::GetAttr[name="weight"](%160)
  %2786 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_264.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%160)
  %2787 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_263.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%160)
  %quant_input.88 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs6.1, %2775, %2776, %2777, %2778)
  %quant_weight.88 : Tensor = aten::fake_quantize_per_channel_affine(%2785, %2774, %2773, %2776, %2777, %2778)
  %input.141 : Tensor = aten::_convolution(%quant_input.88, %quant_weight.88, %2780, %2781, %2781, %2781, %2782, %2783, %2779, %2782, %2782, %2784, %2784)
  %2791 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2792 : float = prim::Constant[value=0.10000000000000001]()
  %2793 : bool = prim::Constant[value=0]()
  %2794 : bool = prim::Constant[value=1]()
  %2795 : Tensor = prim::GetAttr[name="running_var"](%155)
  %2796 : Tensor = prim::GetAttr[name="running_mean"](%155)
  %2797 : Tensor = prim::GetAttr[name="bias"](%155)
  %2798 : Tensor = prim::GetAttr[name="weight"](%155)
  %input.143 : Tensor = aten::batch_norm(%input.141, %2798, %2797, %2796, %2795, %2793, %2792, %2791, %2794)
  %2800 : Tensor = aten::relu_(%input.143)
  %1330 : Tensor[] = prim::ListConstruct(%2800, %dla_up_ida_1_up_2.1)
  %inputs7.1 : Tensor = aten::cat(%1330, %944)
  %2801 : Tensor = prim::Constant[value=<Tensor>]()
  %2802 : Tensor = prim::Constant[value=<Tensor>]()
  %2803 : float = prim::Constant[value=0.23995789955920122]()
  %2804 : int = prim::Constant[value=0]()
  %2805 : int = prim::Constant[value=-128]()
  %2806 : int = prim::Constant[value=127]()
  %2807 : int = prim::Constant[value=1]()
  %2808 : NoneType = prim::Constant()
  %2809 : int[] = prim::Constant[value=[1, 1]]()
  %2810 : bool = prim::Constant[value=0]()
  %2811 : int[] = prim::Constant[value=[0, 0]]()
  %2812 : bool = prim::Constant[value=1]()
  %2813 : Tensor = prim::GetAttr[name="weight"](%145)
  %2814 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_270.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%145)
  %2815 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_269.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%145)
  %quant_input.90 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs7.1, %2803, %2804, %2805, %2806)
  %quant_weight.90 : Tensor = aten::fake_quantize_per_channel_affine(%2813, %2802, %2801, %2804, %2805, %2806)
  %input.145 : Tensor = aten::_convolution(%quant_input.90, %quant_weight.90, %2808, %2809, %2809, %2809, %2810, %2811, %2807, %2810, %2810, %2812, %2812)
  %2819 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2820 : float = prim::Constant[value=0.10000000000000001]()
  %2821 : bool = prim::Constant[value=0]()
  %2822 : bool = prim::Constant[value=1]()
  %2823 : Tensor = prim::GetAttr[name="running_var"](%140)
  %2824 : Tensor = prim::GetAttr[name="running_mean"](%140)
  %2825 : Tensor = prim::GetAttr[name="bias"](%140)
  %2826 : Tensor = prim::GetAttr[name="weight"](%140)
  %input.147 : Tensor = aten::batch_norm(%input.145, %2826, %2825, %2824, %2823, %2821, %2820, %2819, %2822)
  %2828 : Tensor = aten::relu_(%input.147)
  %2829 : Tensor = prim::Constant[value=<Tensor>]()
  %2830 : Tensor = prim::Constant[value=<Tensor>]()
  %2831 : float = prim::Constant[value=0.046685489143912247]()
  %2832 : int = prim::Constant[value=0]()
  %2833 : int = prim::Constant[value=-128]()
  %2834 : int = prim::Constant[value=127]()
  %2835 : int = prim::Constant[value=1]()
  %2836 : NoneType = prim::Constant()
  %2837 : int[] = prim::Constant[value=[1, 1]]()
  %2838 : int[] = prim::Constant[value=[0, 0]]()
  %2839 : bool = prim::Constant[value=0]()
  %2840 : bool = prim::Constant[value=1]()
  %2841 : Tensor = prim::GetAttr[name="weight"](%130)
  %2842 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_277.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%130)
  %2843 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_276.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%130)
  %quant_input.92 : Tensor = aten::fake_quantize_per_tensor_affine(%2087, %2831, %2832, %2833, %2834)
  %quant_weight.92 : Tensor = aten::fake_quantize_per_channel_affine(%2841, %2830, %2829, %2832, %2833, %2834)
  %input.149 : Tensor = aten::_convolution(%quant_input.92, %quant_weight.92, %2836, %2837, %2838, %2837, %2839, %2838, %2835, %2839, %2839, %2840, %2840)
  %2847 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2848 : float = prim::Constant[value=0.10000000000000001]()
  %2849 : bool = prim::Constant[value=0]()
  %2850 : bool = prim::Constant[value=1]()
  %2851 : Tensor = prim::GetAttr[name="running_var"](%125)
  %2852 : Tensor = prim::GetAttr[name="running_mean"](%125)
  %2853 : Tensor = prim::GetAttr[name="bias"](%125)
  %2854 : Tensor = prim::GetAttr[name="weight"](%125)
  %input.151 : Tensor = aten::batch_norm(%input.149, %2854, %2853, %2852, %2851, %2849, %2848, %2847, %2850)
  %2856 : Tensor = aten::relu_(%input.151)
  %2857 : int = prim::Constant[value=64]()
  %2858 : NoneType = prim::Constant()
  %2859 : int[] = prim::Constant[value=[2, 2]]()
  %2860 : int[] = prim::Constant[value=[1, 1]]()
  %2861 : bool = prim::Constant[value=1]()
  %2862 : int[] = prim::Constant[value=[0, 0]]()
  %2863 : bool = prim::Constant[value=0]()
  %2864 : Tensor = prim::GetAttr[name="weight"](%115)
  %dla_up_ida_2_up_1.1 : Tensor = aten::_convolution(%2856, %2864, %2858, %2859, %2860, %2860, %2861, %2862, %2857, %2863, %2863, %2861, %2861)
  %2866 : Tensor = prim::Constant[value=<Tensor>]()
  %2867 : Tensor = prim::Constant[value=<Tensor>]()
  %2868 : float = prim::Constant[value=0.12479710766649622]()
  %2869 : int = prim::Constant[value=0]()
  %2870 : int = prim::Constant[value=-128]()
  %2871 : int = prim::Constant[value=127]()
  %2872 : int = prim::Constant[value=1]()
  %2873 : NoneType = prim::Constant()
  %2874 : int[] = prim::Constant[value=[1, 1]]()
  %2875 : int[] = prim::Constant[value=[0, 0]]()
  %2876 : bool = prim::Constant[value=0]()
  %2877 : bool = prim::Constant[value=1]()
  %2878 : Tensor = prim::GetAttr[name="weight"](%111)
  %2879 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_284.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%111)
  %2880 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_283.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%111)
  %quant_input.94 : Tensor = aten::fake_quantize_per_tensor_affine(%2800, %2868, %2869, %2870, %2871)
  %quant_weight.94 : Tensor = aten::fake_quantize_per_channel_affine(%2878, %2867, %2866, %2869, %2870, %2871)
  %input.153 : Tensor = aten::_convolution(%quant_input.94, %quant_weight.94, %2873, %2874, %2875, %2874, %2876, %2875, %2872, %2876, %2876, %2877, %2877)
  %2884 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2885 : float = prim::Constant[value=0.10000000000000001]()
  %2886 : bool = prim::Constant[value=0]()
  %2887 : bool = prim::Constant[value=1]()
  %2888 : Tensor = prim::GetAttr[name="running_var"](%106)
  %2889 : Tensor = prim::GetAttr[name="running_mean"](%106)
  %2890 : Tensor = prim::GetAttr[name="bias"](%106)
  %2891 : Tensor = prim::GetAttr[name="weight"](%106)
  %input.155 : Tensor = aten::batch_norm(%input.153, %2891, %2890, %2889, %2888, %2886, %2885, %2884, %2887)
  %2893 : Tensor = aten::relu_(%input.155)
  %2894 : int = prim::Constant[value=64]()
  %2895 : NoneType = prim::Constant()
  %2896 : int[] = prim::Constant[value=[2, 2]]()
  %2897 : int[] = prim::Constant[value=[1, 1]]()
  %2898 : bool = prim::Constant[value=1]()
  %2899 : int[] = prim::Constant[value=[0, 0]]()
  %2900 : bool = prim::Constant[value=0]()
  %2901 : Tensor = prim::GetAttr[name="weight"](%96)
  %dla_up_ida_2_up_2.1 : Tensor = aten::_convolution(%2893, %2901, %2895, %2896, %2897, %2897, %2898, %2899, %2894, %2900, %2900, %2898, %2898)
  %2903 : Tensor = prim::Constant[value=<Tensor>]()
  %2904 : Tensor = prim::Constant[value=<Tensor>]()
  %2905 : float = prim::Constant[value=0.12283295909250815]()
  %2906 : int = prim::Constant[value=0]()
  %2907 : int = prim::Constant[value=-128]()
  %2908 : int = prim::Constant[value=127]()
  %2909 : int = prim::Constant[value=1]()
  %2910 : NoneType = prim::Constant()
  %2911 : int[] = prim::Constant[value=[1, 1]]()
  %2912 : int[] = prim::Constant[value=[0, 0]]()
  %2913 : bool = prim::Constant[value=0]()
  %2914 : bool = prim::Constant[value=1]()
  %2915 : Tensor = prim::GetAttr[name="weight"](%92)
  %2916 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_291.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%92)
  %2917 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_290.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%92)
  %quant_input.96 : Tensor = aten::fake_quantize_per_tensor_affine(%2828, %2905, %2906, %2907, %2908)
  %quant_weight.96 : Tensor = aten::fake_quantize_per_channel_affine(%2915, %2904, %2903, %2906, %2907, %2908)
  %input.157 : Tensor = aten::_convolution(%quant_input.96, %quant_weight.96, %2910, %2911, %2912, %2911, %2913, %2912, %2909, %2913, %2913, %2914, %2914)
  %2921 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2922 : float = prim::Constant[value=0.10000000000000001]()
  %2923 : bool = prim::Constant[value=0]()
  %2924 : bool = prim::Constant[value=1]()
  %2925 : Tensor = prim::GetAttr[name="running_var"](%87)
  %2926 : Tensor = prim::GetAttr[name="running_mean"](%87)
  %2927 : Tensor = prim::GetAttr[name="bias"](%87)
  %2928 : Tensor = prim::GetAttr[name="weight"](%87)
  %input.159 : Tensor = aten::batch_norm(%input.157, %2928, %2927, %2926, %2925, %2923, %2922, %2921, %2924)
  %2930 : Tensor = aten::relu_(%input.159)
  %2931 : int = prim::Constant[value=64]()
  %2932 : NoneType = prim::Constant()
  %2933 : int[] = prim::Constant[value=[2, 2]]()
  %2934 : int[] = prim::Constant[value=[1, 1]]()
  %2935 : bool = prim::Constant[value=1]()
  %2936 : int[] = prim::Constant[value=[0, 0]]()
  %2937 : bool = prim::Constant[value=0]()
  %2938 : Tensor = prim::GetAttr[name="weight"](%77)
  %dla_up_ida_2_up_3.1 : Tensor = aten::_convolution(%2930, %2938, %2932, %2933, %2934, %2934, %2935, %2936, %2931, %2937, %2937, %2935, %2935)
  %1372 : Tensor[] = prim::ListConstruct(%1726, %dla_up_ida_2_up_1.1)
  %inputs8.1 : Tensor = aten::cat(%1372, %944)
  %2940 : Tensor = prim::Constant[value=<Tensor>]()
  %2941 : Tensor = prim::Constant[value=<Tensor>]()
  %2942 : float = prim::Constant[value=0.072918140982079688]()
  %2943 : int = prim::Constant[value=0]()
  %2944 : int = prim::Constant[value=-128]()
  %2945 : int = prim::Constant[value=127]()
  %2946 : int = prim::Constant[value=1]()
  %2947 : NoneType = prim::Constant()
  %2948 : int[] = prim::Constant[value=[1, 1]]()
  %2949 : bool = prim::Constant[value=0]()
  %2950 : int[] = prim::Constant[value=[0, 0]]()
  %2951 : bool = prim::Constant[value=1]()
  %2952 : Tensor = prim::GetAttr[name="weight"](%73)
  %2953 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_298.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%73)
  %2954 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_297.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%73)
  %quant_input.98 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs8.1, %2942, %2943, %2944, %2945)
  %quant_weight.98 : Tensor = aten::fake_quantize_per_channel_affine(%2952, %2941, %2940, %2943, %2944, %2945)
  %input.161 : Tensor = aten::_convolution(%quant_input.98, %quant_weight.98, %2947, %2948, %2948, %2948, %2949, %2950, %2946, %2949, %2949, %2951, %2951)
  %2958 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2959 : float = prim::Constant[value=0.10000000000000001]()
  %2960 : bool = prim::Constant[value=0]()
  %2961 : bool = prim::Constant[value=1]()
  %2962 : Tensor = prim::GetAttr[name="running_var"](%68)
  %2963 : Tensor = prim::GetAttr[name="running_mean"](%68)
  %2964 : Tensor = prim::GetAttr[name="bias"](%68)
  %2965 : Tensor = prim::GetAttr[name="weight"](%68)
  %input.163 : Tensor = aten::batch_norm(%input.161, %2965, %2964, %2963, %2962, %2960, %2959, %2958, %2961)
  %2967 : Tensor = aten::relu_(%input.163)
  %1383 : Tensor[] = prim::ListConstruct(%2967, %dla_up_ida_2_up_2.1)
  %inputs9.1 : Tensor = aten::cat(%1383, %944)
  %2968 : Tensor = prim::Constant[value=<Tensor>]()
  %2969 : Tensor = prim::Constant[value=<Tensor>]()
  %2970 : float = prim::Constant[value=0.16638957046148345]()
  %2971 : int = prim::Constant[value=0]()
  %2972 : int = prim::Constant[value=-128]()
  %2973 : int = prim::Constant[value=127]()
  %2974 : int = prim::Constant[value=1]()
  %2975 : NoneType = prim::Constant()
  %2976 : int[] = prim::Constant[value=[1, 1]]()
  %2977 : bool = prim::Constant[value=0]()
  %2978 : int[] = prim::Constant[value=[0, 0]]()
  %2979 : bool = prim::Constant[value=1]()
  %2980 : Tensor = prim::GetAttr[name="weight"](%58)
  %2981 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_304.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%58)
  %2982 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_303.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%58)
  %quant_input.100 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs9.1, %2970, %2971, %2972, %2973)
  %quant_weight.100 : Tensor = aten::fake_quantize_per_channel_affine(%2980, %2969, %2968, %2971, %2972, %2973)
  %input.165 : Tensor = aten::_convolution(%quant_input.100, %quant_weight.100, %2975, %2976, %2976, %2976, %2977, %2978, %2974, %2977, %2977, %2979, %2979)
  %2986 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2987 : float = prim::Constant[value=0.10000000000000001]()
  %2988 : bool = prim::Constant[value=0]()
  %2989 : bool = prim::Constant[value=1]()
  %2990 : Tensor = prim::GetAttr[name="running_var"](%53)
  %2991 : Tensor = prim::GetAttr[name="running_mean"](%53)
  %2992 : Tensor = prim::GetAttr[name="bias"](%53)
  %2993 : Tensor = prim::GetAttr[name="weight"](%53)
  %input.167 : Tensor = aten::batch_norm(%input.165, %2993, %2992, %2991, %2990, %2988, %2987, %2986, %2989)
  %2995 : Tensor = aten::relu_(%input.167)
  %1394 : Tensor[] = prim::ListConstruct(%2995, %dla_up_ida_2_up_3.1)
  %inputs10.1 : Tensor = aten::cat(%1394, %944)
  %2996 : Tensor = prim::Constant[value=<Tensor>]()
  %2997 : Tensor = prim::Constant[value=<Tensor>]()
  %2998 : float = prim::Constant[value=0.11740763356366496]()
  %2999 : int = prim::Constant[value=0]()
  %3000 : int = prim::Constant[value=-128]()
  %3001 : int = prim::Constant[value=127]()
  %3002 : int = prim::Constant[value=1]()
  %3003 : NoneType = prim::Constant()
  %3004 : int[] = prim::Constant[value=[1, 1]]()
  %3005 : bool = prim::Constant[value=0]()
  %3006 : int[] = prim::Constant[value=[0, 0]]()
  %3007 : bool = prim::Constant[value=1]()
  %3008 : Tensor = prim::GetAttr[name="weight"](%43)
  %3009 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_310.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%43)
  %3010 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_309.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%43)
  %quant_input.102 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs10.1, %2998, %2999, %3000, %3001)
  %quant_weight.102 : Tensor = aten::fake_quantize_per_channel_affine(%3008, %2997, %2996, %2999, %3000, %3001)
  %input.169 : Tensor = aten::_convolution(%quant_input.102, %quant_weight.102, %3003, %3004, %3004, %3004, %3005, %3006, %3002, %3005, %3005, %3007, %3007)
  %3014 : float = prim::Constant[value=1.0000000000000001e-05]()
  %3015 : float = prim::Constant[value=0.10000000000000001]()
  %3016 : bool = prim::Constant[value=0]()
  %3017 : bool = prim::Constant[value=1]()
  %3018 : Tensor = prim::GetAttr[name="running_var"](%38)
  %3019 : Tensor = prim::GetAttr[name="running_mean"](%38)
  %3020 : Tensor = prim::GetAttr[name="bias"](%38)
  %3021 : Tensor = prim::GetAttr[name="weight"](%38)
  %input.171 : Tensor = aten::batch_norm(%input.169, %3021, %3020, %3019, %3018, %3016, %3015, %3014, %3017)
  %3023 : Tensor = aten::relu_(%input.171)
  %3024 : Tensor = prim::Constant[value=<Tensor>]()
  %3025 : Tensor = prim::Constant[value=<Tensor>]()
  %3026 : float = prim::Constant[value=0.079049185505063513]()
  %3027 : int = prim::Constant[value=0]()
  %3028 : int = prim::Constant[value=-128]()
  %3029 : int = prim::Constant[value=127]()
  %3030 : int = prim::Constant[value=1]()
  %3031 : int[] = prim::Constant[value=[1, 1]]()
  %3032 : bool = prim::Constant[value=0]()
  %3033 : int[] = prim::Constant[value=[0, 0]]()
  %3034 : bool = prim::Constant[value=1]()
  %3035 : Tensor = prim::GetAttr[name="bias"](%28)
  %3036 : Tensor = prim::GetAttr[name="weight"](%28)
  %3037 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_318.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%28)
  %3038 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_317.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%28)
  %quant_input.104 : Tensor = aten::fake_quantize_per_tensor_affine(%3023, %3026, %3027, %3028, %3029)
  %quant_weight.104 : Tensor = aten::fake_quantize_per_channel_affine(%3036, %3025, %3024, %3027, %3028, %3029)
  %input.173 : Tensor = aten::_convolution(%quant_input.104, %quant_weight.104, %3035, %3031, %3031, %3031, %3032, %3033, %3030, %3032, %3032, %3034, %3034)
  %3042 : Tensor = aten::relu_(%input.173)
  %3043 : Tensor = prim::Constant[value=<Tensor>]()
  %3044 : Tensor = prim::Constant[value=<Tensor>]()
  %3045 : float = prim::Constant[value=0.32104384054349161]()
  %3046 : int = prim::Constant[value=0]()
  %3047 : int = prim::Constant[value=-128]()
  %3048 : int = prim::Constant[value=127]()
  %3049 : int = prim::Constant[value=1]()
  %3050 : int[] = prim::Constant[value=[1, 1]]()
  %3051 : int[] = prim::Constant[value=[0, 0]]()
  %3052 : bool = prim::Constant[value=0]()
  %3053 : bool = prim::Constant[value=1]()
  %3054 : Tensor = prim::GetAttr[name="bias"](%22)
  %3055 : Tensor = prim::GetAttr[name="weight"](%22)
  %3056 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_322.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%22)
  %3057 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_321.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%22)
  %quant_input.106 : Tensor = aten::fake_quantize_per_tensor_affine(%3042, %3045, %3046, %3047, %3048)
  %quant_weight.106 : Tensor = aten::fake_quantize_per_channel_affine(%3055, %3044, %3043, %3046, %3047, %3048)
  %hm_2.1 : Tensor = aten::_convolution(%quant_input.106, %quant_weight.106, %3054, %3050, %3051, %3050, %3052, %3051, %3049, %3052, %3052, %3053, %3053)
  %input11.1 : Tensor = aten::sigmoid_(%hm_2.1)
  %max_pool2d.1 : Tensor = aten::max_pool2d(%input11.1, %1452, %1453, %1453, %1453, %1419)
  %3061 : Tensor = prim::Constant[value=<Tensor>]()
  %3062 : Tensor = prim::Constant[value=<Tensor>]()
  %3063 : float = prim::Constant[value=0.079049185505063513]()
  %3064 : int = prim::Constant[value=0]()
  %3065 : int = prim::Constant[value=-128]()
  %3066 : int = prim::Constant[value=127]()
  %3067 : int = prim::Constant[value=1]()
  %3068 : int[] = prim::Constant[value=[1, 1]]()
  %3069 : bool = prim::Constant[value=0]()
  %3070 : int[] = prim::Constant[value=[0, 0]]()
  %3071 : bool = prim::Constant[value=1]()
  %3072 : Tensor = prim::GetAttr[name="bias"](%19)
  %3073 : Tensor = prim::GetAttr[name="weight"](%19)
  %3074 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_326.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%19)
  %3075 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_325.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%19)
  %quant_input.108 : Tensor = aten::fake_quantize_per_tensor_affine(%3023, %3063, %3064, %3065, %3066)
  %quant_weight.108 : Tensor = aten::fake_quantize_per_channel_affine(%3073, %3062, %3061, %3064, %3065, %3066)
  %input.175 : Tensor = aten::_convolution(%quant_input.108, %quant_weight.108, %3072, %3068, %3068, %3068, %3069, %3070, %3067, %3069, %3069, %3071, %3071)
  %3079 : Tensor = aten::relu_(%input.175)
  %3080 : Tensor = prim::Constant[value= 0  0 [ CUDALongType{2} ]]()
  %3081 : Tensor = prim::Constant[value=0.001 *  2.8907  3.4067 [ CUDAFloatType{2} ]]()
  %3082 : float = prim::Constant[value=0.34322086844857286]()
  %3083 : int = prim::Constant[value=0]()
  %3084 : int = prim::Constant[value=-128]()
  %3085 : int = prim::Constant[value=127]()
  %3086 : int = prim::Constant[value=1]()
  %3087 : int[] = prim::Constant[value=[1, 1]]()
  %3088 : int[] = prim::Constant[value=[0, 0]]()
  %3089 : bool = prim::Constant[value=0]()
  %3090 : bool = prim::Constant[value=1]()
  %3091 : Tensor = prim::GetAttr[name="bias"](%13)
  %3092 : Tensor = prim::GetAttr[name="weight"](%13)
  %3093 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_330.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%13)
  %3094 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_329.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%13)
  %quant_input.110 : Tensor = aten::fake_quantize_per_tensor_affine(%3079, %3082, %3083, %3084, %3085)
  %quant_weight.110 : Tensor = aten::fake_quantize_per_channel_affine(%3092, %3081, %3080, %3083, %3084, %3085)
  %wh_2.1 : Tensor = aten::_convolution(%quant_input.110, %quant_weight.110, %3091, %3087, %3088, %3087, %3089, %3088, %3086, %3089, %3089, %3090, %3090)
  %3098 : Tensor = prim::Constant[value=<Tensor>]()
  %3099 : Tensor = prim::Constant[value=<Tensor>]()
  %3100 : float = prim::Constant[value=0.079049185505063513]()
  %3101 : int = prim::Constant[value=0]()
  %3102 : int = prim::Constant[value=-128]()
  %3103 : int = prim::Constant[value=127]()
  %3104 : int = prim::Constant[value=1]()
  %3105 : int[] = prim::Constant[value=[1, 1]]()
  %3106 : bool = prim::Constant[value=0]()
  %3107 : int[] = prim::Constant[value=[0, 0]]()
  %3108 : bool = prim::Constant[value=1]()
  %3109 : Tensor = prim::GetAttr[name="bias"](%10)
  %3110 : Tensor = prim::GetAttr[name="weight"](%10)
  %3111 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_334.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%10)
  %3112 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_333.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%10)
  %quant_input.112 : Tensor = aten::fake_quantize_per_tensor_affine(%3023, %3100, %3101, %3102, %3103)
  %quant_weight.112 : Tensor = aten::fake_quantize_per_channel_affine(%3110, %3099, %3098, %3101, %3102, %3103)
  %input.1 : Tensor = aten::_convolution(%quant_input.112, %quant_weight.112, %3109, %3105, %3105, %3105, %3106, %3107, %3104, %3106, %3106, %3108, %3108)
  %3116 : Tensor = aten::relu_(%input.1)
  %3117 : Tensor = prim::Constant[value= 0  0 [ CUDALongType{2} ]]()
  %3118 : Tensor = prim::Constant[value=0.0001 *  7.2973  9.0729 [ CUDAFloatType{2} ]]()
  %3119 : float = prim::Constant[value=0.12249077774408296]()
  %3120 : int = prim::Constant[value=0]()
  %3121 : int = prim::Constant[value=-128]()
  %3122 : int = prim::Constant[value=127]()
  %3123 : int = prim::Constant[value=1]()
  %3124 : int[] = prim::Constant[value=[1, 1]]()
  %3125 : int[] = prim::Constant[value=[0, 0]]()
  %3126 : bool = prim::Constant[value=0]()
  %3127 : bool = prim::Constant[value=1]()
  %3128 : Tensor = prim::GetAttr[name="bias"](%4)
  %3129 : Tensor = prim::GetAttr[name="weight"](%4)
  %3130 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_338.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%4)
  %3131 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_337.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%4)
  %quant_input.1 : Tensor = aten::fake_quantize_per_tensor_affine(%3116, %3119, %3120, %3121, %3122)
  %quant_weight.1 : Tensor = aten::fake_quantize_per_channel_affine(%3129, %3118, %3117, %3120, %3121, %3122)
  %reg_2.1 : Tensor = aten::_convolution(%quant_input.1, %quant_weight.1, %3128, %3124, %3125, %3124, %3126, %3125, %3123, %3126, %3126, %3127, %3127)
  %1440 : Tensor[] = prim::ListConstruct(%input11.1, %max_pool2d.1, %wh_2.1, %reg_2.1)
  %1442 : Tensor = aten::cat(%1440, %944)
  return (%1442)

attrModule::reg
attrModule::2
attrModule::reg
attrModule::1
attrModule::reg
attrModule::0
attrModule::wh
attrModule::2
attrModule::wh
attrModule::1
attrModule::wh
attrModule::0
attrModule::hm
attrModule::2
attrModule::hm
attrModule::1
attrModule::hm
attrModule::0
attrModule::dla_up
attrModule::ida_2
attrModule::node_3
attrModule::2
attrModule::dla_up
attrModule::ida_2
attrModule::node_3
attrModule::1
attrModule::dla_up
attrModule::ida_2
attrModule::node_3
attrModule::0
attrModule::dla_up
attrModule::ida_2
attrModule::node_2
attrModule::2
attrModule::dla_up
attrModule::ida_2
attrModule::node_2
attrModule::1
attrModule::dla_up
attrModule::ida_2
attrModule::node_2
attrModule::0
attrModule::dla_up
attrModule::ida_2
attrModule::node_1
attrModule::2
attrModule::dla_up
attrModule::ida_2
attrModule::node_1
attrModule::1
attrModule::dla_up
attrModule::ida_2
attrModule::node_1
attrModule::0
attrModule::dla_up
attrModule::ida_2
attrModule::up_3
attrModule::dla_up
attrModule::ida_2
attrModule::proj_3
attrModule::2
attrModule::dla_up
attrModule::ida_2
attrModule::proj_3
attrModule::1
attrModule::dla_up
attrModule::ida_2
attrModule::proj_3
attrModule::0
attrModule::dla_up
attrModule::ida_2
attrModule::up_2
attrModule::dla_up
attrModule::ida_2
attrModule::proj_2
attrModule::2
attrModule::dla_up
attrModule::ida_2
attrModule::proj_2
attrModule::1
attrModule::dla_up
attrModule::ida_2
attrModule::proj_2
attrModule::0
attrModule::dla_up
attrModule::ida_2
attrModule::up_1
attrModule::dla_up
attrModule::ida_2
attrModule::proj_1
attrModule::2
attrModule::dla_up
attrModule::ida_2
attrModule::proj_1
attrModule::1
attrModule::dla_up
attrModule::ida_2
attrModule::proj_1
attrModule::0
attrModule::dla_up
attrModule::ida_1
attrModule::node_2
attrModule::2
attrModule::dla_up
attrModule::ida_1
attrModule::node_2
attrModule::1
attrModule::dla_up
attrModule::ida_1
attrModule::node_2
attrModule::0
attrModule::dla_up
attrModule::ida_1
attrModule::node_1
attrModule::2
attrModule::dla_up
attrModule::ida_1
attrModule::node_1
attrModule::1
attrModule::dla_up
attrModule::ida_1
attrModule::node_1
attrModule::0
attrModule::dla_up
attrModule::ida_1
attrModule::up_2
attrModule::dla_up
attrModule::ida_1
attrModule::proj_2
attrModule::2
attrModule::dla_up
attrModule::ida_1
attrModule::proj_2
attrModule::1
attrModule::dla_up
attrModule::ida_1
attrModule::proj_2
attrModule::0
attrModule::dla_up
attrModule::ida_1
attrModule::up_1
attrModule::dla_up
attrModule::ida_1
attrModule::proj_1
attrModule::2
attrModule::dla_up
attrModule::ida_1
attrModule::proj_1
attrModule::1
attrModule::dla_up
attrModule::ida_1
attrModule::proj_1
attrModule::0
attrModule::dla_up
attrModule::ida_0
attrModule::node_1
attrModule::2
attrModule::dla_up
attrModule::ida_0
attrModule::node_1
attrModule::1
attrModule::dla_up
attrModule::ida_0
attrModule::node_1
attrModule::0
attrModule::dla_up
attrModule::ida_0
attrModule::up_1
attrModule::dla_up
attrModule::ida_0
attrModule::proj_1
attrModule::2
attrModule::dla_up
attrModule::ida_0
attrModule::proj_1
attrModule::1
attrModule::dla_up
attrModule::ida_0
attrModule::proj_1
attrModule::0
attrModule::base
attrModule::level5
attrModule::root
attrModule::relu
attrModule::base
attrModule::level5
attrModule::root
attrModule::bn
attrModule::base
attrModule::level5
attrModule::root
attrModule::conv
attrModule::base
attrModule::level5
attrModule::tree2
attrModule::bn2
attrModule::base
attrModule::level5
attrModule::tree2
attrModule::conv2
attrModule::base
attrModule::level5
attrModule::tree2
attrModule::relu
attrModule::base
attrModule::level5
attrModule::tree2
attrModule::bn1
attrModule::base
attrModule::level5
attrModule::tree2
attrModule::conv1
attrModule::base
attrModule::level5
attrModule::tree1
attrModule::relu_output_quant
attrModule::base
attrModule::level5
attrModule::tree1
attrModule::bn2
attrModule::base
attrModule::level5
attrModule::tree1
attrModule::conv2
attrModule::base
attrModule::level5
attrModule::tree1
attrModule::relu
attrModule::base
attrModule::level5
attrModule::tree1
attrModule::bn1
attrModule::base
attrModule::level5
attrModule::tree1
attrModule::conv1
attrModule::base
attrModule::level5
attrModule::project
attrModule::1_output_quant
attrModule::base
attrModule::level5
attrModule::project
attrModule::1
attrModule::base
attrModule::level5
attrModule::project
attrModule::0
attrModule::base
attrModule::level5
attrModule::downsample
attrModule::base
attrModule::level4
attrModule::tree2
attrModule::root
attrModule::relu
attrModule::base
attrModule::level4
attrModule::tree2
attrModule::root
attrModule::bn
attrModule::base
attrModule::level4
attrModule::tree2
attrModule::root
attrModule::conv
attrModule::base
attrModule::level4
attrModule::tree2
attrModule::tree2
attrModule::bn2
attrModule::base
attrModule::level4
attrModule::tree2
attrModule::tree2
attrModule::conv2
attrModule::base
attrModule::level4
attrModule::tree2
attrModule::tree2
attrModule::relu
attrModule::base
attrModule::level4
attrModule::tree2
attrModule::tree2
attrModule::bn1
attrModule::base
attrModule::level4
attrModule::tree2
attrModule::tree2
attrModule::conv1
attrModule::base
attrModule::level4
attrModule::tree2
attrModule::tree1
attrModule::relu_output_quant
attrModule::base
attrModule::level4
attrModule::tree2
attrModule::tree1
attrModule::bn2
attrModule::base
attrModule::level4
attrModule::tree2
attrModule::tree1
attrModule::conv2
attrModule::base
attrModule::level4
attrModule::tree2
attrModule::tree1
attrModule::relu
attrModule::base
attrModule::level4
attrModule::tree2
attrModule::tree1
attrModule::bn1
attrModule::base
attrModule::level4
attrModule::tree2
attrModule::tree1
attrModule::conv1
attrModule::base
attrModule::level4
attrModule::tree1
attrModule::root
attrModule::relu_output_quant
attrModule::base
attrModule::level4
attrModule::tree1
attrModule::root
attrModule::relu
attrModule::base
attrModule::level4
attrModule::tree1
attrModule::root
attrModule::bn
attrModule::base
attrModule::level4
attrModule::tree1
attrModule::root
attrModule::conv
attrModule::base
attrModule::level4
attrModule::tree1
attrModule::tree2
attrModule::bn2
attrModule::base
attrModule::level4
attrModule::tree1
attrModule::tree2
attrModule::conv2
attrModule::base
attrModule::level4
attrModule::tree1
attrModule::tree2
attrModule::relu
attrModule::base
attrModule::level4
attrModule::tree1
attrModule::tree2
attrModule::bn1
attrModule::base
attrModule::level4
attrModule::tree1
attrModule::tree2
attrModule::conv1
attrModule::base
attrModule::level4
attrModule::tree1
attrModule::tree1
attrModule::relu_output_quant
attrModule::base
attrModule::level4
attrModule::tree1
attrModule::tree1
attrModule::bn2
attrModule::base
attrModule::level4
attrModule::tree1
attrModule::tree1
attrModule::conv2
attrModule::base
attrModule::level4
attrModule::tree1
attrModule::tree1
attrModule::relu
attrModule::base
attrModule::level4
attrModule::tree1
attrModule::tree1
attrModule::bn1
attrModule::base
attrModule::level4
attrModule::tree1
attrModule::tree1
attrModule::conv1
attrModule::base
attrModule::level4
attrModule::tree1
attrModule::project
attrModule::1_output_quant
attrModule::base
attrModule::level4
attrModule::tree1
attrModule::project
attrModule::1
attrModule::base
attrModule::level4
attrModule::tree1
attrModule::project
attrModule::0
attrModule::base
attrModule::level4
attrModule::tree1
attrModule::downsample
attrModule::base
attrModule::level4
attrModule::project
attrModule::1
attrModule::base
attrModule::level4
attrModule::project
attrModule::0
attrModule::base
attrModule::level4
attrModule::downsample
attrModule::base
attrModule::level3
attrModule::tree2
attrModule::root
attrModule::relu
attrModule::base
attrModule::level3
attrModule::tree2
attrModule::root
attrModule::bn
attrModule::base
attrModule::level3
attrModule::tree2
attrModule::root
attrModule::conv
attrModule::base
attrModule::level3
attrModule::tree2
attrModule::tree2
attrModule::bn2
attrModule::base
attrModule::level3
attrModule::tree2
attrModule::tree2
attrModule::conv2
attrModule::base
attrModule::level3
attrModule::tree2
attrModule::tree2
attrModule::relu
attrModule::base
attrModule::level3
attrModule::tree2
attrModule::tree2
attrModule::bn1
attrModule::base
attrModule::level3
attrModule::tree2
attrModule::tree2
attrModule::conv1
attrModule::base
attrModule::level3
attrModule::tree2
attrModule::tree1
attrModule::relu_output_quant
attrModule::base
attrModule::level3
attrModule::tree2
attrModule::tree1
attrModule::bn2
attrModule::base
attrModule::level3
attrModule::tree2
attrModule::tree1
attrModule::conv2
attrModule::base
attrModule::level3
attrModule::tree2
attrModule::tree1
attrModule::relu
attrModule::base
attrModule::level3
attrModule::tree2
attrModule::tree1
attrModule::bn1
attrModule::base
attrModule::level3
attrModule::tree2
attrModule::tree1
attrModule::conv1
attrModule::base
attrModule::level3
attrModule::tree1
attrModule::root
attrModule::relu_output_quant
attrModule::base
attrModule::level3
attrModule::tree1
attrModule::root
attrModule::relu
attrModule::base
attrModule::level3
attrModule::tree1
attrModule::root
attrModule::bn
attrModule::base
attrModule::level3
attrModule::tree1
attrModule::root
attrModule::conv
attrModule::base
attrModule::level3
attrModule::tree1
attrModule::tree2
attrModule::bn2
attrModule::base
attrModule::level3
attrModule::tree1
attrModule::tree2
attrModule::conv2
attrModule::base
attrModule::level3
attrModule::tree1
attrModule::tree2
attrModule::relu
attrModule::base
attrModule::level3
attrModule::tree1
attrModule::tree2
attrModule::bn1
attrModule::base
attrModule::level3
attrModule::tree1
attrModule::tree2
attrModule::conv1
attrModule::base
attrModule::level3
attrModule::tree1
attrModule::tree1
attrModule::relu_output_quant
attrModule::base
attrModule::level3
attrModule::tree1
attrModule::tree1
attrModule::bn2
attrModule::base
attrModule::level3
attrModule::tree1
attrModule::tree1
attrModule::conv2
attrModule::base
attrModule::level3
attrModule::tree1
attrModule::tree1
attrModule::relu
attrModule::base
attrModule::level3
attrModule::tree1
attrModule::tree1
attrModule::bn1
attrModule::base
attrModule::level3
attrModule::tree1
attrModule::tree1
attrModule::conv1
attrModule::base
attrModule::level3
attrModule::tree1
attrModule::project
attrModule::1_output_quant
attrModule::base
attrModule::level3
attrModule::tree1
attrModule::project
attrModule::1
attrModule::base
attrModule::level3
attrModule::tree1
attrModule::project
attrModule::0
attrModule::base
attrModule::level3
attrModule::tree1
attrModule::downsample
attrModule::base
attrModule::level3
attrModule::project
attrModule::1
attrModule::base
attrModule::level3
attrModule::project
attrModule::0
attrModule::base
attrModule::level3
attrModule::downsample
attrModule::base
attrModule::level2
attrModule::root
attrModule::relu
attrModule::base
attrModule::level2
attrModule::root
attrModule::bn
attrModule::base
attrModule::level2
attrModule::root
attrModule::conv
attrModule::base
attrModule::level2
attrModule::tree2
attrModule::bn2
attrModule::base
attrModule::level2
attrModule::tree2
attrModule::conv2
attrModule::base
attrModule::level2
attrModule::tree2
attrModule::relu
attrModule::base
attrModule::level2
attrModule::tree2
attrModule::bn1
attrModule::base
attrModule::level2
attrModule::tree2
attrModule::conv1
attrModule::base
attrModule::level2
attrModule::tree1
attrModule::relu_output_quant
attrModule::base
attrModule::level2
attrModule::tree1
attrModule::bn2
attrModule::base
attrModule::level2
attrModule::tree1
attrModule::conv2
attrModule::base
attrModule::level2
attrModule::tree1
attrModule::relu
attrModule::base
attrModule::level2
attrModule::tree1
attrModule::bn1
attrModule::base
attrModule::level2
attrModule::tree1
attrModule::conv1
attrModule::base
attrModule::level2
attrModule::project
attrModule::1_output_quant
attrModule::base
attrModule::level2
attrModule::project
attrModule::1
attrModule::base
attrModule::level2
attrModule::project
attrModule::0
attrModule::base
attrModule::level2
attrModule::downsample
attrModule::base
attrModule::level1
attrModule::2
attrModule::base
attrModule::level1
attrModule::1
attrModule::base
attrModule::level1
attrModule::0
attrModule::base
attrModule::level0
attrModule::2
attrModule::base
attrModule::level0
attrModule::1
attrModule::base
attrModule::level0
attrModule::0
attrModule::base
attrModule::base_layer
attrModule::2
attrModule::base
attrModule::base_layer
attrModule::1
attrModule::base
attrModule::base_layer
attrModule::0
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::_input_quantizer
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::_input_quantizer
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::_input_quantizer
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::_input_quantizer
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::_input_quantizer
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::_input_quantizer
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::_input_quantizer
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::_input_quantizer
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::_input_quantizer
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::_input_quantizer
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::_input_quantizer
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::_input_quantizer
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::weight
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::weight
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::weight
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::weight
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::weight
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::weight
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::running_var
attrModule::running_mean
attrModule::bias
attrModule::weight
attrModule::bias
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::bias
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::bias
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::bias
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::bias
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
attrModule::bias
attrModule::weight
attrModule::_weight_quantizer
attrModule::_input_quantizer
Graph after AttributePropagator
graph(%self.1 : __torch__.torch.fx.graph_module.GraphModule,
      %x.1 : Tensor):
  %self.base.base_layer.0.weight : Float(16, 3, 7, 7, strides=[147, 49, 7, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.base_layer.1.running_var : Float(16, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.base_layer.1.running_mean : Float(16, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.base_layer.1.bias : Float(16, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.base_layer.1.weight : Float(16, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level0.0.weight : Float(16, 16, 3, 3, strides=[144, 9, 3, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level0.1.running_var : Float(16, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level0.1.running_mean : Float(16, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level0.1.bias : Float(16, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level0.1.weight : Float(16, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level1.0.weight : Float(32, 16, 3, 3, strides=[144, 9, 3, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level1.1.running_var : Float(32, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level1.1.running_mean : Float(32, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level1.1.bias : Float(32, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level1.1.weight : Float(32, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level2.project.0.weight : Float(64, 32, 1, 1, strides=[32, 1, 1, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level2.project.1.running_var : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level2.project.1.running_mean : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level2.project.1.bias : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level2.project.1.weight : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level2.tree1.conv1.weight : Float(64, 32, 3, 3, strides=[288, 9, 3, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level2.tree1.bn1.running_var : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level2.tree1.bn1.running_mean : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level2.tree1.bn1.bias : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level2.tree1.bn1.weight : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level2.tree1.conv2.weight : Float(64, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level2.tree1.bn2.running_var : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level2.tree1.bn2.running_mean : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level2.tree1.bn2.bias : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level2.tree1.bn2.weight : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level2.tree2.conv1.weight : Float(64, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level2.tree2.bn1.running_var : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level2.tree2.bn1.running_mean : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level2.tree2.bn1.bias : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level2.tree2.bn1.weight : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level2.tree2.conv2.weight : Float(64, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level2.tree2.bn2.running_var : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level2.tree2.bn2.running_mean : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level2.tree2.bn2.bias : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level2.tree2.bn2.weight : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level2.root.conv.weight : Float(64, 128, 1, 1, strides=[128, 1, 1, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level2.root.bn.running_var : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level2.root.bn.running_mean : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level2.root.bn.bias : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level2.root.bn.weight : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.project.0.weight : Float(128, 64, 1, 1, strides=[64, 1, 1, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree1.project.0.weight : Float(128, 64, 1, 1, strides=[64, 1, 1, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree1.project.1.running_var : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree1.project.1.running_mean : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree1.project.1.bias : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree1.project.1.weight : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree1.tree1.conv1.weight : Float(128, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree1.tree1.bn1.running_var : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree1.tree1.bn1.running_mean : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree1.tree1.bn1.bias : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree1.tree1.bn1.weight : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree1.tree1.conv2.weight : Float(128, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree1.tree1.bn2.running_var : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree1.tree1.bn2.running_mean : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree1.tree1.bn2.bias : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree1.tree1.bn2.weight : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree1.tree2.conv1.weight : Float(128, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree1.tree2.bn1.running_var : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree1.tree2.bn1.running_mean : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree1.tree2.bn1.bias : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree1.tree2.bn1.weight : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree1.tree2.conv2.weight : Float(128, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree1.tree2.bn2.running_var : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree1.tree2.bn2.running_mean : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree1.tree2.bn2.bias : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree1.tree2.bn2.weight : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree1.root.conv.weight : Float(128, 256, 1, 1, strides=[256, 1, 1, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree1.root.bn.running_var : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree1.root.bn.running_mean : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree1.root.bn.bias : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree1.root.bn.weight : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree2.tree1.conv1.weight : Float(128, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree2.tree1.bn1.running_var : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree2.tree1.bn1.running_mean : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree2.tree1.bn1.bias : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree2.tree1.bn1.weight : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree2.tree1.conv2.weight : Float(128, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree2.tree1.bn2.running_var : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree2.tree1.bn2.running_mean : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree2.tree1.bn2.bias : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree2.tree1.bn2.weight : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree2.tree2.conv1.weight : Float(128, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree2.tree2.bn1.running_var : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree2.tree2.bn1.running_mean : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree2.tree2.bn1.bias : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree2.tree2.bn1.weight : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree2.tree2.conv2.weight : Float(128, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree2.tree2.bn2.running_var : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree2.tree2.bn2.running_mean : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree2.tree2.bn2.bias : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree2.tree2.bn2.weight : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree2.root.conv.weight : Float(128, 448, 1, 1, strides=[448, 1, 1, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree2.root.bn.running_var : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree2.root.bn.running_mean : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree2.root.bn.bias : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level3.tree2.root.bn.weight : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.project.0.weight : Float(256, 128, 1, 1, strides=[128, 1, 1, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree1.project.0.weight : Float(256, 128, 1, 1, strides=[128, 1, 1, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree1.project.1.running_var : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree1.project.1.running_mean : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree1.project.1.bias : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree1.project.1.weight : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree1.tree1.conv1.weight : Float(256, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree1.tree1.bn1.running_var : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree1.tree1.bn1.running_mean : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree1.tree1.bn1.bias : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree1.tree1.bn1.weight : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree1.tree1.conv2.weight : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree1.tree1.bn2.running_var : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree1.tree1.bn2.running_mean : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree1.tree1.bn2.bias : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree1.tree1.bn2.weight : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree1.tree2.conv1.weight : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree1.tree2.bn1.running_var : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree1.tree2.bn1.running_mean : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree1.tree2.bn1.bias : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree1.tree2.bn1.weight : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree1.tree2.conv2.weight : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree1.tree2.bn2.running_var : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree1.tree2.bn2.running_mean : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree1.tree2.bn2.bias : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree1.tree2.bn2.weight : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree1.root.conv.weight : Float(256, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree1.root.bn.running_var : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree1.root.bn.running_mean : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree1.root.bn.bias : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree1.root.bn.weight : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree2.tree1.conv1.weight : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree2.tree1.bn1.running_var : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree2.tree1.bn1.running_mean : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree2.tree1.bn1.bias : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree2.tree1.bn1.weight : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree2.tree1.conv2.weight : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree2.tree1.bn2.running_var : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree2.tree1.bn2.running_mean : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree2.tree1.bn2.bias : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree2.tree1.bn2.weight : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree2.tree2.conv1.weight : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree2.tree2.bn1.running_var : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree2.tree2.bn1.running_mean : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree2.tree2.bn1.bias : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree2.tree2.bn1.weight : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree2.tree2.conv2.weight : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree2.tree2.bn2.running_var : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree2.tree2.bn2.running_mean : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree2.tree2.bn2.bias : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree2.tree2.bn2.weight : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree2.root.conv.weight : Float(256, 896, 1, 1, strides=[896, 1, 1, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree2.root.bn.running_var : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree2.root.bn.running_mean : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree2.root.bn.bias : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level4.tree2.root.bn.weight : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level5.project.0.weight : Float(512, 256, 1, 1, strides=[256, 1, 1, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level5.project.1.running_var : Float(512, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level5.project.1.running_mean : Float(512, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level5.project.1.bias : Float(512, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level5.project.1.weight : Float(512, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level5.tree1.conv1.weight : Float(512, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level5.tree1.bn1.running_var : Float(512, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level5.tree1.bn1.running_mean : Float(512, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level5.tree1.bn1.bias : Float(512, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level5.tree1.bn1.weight : Float(512, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level5.tree1.conv2.weight : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level5.tree1.bn2.running_var : Float(512, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level5.tree1.bn2.running_mean : Float(512, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level5.tree1.bn2.bias : Float(512, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level5.tree1.bn2.weight : Float(512, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level5.tree2.conv1.weight : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level5.tree2.bn1.running_var : Float(512, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level5.tree2.bn1.running_mean : Float(512, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level5.tree2.bn1.bias : Float(512, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level5.tree2.bn1.weight : Float(512, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level5.tree2.conv2.weight : Float(512, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level5.tree2.bn2.running_var : Float(512, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level5.tree2.bn2.running_mean : Float(512, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level5.tree2.bn2.bias : Float(512, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level5.tree2.bn2.weight : Float(512, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level5.root.conv.weight : Float(512, 1280, 1, 1, strides=[1280, 1, 1, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level5.root.bn.running_var : Float(512, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level5.root.bn.running_mean : Float(512, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level5.root.bn.bias : Float(512, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.base.level5.root.bn.weight : Float(512, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_0.proj_1.0.weight : Float(256, 512, 1, 1, strides=[512, 1, 1, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_0.proj_1.1.running_var : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_0.proj_1.1.running_mean : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_0.proj_1.1.bias : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_0.proj_1.1.weight : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_0.up_1.weight : Float(256, 1, 4, 4, strides=[16, 16, 4, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_0.node_1.0.weight : Float(256, 512, 3, 3, strides=[4608, 9, 3, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_0.node_1.1.running_var : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_0.node_1.1.running_mean : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_0.node_1.1.bias : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_0.node_1.1.weight : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_1.proj_1.0.weight : Float(128, 256, 1, 1, strides=[256, 1, 1, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_1.proj_1.1.running_var : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_1.proj_1.1.running_mean : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_1.proj_1.1.bias : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_1.proj_1.1.weight : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_1.up_1.weight : Float(128, 1, 4, 4, strides=[16, 16, 4, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_1.proj_2.0.weight : Float(128, 256, 1, 1, strides=[256, 1, 1, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_1.proj_2.1.running_var : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_1.proj_2.1.running_mean : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_1.proj_2.1.bias : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_1.proj_2.1.weight : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_1.up_2.weight : Float(128, 1, 4, 4, strides=[16, 16, 4, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_1.node_1.0.weight : Float(128, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_1.node_1.1.running_var : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_1.node_1.1.running_mean : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_1.node_1.1.bias : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_1.node_1.1.weight : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_1.node_2.0.weight : Float(128, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_1.node_2.1.running_var : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_1.node_2.1.running_mean : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_1.node_2.1.bias : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_1.node_2.1.weight : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_2.proj_1.0.weight : Float(64, 128, 1, 1, strides=[128, 1, 1, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_2.proj_1.1.running_var : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_2.proj_1.1.running_mean : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_2.proj_1.1.bias : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_2.proj_1.1.weight : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_2.up_1.weight : Float(64, 1, 4, 4, strides=[16, 16, 4, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_2.proj_2.0.weight : Float(64, 128, 1, 1, strides=[128, 1, 1, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_2.proj_2.1.running_var : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_2.proj_2.1.running_mean : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_2.proj_2.1.bias : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_2.proj_2.1.weight : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_2.up_2.weight : Float(64, 1, 4, 4, strides=[16, 16, 4, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_2.proj_3.0.weight : Float(64, 128, 1, 1, strides=[128, 1, 1, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_2.proj_3.1.running_var : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_2.proj_3.1.running_mean : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_2.proj_3.1.bias : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_2.proj_3.1.weight : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_2.up_3.weight : Float(64, 1, 4, 4, strides=[16, 16, 4, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_2.node_1.0.weight : Float(64, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_2.node_1.1.running_var : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_2.node_1.1.running_mean : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_2.node_1.1.bias : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_2.node_1.1.weight : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_2.node_2.0.weight : Float(64, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_2.node_2.1.running_var : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_2.node_2.1.running_mean : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_2.node_2.1.bias : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_2.node_2.1.weight : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_2.node_3.0.weight : Float(64, 128, 3, 3, strides=[1152, 9, 3, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_2.node_3.1.running_var : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_2.node_3.1.running_mean : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_2.node_3.1.bias : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.dla_up.ida_2.node_3.1.weight : Float(64, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.hm.0.bias : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.hm.0.weight : Float(256, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.hm.2.bias : Float(128, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.hm.2.weight : Float(128, 256, 1, 1, strides=[256, 1, 1, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.wh.0.bias : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.wh.0.weight : Float(256, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.wh.2.bias : Float(2, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value= 0.1654  0.3120 [ CUDAFloatType{2} ]]()
  %self.wh.2.weight : Float(2, 256, 1, 1, strides=[256, 1, 1, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.reg.0.bias : Float(256, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.reg.0.weight : Float(256, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %self.reg.2.bias : Float(2, strides=[1], requires_grad=0, device=cuda:0) = prim::Constant[value= 0.4087  0.3965 [ CUDAFloatType{2} ]]()
  %self.reg.2.weight : Float(2, 256, 1, 1, strides=[256, 1, 1, 1], requires_grad=0, device=cuda:0) = prim::Constant[value=<Tensor>]()
  %1419 : bool = prim::Constant[value=0]()
  %1453 : int[] = prim::Constant[value=[1, 1]]()
  %1452 : int[] = prim::Constant[value=[3, 3]]()
  %944 : int = prim::Constant[value=1]()
  %3 : __torch__.torch.nn.modules.module.___torch_mangle_340.Module = prim::GetAttr[name="reg"](%self.1)
  %4 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_339.QuantConv2d = prim::GetAttr[name="2"](%3)
  %6 : __torch__.torch.nn.modules.module.___torch_mangle_340.Module = prim::GetAttr[name="reg"](%self.1)
  %7 : __torch__.torch.nn.modules.activation.___torch_mangle_336.ReLU = prim::GetAttr[name="1"](%6)
  %9 : __torch__.torch.nn.modules.module.___torch_mangle_340.Module = prim::GetAttr[name="reg"](%self.1)
  %10 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_335.QuantConv2d = prim::GetAttr[name="0"](%9)
  %12 : __torch__.torch.nn.modules.module.___torch_mangle_332.Module = prim::GetAttr[name="wh"](%self.1)
  %13 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_331.QuantConv2d = prim::GetAttr[name="2"](%12)
  %15 : __torch__.torch.nn.modules.module.___torch_mangle_332.Module = prim::GetAttr[name="wh"](%self.1)
  %16 : __torch__.torch.nn.modules.activation.___torch_mangle_328.ReLU = prim::GetAttr[name="1"](%15)
  %18 : __torch__.torch.nn.modules.module.___torch_mangle_332.Module = prim::GetAttr[name="wh"](%self.1)
  %19 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_327.QuantConv2d = prim::GetAttr[name="0"](%18)
  %21 : __torch__.torch.nn.modules.module.___torch_mangle_324.Module = prim::GetAttr[name="hm"](%self.1)
  %22 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_323.QuantConv2d = prim::GetAttr[name="2"](%21)
  %24 : __torch__.torch.nn.modules.module.___torch_mangle_324.Module = prim::GetAttr[name="hm"](%self.1)
  %25 : __torch__.torch.nn.modules.activation.___torch_mangle_320.ReLU = prim::GetAttr[name="1"](%24)
  %27 : __torch__.torch.nn.modules.module.___torch_mangle_324.Module = prim::GetAttr[name="hm"](%self.1)
  %28 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_319.QuantConv2d = prim::GetAttr[name="0"](%27)
  %30 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %31 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%30)
  %32 : __torch__.torch.nn.modules.module.___torch_mangle_314.Module = prim::GetAttr[name="node_3"](%31)
  %33 : __torch__.torch.nn.modules.activation.___torch_mangle_313.ReLU = prim::GetAttr[name="2"](%32)
  %35 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %36 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%35)
  %37 : __torch__.torch.nn.modules.module.___torch_mangle_314.Module = prim::GetAttr[name="node_3"](%36)
  %38 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_312.BatchNorm2d = prim::GetAttr[name="1"](%37)
  %40 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %41 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%40)
  %42 : __torch__.torch.nn.modules.module.___torch_mangle_314.Module = prim::GetAttr[name="node_3"](%41)
  %43 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_311.QuantConv2d = prim::GetAttr[name="0"](%42)
  %45 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %46 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%45)
  %47 : __torch__.torch.nn.modules.module.___torch_mangle_308.Module = prim::GetAttr[name="node_2"](%46)
  %48 : __torch__.torch.nn.modules.activation.___torch_mangle_307.ReLU = prim::GetAttr[name="2"](%47)
  %50 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %51 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%50)
  %52 : __torch__.torch.nn.modules.module.___torch_mangle_308.Module = prim::GetAttr[name="node_2"](%51)
  %53 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_306.BatchNorm2d = prim::GetAttr[name="1"](%52)
  %55 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %56 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%55)
  %57 : __torch__.torch.nn.modules.module.___torch_mangle_308.Module = prim::GetAttr[name="node_2"](%56)
  %58 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_305.QuantConv2d = prim::GetAttr[name="0"](%57)
  %60 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %61 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%60)
  %62 : __torch__.torch.nn.modules.module.___torch_mangle_302.Module = prim::GetAttr[name="node_1"](%61)
  %63 : __torch__.torch.nn.modules.activation.___torch_mangle_301.ReLU = prim::GetAttr[name="2"](%62)
  %65 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %66 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%65)
  %67 : __torch__.torch.nn.modules.module.___torch_mangle_302.Module = prim::GetAttr[name="node_1"](%66)
  %68 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_300.BatchNorm2d = prim::GetAttr[name="1"](%67)
  %70 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %71 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%70)
  %72 : __torch__.torch.nn.modules.module.___torch_mangle_302.Module = prim::GetAttr[name="node_1"](%71)
  %73 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_299.QuantConv2d = prim::GetAttr[name="0"](%72)
  %75 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %76 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%75)
  %77 : __torch__.torch.nn.modules.conv.___torch_mangle_296.ConvTranspose2d = prim::GetAttr[name="up_3"](%76)
  %79 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %80 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%79)
  %81 : __torch__.torch.nn.modules.module.___torch_mangle_295.Module = prim::GetAttr[name="proj_3"](%80)
  %82 : __torch__.torch.nn.modules.activation.___torch_mangle_294.ReLU = prim::GetAttr[name="2"](%81)
  %84 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %85 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%84)
  %86 : __torch__.torch.nn.modules.module.___torch_mangle_295.Module = prim::GetAttr[name="proj_3"](%85)
  %87 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_293.BatchNorm2d = prim::GetAttr[name="1"](%86)
  %89 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %90 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%89)
  %91 : __torch__.torch.nn.modules.module.___torch_mangle_295.Module = prim::GetAttr[name="proj_3"](%90)
  %92 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_292.QuantConv2d = prim::GetAttr[name="0"](%91)
  %94 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %95 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%94)
  %96 : __torch__.torch.nn.modules.conv.___torch_mangle_289.ConvTranspose2d = prim::GetAttr[name="up_2"](%95)
  %98 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %99 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%98)
  %100 : __torch__.torch.nn.modules.module.___torch_mangle_288.Module = prim::GetAttr[name="proj_2"](%99)
  %101 : __torch__.torch.nn.modules.activation.___torch_mangle_287.ReLU = prim::GetAttr[name="2"](%100)
  %103 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %104 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%103)
  %105 : __torch__.torch.nn.modules.module.___torch_mangle_288.Module = prim::GetAttr[name="proj_2"](%104)
  %106 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_286.BatchNorm2d = prim::GetAttr[name="1"](%105)
  %108 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %109 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%108)
  %110 : __torch__.torch.nn.modules.module.___torch_mangle_288.Module = prim::GetAttr[name="proj_2"](%109)
  %111 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_285.QuantConv2d = prim::GetAttr[name="0"](%110)
  %113 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %114 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%113)
  %115 : __torch__.torch.nn.modules.conv.___torch_mangle_282.ConvTranspose2d = prim::GetAttr[name="up_1"](%114)
  %117 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %118 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%117)
  %119 : __torch__.torch.nn.modules.module.___torch_mangle_281.Module = prim::GetAttr[name="proj_1"](%118)
  %120 : __torch__.torch.nn.modules.activation.___torch_mangle_280.ReLU = prim::GetAttr[name="2"](%119)
  %122 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %123 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%122)
  %124 : __torch__.torch.nn.modules.module.___torch_mangle_281.Module = prim::GetAttr[name="proj_1"](%123)
  %125 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_279.BatchNorm2d = prim::GetAttr[name="1"](%124)
  %127 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %128 : __torch__.torch.nn.modules.module.___torch_mangle_315.Module = prim::GetAttr[name="ida_2"](%127)
  %129 : __torch__.torch.nn.modules.module.___torch_mangle_281.Module = prim::GetAttr[name="proj_1"](%128)
  %130 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_278.QuantConv2d = prim::GetAttr[name="0"](%129)
  %132 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %133 : __torch__.torch.nn.modules.module.___torch_mangle_275.Module = prim::GetAttr[name="ida_1"](%132)
  %134 : __torch__.torch.nn.modules.module.___torch_mangle_274.Module = prim::GetAttr[name="node_2"](%133)
  %135 : __torch__.torch.nn.modules.activation.___torch_mangle_273.ReLU = prim::GetAttr[name="2"](%134)
  %137 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %138 : __torch__.torch.nn.modules.module.___torch_mangle_275.Module = prim::GetAttr[name="ida_1"](%137)
  %139 : __torch__.torch.nn.modules.module.___torch_mangle_274.Module = prim::GetAttr[name="node_2"](%138)
  %140 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_272.BatchNorm2d = prim::GetAttr[name="1"](%139)
  %142 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %143 : __torch__.torch.nn.modules.module.___torch_mangle_275.Module = prim::GetAttr[name="ida_1"](%142)
  %144 : __torch__.torch.nn.modules.module.___torch_mangle_274.Module = prim::GetAttr[name="node_2"](%143)
  %145 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_271.QuantConv2d = prim::GetAttr[name="0"](%144)
  %147 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %148 : __torch__.torch.nn.modules.module.___torch_mangle_275.Module = prim::GetAttr[name="ida_1"](%147)
  %149 : __torch__.torch.nn.modules.module.___torch_mangle_268.Module = prim::GetAttr[name="node_1"](%148)
  %150 : __torch__.torch.nn.modules.activation.___torch_mangle_267.ReLU = prim::GetAttr[name="2"](%149)
  %152 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %153 : __torch__.torch.nn.modules.module.___torch_mangle_275.Module = prim::GetAttr[name="ida_1"](%152)
  %154 : __torch__.torch.nn.modules.module.___torch_mangle_268.Module = prim::GetAttr[name="node_1"](%153)
  %155 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_266.BatchNorm2d = prim::GetAttr[name="1"](%154)
  %157 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %158 : __torch__.torch.nn.modules.module.___torch_mangle_275.Module = prim::GetAttr[name="ida_1"](%157)
  %159 : __torch__.torch.nn.modules.module.___torch_mangle_268.Module = prim::GetAttr[name="node_1"](%158)
  %160 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_265.QuantConv2d = prim::GetAttr[name="0"](%159)
  %162 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %163 : __torch__.torch.nn.modules.module.___torch_mangle_275.Module = prim::GetAttr[name="ida_1"](%162)
  %164 : __torch__.torch.nn.modules.conv.___torch_mangle_262.ConvTranspose2d = prim::GetAttr[name="up_2"](%163)
  %166 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %167 : __torch__.torch.nn.modules.module.___torch_mangle_275.Module = prim::GetAttr[name="ida_1"](%166)
  %168 : __torch__.torch.nn.modules.module.___torch_mangle_261.Module = prim::GetAttr[name="proj_2"](%167)
  %169 : __torch__.torch.nn.modules.activation.___torch_mangle_260.ReLU = prim::GetAttr[name="2"](%168)
  %171 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %172 : __torch__.torch.nn.modules.module.___torch_mangle_275.Module = prim::GetAttr[name="ida_1"](%171)
  %173 : __torch__.torch.nn.modules.module.___torch_mangle_261.Module = prim::GetAttr[name="proj_2"](%172)
  %174 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_259.BatchNorm2d = prim::GetAttr[name="1"](%173)
  %176 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %177 : __torch__.torch.nn.modules.module.___torch_mangle_275.Module = prim::GetAttr[name="ida_1"](%176)
  %178 : __torch__.torch.nn.modules.module.___torch_mangle_261.Module = prim::GetAttr[name="proj_2"](%177)
  %179 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_258.QuantConv2d = prim::GetAttr[name="0"](%178)
  %181 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %182 : __torch__.torch.nn.modules.module.___torch_mangle_275.Module = prim::GetAttr[name="ida_1"](%181)
  %183 : __torch__.torch.nn.modules.conv.___torch_mangle_255.ConvTranspose2d = prim::GetAttr[name="up_1"](%182)
  %185 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %186 : __torch__.torch.nn.modules.module.___torch_mangle_275.Module = prim::GetAttr[name="ida_1"](%185)
  %187 : __torch__.torch.nn.modules.module.___torch_mangle_254.Module = prim::GetAttr[name="proj_1"](%186)
  %188 : __torch__.torch.nn.modules.activation.___torch_mangle_253.ReLU = prim::GetAttr[name="2"](%187)
  %190 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %191 : __torch__.torch.nn.modules.module.___torch_mangle_275.Module = prim::GetAttr[name="ida_1"](%190)
  %192 : __torch__.torch.nn.modules.module.___torch_mangle_254.Module = prim::GetAttr[name="proj_1"](%191)
  %193 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_252.BatchNorm2d = prim::GetAttr[name="1"](%192)
  %195 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %196 : __torch__.torch.nn.modules.module.___torch_mangle_275.Module = prim::GetAttr[name="ida_1"](%195)
  %197 : __torch__.torch.nn.modules.module.___torch_mangle_254.Module = prim::GetAttr[name="proj_1"](%196)
  %198 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_251.QuantConv2d = prim::GetAttr[name="0"](%197)
  %200 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %201 : __torch__.torch.nn.modules.module.___torch_mangle_248.Module = prim::GetAttr[name="ida_0"](%200)
  %202 : __torch__.torch.nn.modules.module.___torch_mangle_247.Module = prim::GetAttr[name="node_1"](%201)
  %203 : __torch__.torch.nn.modules.activation.___torch_mangle_246.ReLU = prim::GetAttr[name="2"](%202)
  %205 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %206 : __torch__.torch.nn.modules.module.___torch_mangle_248.Module = prim::GetAttr[name="ida_0"](%205)
  %207 : __torch__.torch.nn.modules.module.___torch_mangle_247.Module = prim::GetAttr[name="node_1"](%206)
  %208 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_245.BatchNorm2d = prim::GetAttr[name="1"](%207)
  %210 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %211 : __torch__.torch.nn.modules.module.___torch_mangle_248.Module = prim::GetAttr[name="ida_0"](%210)
  %212 : __torch__.torch.nn.modules.module.___torch_mangle_247.Module = prim::GetAttr[name="node_1"](%211)
  %213 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_244.QuantConv2d = prim::GetAttr[name="0"](%212)
  %215 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %216 : __torch__.torch.nn.modules.module.___torch_mangle_248.Module = prim::GetAttr[name="ida_0"](%215)
  %217 : __torch__.torch.nn.modules.conv.ConvTranspose2d = prim::GetAttr[name="up_1"](%216)
  %219 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %220 : __torch__.torch.nn.modules.module.___torch_mangle_248.Module = prim::GetAttr[name="ida_0"](%219)
  %221 : __torch__.torch.nn.modules.module.___torch_mangle_241.Module = prim::GetAttr[name="proj_1"](%220)
  %222 : __torch__.torch.nn.modules.activation.___torch_mangle_240.ReLU = prim::GetAttr[name="2"](%221)
  %224 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %225 : __torch__.torch.nn.modules.module.___torch_mangle_248.Module = prim::GetAttr[name="ida_0"](%224)
  %226 : __torch__.torch.nn.modules.module.___torch_mangle_241.Module = prim::GetAttr[name="proj_1"](%225)
  %227 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_239.BatchNorm2d = prim::GetAttr[name="1"](%226)
  %229 : __torch__.torch.nn.modules.module.___torch_mangle_316.Module = prim::GetAttr[name="dla_up"](%self.1)
  %230 : __torch__.torch.nn.modules.module.___torch_mangle_248.Module = prim::GetAttr[name="ida_0"](%229)
  %231 : __torch__.torch.nn.modules.module.___torch_mangle_241.Module = prim::GetAttr[name="proj_1"](%230)
  %232 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_238.QuantConv2d = prim::GetAttr[name="0"](%231)
  %234 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %235 : __torch__.torch.nn.modules.module.___torch_mangle_234.Module = prim::GetAttr[name="level5"](%234)
  %236 : __torch__.torch.nn.modules.module.___torch_mangle_233.Module = prim::GetAttr[name="root"](%235)
  %237 : __torch__.torch.nn.modules.activation.___torch_mangle_232.ReLU = prim::GetAttr[name="relu"](%236)
  %239 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %240 : __torch__.torch.nn.modules.module.___torch_mangle_234.Module = prim::GetAttr[name="level5"](%239)
  %241 : __torch__.torch.nn.modules.module.___torch_mangle_233.Module = prim::GetAttr[name="root"](%240)
  %242 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_231.BatchNorm2d = prim::GetAttr[name="bn"](%241)
  %244 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %245 : __torch__.torch.nn.modules.module.___torch_mangle_234.Module = prim::GetAttr[name="level5"](%244)
  %246 : __torch__.torch.nn.modules.module.___torch_mangle_233.Module = prim::GetAttr[name="root"](%245)
  %247 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_230.QuantConv2d = prim::GetAttr[name="conv"](%246)
  %249 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %250 : __torch__.torch.nn.modules.module.___torch_mangle_234.Module = prim::GetAttr[name="level5"](%249)
  %251 : __torch__.torch.nn.modules.module.___torch_mangle_227.Module = prim::GetAttr[name="tree2"](%250)
  %252 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_226.BatchNorm2d = prim::GetAttr[name="bn2"](%251)
  %254 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %255 : __torch__.torch.nn.modules.module.___torch_mangle_234.Module = prim::GetAttr[name="level5"](%254)
  %256 : __torch__.torch.nn.modules.module.___torch_mangle_227.Module = prim::GetAttr[name="tree2"](%255)
  %257 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_225.QuantConv2d = prim::GetAttr[name="conv2"](%256)
  %259 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %260 : __torch__.torch.nn.modules.module.___torch_mangle_234.Module = prim::GetAttr[name="level5"](%259)
  %261 : __torch__.torch.nn.modules.module.___torch_mangle_227.Module = prim::GetAttr[name="tree2"](%260)
  %262 : __torch__.torch.nn.modules.activation.___torch_mangle_222.ReLU = prim::GetAttr[name="relu"](%261)
  %264 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %265 : __torch__.torch.nn.modules.module.___torch_mangle_234.Module = prim::GetAttr[name="level5"](%264)
  %266 : __torch__.torch.nn.modules.module.___torch_mangle_227.Module = prim::GetAttr[name="tree2"](%265)
  %267 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_221.BatchNorm2d = prim::GetAttr[name="bn1"](%266)
  %269 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %270 : __torch__.torch.nn.modules.module.___torch_mangle_234.Module = prim::GetAttr[name="level5"](%269)
  %271 : __torch__.torch.nn.modules.module.___torch_mangle_227.Module = prim::GetAttr[name="tree2"](%270)
  %272 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_220.QuantConv2d = prim::GetAttr[name="conv1"](%271)
  %274 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %275 : __torch__.torch.nn.modules.module.___torch_mangle_234.Module = prim::GetAttr[name="level5"](%274)
  %276 : __torch__.torch.nn.modules.module.___torch_mangle_217.Module = prim::GetAttr[name="tree1"](%275)
  %277 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_identify.___torch_mangle_216.QuantIdentity = prim::GetAttr[name="relu_output_quant"](%276)
  %279 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %280 : __torch__.torch.nn.modules.module.___torch_mangle_234.Module = prim::GetAttr[name="level5"](%279)
  %281 : __torch__.torch.nn.modules.module.___torch_mangle_217.Module = prim::GetAttr[name="tree1"](%280)
  %282 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_214.BatchNorm2d = prim::GetAttr[name="bn2"](%281)
  %284 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %285 : __torch__.torch.nn.modules.module.___torch_mangle_234.Module = prim::GetAttr[name="level5"](%284)
  %286 : __torch__.torch.nn.modules.module.___torch_mangle_217.Module = prim::GetAttr[name="tree1"](%285)
  %287 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_213.QuantConv2d = prim::GetAttr[name="conv2"](%286)
  %289 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %290 : __torch__.torch.nn.modules.module.___torch_mangle_234.Module = prim::GetAttr[name="level5"](%289)
  %291 : __torch__.torch.nn.modules.module.___torch_mangle_217.Module = prim::GetAttr[name="tree1"](%290)
  %292 : __torch__.torch.nn.modules.activation.___torch_mangle_210.ReLU = prim::GetAttr[name="relu"](%291)
  %294 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %295 : __torch__.torch.nn.modules.module.___torch_mangle_234.Module = prim::GetAttr[name="level5"](%294)
  %296 : __torch__.torch.nn.modules.module.___torch_mangle_217.Module = prim::GetAttr[name="tree1"](%295)
  %297 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_209.BatchNorm2d = prim::GetAttr[name="bn1"](%296)
  %299 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %300 : __torch__.torch.nn.modules.module.___torch_mangle_234.Module = prim::GetAttr[name="level5"](%299)
  %301 : __torch__.torch.nn.modules.module.___torch_mangle_217.Module = prim::GetAttr[name="tree1"](%300)
  %302 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_208.QuantConv2d = prim::GetAttr[name="conv1"](%301)
  %304 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %305 : __torch__.torch.nn.modules.module.___torch_mangle_234.Module = prim::GetAttr[name="level5"](%304)
  %306 : __torch__.torch.nn.modules.module.___torch_mangle_205.Module = prim::GetAttr[name="project"](%305)
  %307 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_identify.___torch_mangle_204.QuantIdentity = prim::GetAttr[name="1_output_quant"](%306)
  %309 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %310 : __torch__.torch.nn.modules.module.___torch_mangle_234.Module = prim::GetAttr[name="level5"](%309)
  %311 : __torch__.torch.nn.modules.module.___torch_mangle_205.Module = prim::GetAttr[name="project"](%310)
  %312 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_202.BatchNorm2d = prim::GetAttr[name="1"](%311)
  %314 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %315 : __torch__.torch.nn.modules.module.___torch_mangle_234.Module = prim::GetAttr[name="level5"](%314)
  %316 : __torch__.torch.nn.modules.module.___torch_mangle_205.Module = prim::GetAttr[name="project"](%315)
  %317 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_201.QuantConv2d = prim::GetAttr[name="0"](%316)
  %319 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %320 : __torch__.torch.nn.modules.module.___torch_mangle_234.Module = prim::GetAttr[name="level5"](%319)
  %321 : __torch__.torch.nn.modules.pooling.___torch_mangle_198.MaxPool2d = prim::GetAttr[name="downsample"](%320)
  %323 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %324 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%323)
  %325 : __torch__.torch.nn.modules.module.___torch_mangle_196.Module = prim::GetAttr[name="tree2"](%324)
  %326 : __torch__.torch.nn.modules.module.___torch_mangle_195.Module = prim::GetAttr[name="root"](%325)
  %327 : __torch__.torch.nn.modules.activation.___torch_mangle_194.ReLU = prim::GetAttr[name="relu"](%326)
  %329 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %330 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%329)
  %331 : __torch__.torch.nn.modules.module.___torch_mangle_196.Module = prim::GetAttr[name="tree2"](%330)
  %332 : __torch__.torch.nn.modules.module.___torch_mangle_195.Module = prim::GetAttr[name="root"](%331)
  %333 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_193.BatchNorm2d = prim::GetAttr[name="bn"](%332)
  %335 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %336 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%335)
  %337 : __torch__.torch.nn.modules.module.___torch_mangle_196.Module = prim::GetAttr[name="tree2"](%336)
  %338 : __torch__.torch.nn.modules.module.___torch_mangle_195.Module = prim::GetAttr[name="root"](%337)
  %339 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_192.QuantConv2d = prim::GetAttr[name="conv"](%338)
  %341 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %342 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%341)
  %343 : __torch__.torch.nn.modules.module.___torch_mangle_196.Module = prim::GetAttr[name="tree2"](%342)
  %344 : __torch__.torch.nn.modules.module.___torch_mangle_189.Module = prim::GetAttr[name="tree2"](%343)
  %345 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_188.BatchNorm2d = prim::GetAttr[name="bn2"](%344)
  %347 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %348 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%347)
  %349 : __torch__.torch.nn.modules.module.___torch_mangle_196.Module = prim::GetAttr[name="tree2"](%348)
  %350 : __torch__.torch.nn.modules.module.___torch_mangle_189.Module = prim::GetAttr[name="tree2"](%349)
  %351 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_187.QuantConv2d = prim::GetAttr[name="conv2"](%350)
  %353 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %354 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%353)
  %355 : __torch__.torch.nn.modules.module.___torch_mangle_196.Module = prim::GetAttr[name="tree2"](%354)
  %356 : __torch__.torch.nn.modules.module.___torch_mangle_189.Module = prim::GetAttr[name="tree2"](%355)
  %357 : __torch__.torch.nn.modules.activation.___torch_mangle_184.ReLU = prim::GetAttr[name="relu"](%356)
  %359 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %360 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%359)
  %361 : __torch__.torch.nn.modules.module.___torch_mangle_196.Module = prim::GetAttr[name="tree2"](%360)
  %362 : __torch__.torch.nn.modules.module.___torch_mangle_189.Module = prim::GetAttr[name="tree2"](%361)
  %363 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_183.BatchNorm2d = prim::GetAttr[name="bn1"](%362)
  %365 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %366 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%365)
  %367 : __torch__.torch.nn.modules.module.___torch_mangle_196.Module = prim::GetAttr[name="tree2"](%366)
  %368 : __torch__.torch.nn.modules.module.___torch_mangle_189.Module = prim::GetAttr[name="tree2"](%367)
  %369 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_182.QuantConv2d = prim::GetAttr[name="conv1"](%368)
  %371 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %372 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%371)
  %373 : __torch__.torch.nn.modules.module.___torch_mangle_196.Module = prim::GetAttr[name="tree2"](%372)
  %374 : __torch__.torch.nn.modules.module.___torch_mangle_179.Module = prim::GetAttr[name="tree1"](%373)
  %375 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_identify.___torch_mangle_178.QuantIdentity = prim::GetAttr[name="relu_output_quant"](%374)
  %377 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %378 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%377)
  %379 : __torch__.torch.nn.modules.module.___torch_mangle_196.Module = prim::GetAttr[name="tree2"](%378)
  %380 : __torch__.torch.nn.modules.module.___torch_mangle_179.Module = prim::GetAttr[name="tree1"](%379)
  %381 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_176.BatchNorm2d = prim::GetAttr[name="bn2"](%380)
  %383 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %384 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%383)
  %385 : __torch__.torch.nn.modules.module.___torch_mangle_196.Module = prim::GetAttr[name="tree2"](%384)
  %386 : __torch__.torch.nn.modules.module.___torch_mangle_179.Module = prim::GetAttr[name="tree1"](%385)
  %387 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_175.QuantConv2d = prim::GetAttr[name="conv2"](%386)
  %389 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %390 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%389)
  %391 : __torch__.torch.nn.modules.module.___torch_mangle_196.Module = prim::GetAttr[name="tree2"](%390)
  %392 : __torch__.torch.nn.modules.module.___torch_mangle_179.Module = prim::GetAttr[name="tree1"](%391)
  %393 : __torch__.torch.nn.modules.activation.___torch_mangle_172.ReLU = prim::GetAttr[name="relu"](%392)
  %395 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %396 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%395)
  %397 : __torch__.torch.nn.modules.module.___torch_mangle_196.Module = prim::GetAttr[name="tree2"](%396)
  %398 : __torch__.torch.nn.modules.module.___torch_mangle_179.Module = prim::GetAttr[name="tree1"](%397)
  %399 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_171.BatchNorm2d = prim::GetAttr[name="bn1"](%398)
  %401 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %402 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%401)
  %403 : __torch__.torch.nn.modules.module.___torch_mangle_196.Module = prim::GetAttr[name="tree2"](%402)
  %404 : __torch__.torch.nn.modules.module.___torch_mangle_179.Module = prim::GetAttr[name="tree1"](%403)
  %405 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_170.QuantConv2d = prim::GetAttr[name="conv1"](%404)
  %407 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %408 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%407)
  %409 : __torch__.torch.nn.modules.module.___torch_mangle_167.Module = prim::GetAttr[name="tree1"](%408)
  %410 : __torch__.torch.nn.modules.module.___torch_mangle_166.Module = prim::GetAttr[name="root"](%409)
  %411 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_identify.___torch_mangle_165.QuantIdentity = prim::GetAttr[name="relu_output_quant"](%410)
  %413 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %414 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%413)
  %415 : __torch__.torch.nn.modules.module.___torch_mangle_167.Module = prim::GetAttr[name="tree1"](%414)
  %416 : __torch__.torch.nn.modules.module.___torch_mangle_166.Module = prim::GetAttr[name="root"](%415)
  %417 : __torch__.torch.nn.modules.activation.___torch_mangle_163.ReLU = prim::GetAttr[name="relu"](%416)
  %419 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %420 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%419)
  %421 : __torch__.torch.nn.modules.module.___torch_mangle_167.Module = prim::GetAttr[name="tree1"](%420)
  %422 : __torch__.torch.nn.modules.module.___torch_mangle_166.Module = prim::GetAttr[name="root"](%421)
  %423 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_162.BatchNorm2d = prim::GetAttr[name="bn"](%422)
  %425 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %426 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%425)
  %427 : __torch__.torch.nn.modules.module.___torch_mangle_167.Module = prim::GetAttr[name="tree1"](%426)
  %428 : __torch__.torch.nn.modules.module.___torch_mangle_166.Module = prim::GetAttr[name="root"](%427)
  %429 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_161.QuantConv2d = prim::GetAttr[name="conv"](%428)
  %431 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %432 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%431)
  %433 : __torch__.torch.nn.modules.module.___torch_mangle_167.Module = prim::GetAttr[name="tree1"](%432)
  %434 : __torch__.torch.nn.modules.module.___torch_mangle_158.Module = prim::GetAttr[name="tree2"](%433)
  %435 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_157.BatchNorm2d = prim::GetAttr[name="bn2"](%434)
  %437 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %438 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%437)
  %439 : __torch__.torch.nn.modules.module.___torch_mangle_167.Module = prim::GetAttr[name="tree1"](%438)
  %440 : __torch__.torch.nn.modules.module.___torch_mangle_158.Module = prim::GetAttr[name="tree2"](%439)
  %441 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_156.QuantConv2d = prim::GetAttr[name="conv2"](%440)
  %443 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %444 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%443)
  %445 : __torch__.torch.nn.modules.module.___torch_mangle_167.Module = prim::GetAttr[name="tree1"](%444)
  %446 : __torch__.torch.nn.modules.module.___torch_mangle_158.Module = prim::GetAttr[name="tree2"](%445)
  %447 : __torch__.torch.nn.modules.activation.___torch_mangle_153.ReLU = prim::GetAttr[name="relu"](%446)
  %449 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %450 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%449)
  %451 : __torch__.torch.nn.modules.module.___torch_mangle_167.Module = prim::GetAttr[name="tree1"](%450)
  %452 : __torch__.torch.nn.modules.module.___torch_mangle_158.Module = prim::GetAttr[name="tree2"](%451)
  %453 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_152.BatchNorm2d = prim::GetAttr[name="bn1"](%452)
  %455 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %456 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%455)
  %457 : __torch__.torch.nn.modules.module.___torch_mangle_167.Module = prim::GetAttr[name="tree1"](%456)
  %458 : __torch__.torch.nn.modules.module.___torch_mangle_158.Module = prim::GetAttr[name="tree2"](%457)
  %459 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_151.QuantConv2d = prim::GetAttr[name="conv1"](%458)
  %461 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %462 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%461)
  %463 : __torch__.torch.nn.modules.module.___torch_mangle_167.Module = prim::GetAttr[name="tree1"](%462)
  %464 : __torch__.torch.nn.modules.module.___torch_mangle_148.Module = prim::GetAttr[name="tree1"](%463)
  %465 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_identify.___torch_mangle_147.QuantIdentity = prim::GetAttr[name="relu_output_quant"](%464)
  %467 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %468 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%467)
  %469 : __torch__.torch.nn.modules.module.___torch_mangle_167.Module = prim::GetAttr[name="tree1"](%468)
  %470 : __torch__.torch.nn.modules.module.___torch_mangle_148.Module = prim::GetAttr[name="tree1"](%469)
  %471 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_145.BatchNorm2d = prim::GetAttr[name="bn2"](%470)
  %473 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %474 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%473)
  %475 : __torch__.torch.nn.modules.module.___torch_mangle_167.Module = prim::GetAttr[name="tree1"](%474)
  %476 : __torch__.torch.nn.modules.module.___torch_mangle_148.Module = prim::GetAttr[name="tree1"](%475)
  %477 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_144.QuantConv2d = prim::GetAttr[name="conv2"](%476)
  %479 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %480 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%479)
  %481 : __torch__.torch.nn.modules.module.___torch_mangle_167.Module = prim::GetAttr[name="tree1"](%480)
  %482 : __torch__.torch.nn.modules.module.___torch_mangle_148.Module = prim::GetAttr[name="tree1"](%481)
  %483 : __torch__.torch.nn.modules.activation.___torch_mangle_141.ReLU = prim::GetAttr[name="relu"](%482)
  %485 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %486 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%485)
  %487 : __torch__.torch.nn.modules.module.___torch_mangle_167.Module = prim::GetAttr[name="tree1"](%486)
  %488 : __torch__.torch.nn.modules.module.___torch_mangle_148.Module = prim::GetAttr[name="tree1"](%487)
  %489 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_140.BatchNorm2d = prim::GetAttr[name="bn1"](%488)
  %491 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %492 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%491)
  %493 : __torch__.torch.nn.modules.module.___torch_mangle_167.Module = prim::GetAttr[name="tree1"](%492)
  %494 : __torch__.torch.nn.modules.module.___torch_mangle_148.Module = prim::GetAttr[name="tree1"](%493)
  %495 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_139.QuantConv2d = prim::GetAttr[name="conv1"](%494)
  %497 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %498 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%497)
  %499 : __torch__.torch.nn.modules.module.___torch_mangle_167.Module = prim::GetAttr[name="tree1"](%498)
  %500 : __torch__.torch.nn.modules.module.___torch_mangle_136.Module = prim::GetAttr[name="project"](%499)
  %501 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_identify.___torch_mangle_135.QuantIdentity = prim::GetAttr[name="1_output_quant"](%500)
  %503 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %504 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%503)
  %505 : __torch__.torch.nn.modules.module.___torch_mangle_167.Module = prim::GetAttr[name="tree1"](%504)
  %506 : __torch__.torch.nn.modules.module.___torch_mangle_136.Module = prim::GetAttr[name="project"](%505)
  %507 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_133.BatchNorm2d = prim::GetAttr[name="1"](%506)
  %509 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %510 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%509)
  %511 : __torch__.torch.nn.modules.module.___torch_mangle_167.Module = prim::GetAttr[name="tree1"](%510)
  %512 : __torch__.torch.nn.modules.module.___torch_mangle_136.Module = prim::GetAttr[name="project"](%511)
  %513 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_132.QuantConv2d = prim::GetAttr[name="0"](%512)
  %515 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %516 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%515)
  %517 : __torch__.torch.nn.modules.module.___torch_mangle_167.Module = prim::GetAttr[name="tree1"](%516)
  %518 : __torch__.torch.nn.modules.pooling.___torch_mangle_129.MaxPool2d = prim::GetAttr[name="downsample"](%517)
  %520 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %521 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%520)
  %522 : __torch__.torch.nn.modules.module.___torch_mangle_128.Module = prim::GetAttr[name="project"](%521)
  %523 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_127.BatchNorm2d = prim::GetAttr[name="1"](%522)
  %525 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %526 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%525)
  %527 : __torch__.torch.nn.modules.module.___torch_mangle_128.Module = prim::GetAttr[name="project"](%526)
  %528 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_126.QuantConv2d = prim::GetAttr[name="0"](%527)
  %530 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %531 : __torch__.torch.nn.modules.module.___torch_mangle_197.Module = prim::GetAttr[name="level4"](%530)
  %532 : __torch__.torch.nn.modules.pooling.___torch_mangle_123.MaxPool2d = prim::GetAttr[name="downsample"](%531)
  %534 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %535 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%534)
  %536 : __torch__.torch.nn.modules.module.___torch_mangle_121.Module = prim::GetAttr[name="tree2"](%535)
  %537 : __torch__.torch.nn.modules.module.___torch_mangle_120.Module = prim::GetAttr[name="root"](%536)
  %538 : __torch__.torch.nn.modules.activation.___torch_mangle_119.ReLU = prim::GetAttr[name="relu"](%537)
  %540 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %541 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%540)
  %542 : __torch__.torch.nn.modules.module.___torch_mangle_121.Module = prim::GetAttr[name="tree2"](%541)
  %543 : __torch__.torch.nn.modules.module.___torch_mangle_120.Module = prim::GetAttr[name="root"](%542)
  %544 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_118.BatchNorm2d = prim::GetAttr[name="bn"](%543)
  %546 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %547 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%546)
  %548 : __torch__.torch.nn.modules.module.___torch_mangle_121.Module = prim::GetAttr[name="tree2"](%547)
  %549 : __torch__.torch.nn.modules.module.___torch_mangle_120.Module = prim::GetAttr[name="root"](%548)
  %550 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_117.QuantConv2d = prim::GetAttr[name="conv"](%549)
  %552 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %553 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%552)
  %554 : __torch__.torch.nn.modules.module.___torch_mangle_121.Module = prim::GetAttr[name="tree2"](%553)
  %555 : __torch__.torch.nn.modules.module.___torch_mangle_114.Module = prim::GetAttr[name="tree2"](%554)
  %556 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_113.BatchNorm2d = prim::GetAttr[name="bn2"](%555)
  %558 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %559 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%558)
  %560 : __torch__.torch.nn.modules.module.___torch_mangle_121.Module = prim::GetAttr[name="tree2"](%559)
  %561 : __torch__.torch.nn.modules.module.___torch_mangle_114.Module = prim::GetAttr[name="tree2"](%560)
  %562 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_112.QuantConv2d = prim::GetAttr[name="conv2"](%561)
  %564 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %565 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%564)
  %566 : __torch__.torch.nn.modules.module.___torch_mangle_121.Module = prim::GetAttr[name="tree2"](%565)
  %567 : __torch__.torch.nn.modules.module.___torch_mangle_114.Module = prim::GetAttr[name="tree2"](%566)
  %568 : __torch__.torch.nn.modules.activation.___torch_mangle_109.ReLU = prim::GetAttr[name="relu"](%567)
  %570 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %571 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%570)
  %572 : __torch__.torch.nn.modules.module.___torch_mangle_121.Module = prim::GetAttr[name="tree2"](%571)
  %573 : __torch__.torch.nn.modules.module.___torch_mangle_114.Module = prim::GetAttr[name="tree2"](%572)
  %574 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_108.BatchNorm2d = prim::GetAttr[name="bn1"](%573)
  %576 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %577 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%576)
  %578 : __torch__.torch.nn.modules.module.___torch_mangle_121.Module = prim::GetAttr[name="tree2"](%577)
  %579 : __torch__.torch.nn.modules.module.___torch_mangle_114.Module = prim::GetAttr[name="tree2"](%578)
  %580 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_107.QuantConv2d = prim::GetAttr[name="conv1"](%579)
  %582 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %583 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%582)
  %584 : __torch__.torch.nn.modules.module.___torch_mangle_121.Module = prim::GetAttr[name="tree2"](%583)
  %585 : __torch__.torch.nn.modules.module.___torch_mangle_104.Module = prim::GetAttr[name="tree1"](%584)
  %586 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_identify.___torch_mangle_103.QuantIdentity = prim::GetAttr[name="relu_output_quant"](%585)
  %588 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %589 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%588)
  %590 : __torch__.torch.nn.modules.module.___torch_mangle_121.Module = prim::GetAttr[name="tree2"](%589)
  %591 : __torch__.torch.nn.modules.module.___torch_mangle_104.Module = prim::GetAttr[name="tree1"](%590)
  %592 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_101.BatchNorm2d = prim::GetAttr[name="bn2"](%591)
  %594 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %595 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%594)
  %596 : __torch__.torch.nn.modules.module.___torch_mangle_121.Module = prim::GetAttr[name="tree2"](%595)
  %597 : __torch__.torch.nn.modules.module.___torch_mangle_104.Module = prim::GetAttr[name="tree1"](%596)
  %598 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_100.QuantConv2d = prim::GetAttr[name="conv2"](%597)
  %600 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %601 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%600)
  %602 : __torch__.torch.nn.modules.module.___torch_mangle_121.Module = prim::GetAttr[name="tree2"](%601)
  %603 : __torch__.torch.nn.modules.module.___torch_mangle_104.Module = prim::GetAttr[name="tree1"](%602)
  %604 : __torch__.torch.nn.modules.activation.___torch_mangle_97.ReLU = prim::GetAttr[name="relu"](%603)
  %606 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %607 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%606)
  %608 : __torch__.torch.nn.modules.module.___torch_mangle_121.Module = prim::GetAttr[name="tree2"](%607)
  %609 : __torch__.torch.nn.modules.module.___torch_mangle_104.Module = prim::GetAttr[name="tree1"](%608)
  %610 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_96.BatchNorm2d = prim::GetAttr[name="bn1"](%609)
  %612 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %613 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%612)
  %614 : __torch__.torch.nn.modules.module.___torch_mangle_121.Module = prim::GetAttr[name="tree2"](%613)
  %615 : __torch__.torch.nn.modules.module.___torch_mangle_104.Module = prim::GetAttr[name="tree1"](%614)
  %616 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_95.QuantConv2d = prim::GetAttr[name="conv1"](%615)
  %618 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %619 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%618)
  %620 : __torch__.torch.nn.modules.module.___torch_mangle_92.Module = prim::GetAttr[name="tree1"](%619)
  %621 : __torch__.torch.nn.modules.module.___torch_mangle_91.Module = prim::GetAttr[name="root"](%620)
  %622 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_identify.___torch_mangle_90.QuantIdentity = prim::GetAttr[name="relu_output_quant"](%621)
  %624 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %625 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%624)
  %626 : __torch__.torch.nn.modules.module.___torch_mangle_92.Module = prim::GetAttr[name="tree1"](%625)
  %627 : __torch__.torch.nn.modules.module.___torch_mangle_91.Module = prim::GetAttr[name="root"](%626)
  %628 : __torch__.torch.nn.modules.activation.___torch_mangle_88.ReLU = prim::GetAttr[name="relu"](%627)
  %630 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %631 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%630)
  %632 : __torch__.torch.nn.modules.module.___torch_mangle_92.Module = prim::GetAttr[name="tree1"](%631)
  %633 : __torch__.torch.nn.modules.module.___torch_mangle_91.Module = prim::GetAttr[name="root"](%632)
  %634 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_87.BatchNorm2d = prim::GetAttr[name="bn"](%633)
  %636 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %637 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%636)
  %638 : __torch__.torch.nn.modules.module.___torch_mangle_92.Module = prim::GetAttr[name="tree1"](%637)
  %639 : __torch__.torch.nn.modules.module.___torch_mangle_91.Module = prim::GetAttr[name="root"](%638)
  %640 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_86.QuantConv2d = prim::GetAttr[name="conv"](%639)
  %642 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %643 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%642)
  %644 : __torch__.torch.nn.modules.module.___torch_mangle_92.Module = prim::GetAttr[name="tree1"](%643)
  %645 : __torch__.torch.nn.modules.module.___torch_mangle_83.Module = prim::GetAttr[name="tree2"](%644)
  %646 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_82.BatchNorm2d = prim::GetAttr[name="bn2"](%645)
  %648 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %649 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%648)
  %650 : __torch__.torch.nn.modules.module.___torch_mangle_92.Module = prim::GetAttr[name="tree1"](%649)
  %651 : __torch__.torch.nn.modules.module.___torch_mangle_83.Module = prim::GetAttr[name="tree2"](%650)
  %652 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_81.QuantConv2d = prim::GetAttr[name="conv2"](%651)
  %654 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %655 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%654)
  %656 : __torch__.torch.nn.modules.module.___torch_mangle_92.Module = prim::GetAttr[name="tree1"](%655)
  %657 : __torch__.torch.nn.modules.module.___torch_mangle_83.Module = prim::GetAttr[name="tree2"](%656)
  %658 : __torch__.torch.nn.modules.activation.___torch_mangle_78.ReLU = prim::GetAttr[name="relu"](%657)
  %660 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %661 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%660)
  %662 : __torch__.torch.nn.modules.module.___torch_mangle_92.Module = prim::GetAttr[name="tree1"](%661)
  %663 : __torch__.torch.nn.modules.module.___torch_mangle_83.Module = prim::GetAttr[name="tree2"](%662)
  %664 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_77.BatchNorm2d = prim::GetAttr[name="bn1"](%663)
  %666 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %667 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%666)
  %668 : __torch__.torch.nn.modules.module.___torch_mangle_92.Module = prim::GetAttr[name="tree1"](%667)
  %669 : __torch__.torch.nn.modules.module.___torch_mangle_83.Module = prim::GetAttr[name="tree2"](%668)
  %670 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_76.QuantConv2d = prim::GetAttr[name="conv1"](%669)
  %672 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %673 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%672)
  %674 : __torch__.torch.nn.modules.module.___torch_mangle_92.Module = prim::GetAttr[name="tree1"](%673)
  %675 : __torch__.torch.nn.modules.module.___torch_mangle_73.Module = prim::GetAttr[name="tree1"](%674)
  %676 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_identify.___torch_mangle_72.QuantIdentity = prim::GetAttr[name="relu_output_quant"](%675)
  %678 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %679 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%678)
  %680 : __torch__.torch.nn.modules.module.___torch_mangle_92.Module = prim::GetAttr[name="tree1"](%679)
  %681 : __torch__.torch.nn.modules.module.___torch_mangle_73.Module = prim::GetAttr[name="tree1"](%680)
  %682 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_70.BatchNorm2d = prim::GetAttr[name="bn2"](%681)
  %684 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %685 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%684)
  %686 : __torch__.torch.nn.modules.module.___torch_mangle_92.Module = prim::GetAttr[name="tree1"](%685)
  %687 : __torch__.torch.nn.modules.module.___torch_mangle_73.Module = prim::GetAttr[name="tree1"](%686)
  %688 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_69.QuantConv2d = prim::GetAttr[name="conv2"](%687)
  %690 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %691 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%690)
  %692 : __torch__.torch.nn.modules.module.___torch_mangle_92.Module = prim::GetAttr[name="tree1"](%691)
  %693 : __torch__.torch.nn.modules.module.___torch_mangle_73.Module = prim::GetAttr[name="tree1"](%692)
  %694 : __torch__.torch.nn.modules.activation.___torch_mangle_66.ReLU = prim::GetAttr[name="relu"](%693)
  %696 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %697 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%696)
  %698 : __torch__.torch.nn.modules.module.___torch_mangle_92.Module = prim::GetAttr[name="tree1"](%697)
  %699 : __torch__.torch.nn.modules.module.___torch_mangle_73.Module = prim::GetAttr[name="tree1"](%698)
  %700 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_65.BatchNorm2d = prim::GetAttr[name="bn1"](%699)
  %702 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %703 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%702)
  %704 : __torch__.torch.nn.modules.module.___torch_mangle_92.Module = prim::GetAttr[name="tree1"](%703)
  %705 : __torch__.torch.nn.modules.module.___torch_mangle_73.Module = prim::GetAttr[name="tree1"](%704)
  %706 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_64.QuantConv2d = prim::GetAttr[name="conv1"](%705)
  %708 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %709 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%708)
  %710 : __torch__.torch.nn.modules.module.___torch_mangle_92.Module = prim::GetAttr[name="tree1"](%709)
  %711 : __torch__.torch.nn.modules.module.___torch_mangle_61.Module = prim::GetAttr[name="project"](%710)
  %712 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_identify.___torch_mangle_60.QuantIdentity = prim::GetAttr[name="1_output_quant"](%711)
  %714 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %715 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%714)
  %716 : __torch__.torch.nn.modules.module.___torch_mangle_92.Module = prim::GetAttr[name="tree1"](%715)
  %717 : __torch__.torch.nn.modules.module.___torch_mangle_61.Module = prim::GetAttr[name="project"](%716)
  %718 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_58.BatchNorm2d = prim::GetAttr[name="1"](%717)
  %720 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %721 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%720)
  %722 : __torch__.torch.nn.modules.module.___torch_mangle_92.Module = prim::GetAttr[name="tree1"](%721)
  %723 : __torch__.torch.nn.modules.module.___torch_mangle_61.Module = prim::GetAttr[name="project"](%722)
  %724 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_57.QuantConv2d = prim::GetAttr[name="0"](%723)
  %726 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %727 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%726)
  %728 : __torch__.torch.nn.modules.module.___torch_mangle_92.Module = prim::GetAttr[name="tree1"](%727)
  %729 : __torch__.torch.nn.modules.pooling.___torch_mangle_54.MaxPool2d = prim::GetAttr[name="downsample"](%728)
  %731 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %732 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%731)
  %733 : __torch__.torch.nn.modules.module.___torch_mangle_53.Module = prim::GetAttr[name="project"](%732)
  %734 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_52.BatchNorm2d = prim::GetAttr[name="1"](%733)
  %736 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %737 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%736)
  %738 : __torch__.torch.nn.modules.module.___torch_mangle_53.Module = prim::GetAttr[name="project"](%737)
  %739 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_51.QuantConv2d = prim::GetAttr[name="0"](%738)
  %741 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %742 : __torch__.torch.nn.modules.module.___torch_mangle_122.Module = prim::GetAttr[name="level3"](%741)
  %743 : __torch__.torch.nn.modules.pooling.___torch_mangle_48.MaxPool2d = prim::GetAttr[name="downsample"](%742)
  %745 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %746 : __torch__.torch.nn.modules.module.___torch_mangle_47.Module = prim::GetAttr[name="level2"](%745)
  %747 : __torch__.torch.nn.modules.module.___torch_mangle_46.Module = prim::GetAttr[name="root"](%746)
  %748 : __torch__.torch.nn.modules.activation.___torch_mangle_45.ReLU = prim::GetAttr[name="relu"](%747)
  %750 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %751 : __torch__.torch.nn.modules.module.___torch_mangle_47.Module = prim::GetAttr[name="level2"](%750)
  %752 : __torch__.torch.nn.modules.module.___torch_mangle_46.Module = prim::GetAttr[name="root"](%751)
  %753 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_44.BatchNorm2d = prim::GetAttr[name="bn"](%752)
  %755 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %756 : __torch__.torch.nn.modules.module.___torch_mangle_47.Module = prim::GetAttr[name="level2"](%755)
  %757 : __torch__.torch.nn.modules.module.___torch_mangle_46.Module = prim::GetAttr[name="root"](%756)
  %758 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_43.QuantConv2d = prim::GetAttr[name="conv"](%757)
  %760 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %761 : __torch__.torch.nn.modules.module.___torch_mangle_47.Module = prim::GetAttr[name="level2"](%760)
  %762 : __torch__.torch.nn.modules.module.___torch_mangle_40.Module = prim::GetAttr[name="tree2"](%761)
  %763 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_39.BatchNorm2d = prim::GetAttr[name="bn2"](%762)
  %765 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %766 : __torch__.torch.nn.modules.module.___torch_mangle_47.Module = prim::GetAttr[name="level2"](%765)
  %767 : __torch__.torch.nn.modules.module.___torch_mangle_40.Module = prim::GetAttr[name="tree2"](%766)
  %768 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_38.QuantConv2d = prim::GetAttr[name="conv2"](%767)
  %770 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %771 : __torch__.torch.nn.modules.module.___torch_mangle_47.Module = prim::GetAttr[name="level2"](%770)
  %772 : __torch__.torch.nn.modules.module.___torch_mangle_40.Module = prim::GetAttr[name="tree2"](%771)
  %773 : __torch__.torch.nn.modules.activation.___torch_mangle_35.ReLU = prim::GetAttr[name="relu"](%772)
  %775 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %776 : __torch__.torch.nn.modules.module.___torch_mangle_47.Module = prim::GetAttr[name="level2"](%775)
  %777 : __torch__.torch.nn.modules.module.___torch_mangle_40.Module = prim::GetAttr[name="tree2"](%776)
  %778 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_34.BatchNorm2d = prim::GetAttr[name="bn1"](%777)
  %780 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %781 : __torch__.torch.nn.modules.module.___torch_mangle_47.Module = prim::GetAttr[name="level2"](%780)
  %782 : __torch__.torch.nn.modules.module.___torch_mangle_40.Module = prim::GetAttr[name="tree2"](%781)
  %783 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_33.QuantConv2d = prim::GetAttr[name="conv1"](%782)
  %785 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %786 : __torch__.torch.nn.modules.module.___torch_mangle_47.Module = prim::GetAttr[name="level2"](%785)
  %787 : __torch__.torch.nn.modules.module.___torch_mangle_30.Module = prim::GetAttr[name="tree1"](%786)
  %788 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_identify.___torch_mangle_29.QuantIdentity = prim::GetAttr[name="relu_output_quant"](%787)
  %790 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %791 : __torch__.torch.nn.modules.module.___torch_mangle_47.Module = prim::GetAttr[name="level2"](%790)
  %792 : __torch__.torch.nn.modules.module.___torch_mangle_30.Module = prim::GetAttr[name="tree1"](%791)
  %793 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_27.BatchNorm2d = prim::GetAttr[name="bn2"](%792)
  %795 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %796 : __torch__.torch.nn.modules.module.___torch_mangle_47.Module = prim::GetAttr[name="level2"](%795)
  %797 : __torch__.torch.nn.modules.module.___torch_mangle_30.Module = prim::GetAttr[name="tree1"](%796)
  %798 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_26.QuantConv2d = prim::GetAttr[name="conv2"](%797)
  %800 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %801 : __torch__.torch.nn.modules.module.___torch_mangle_47.Module = prim::GetAttr[name="level2"](%800)
  %802 : __torch__.torch.nn.modules.module.___torch_mangle_30.Module = prim::GetAttr[name="tree1"](%801)
  %803 : __torch__.torch.nn.modules.activation.___torch_mangle_23.ReLU = prim::GetAttr[name="relu"](%802)
  %805 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %806 : __torch__.torch.nn.modules.module.___torch_mangle_47.Module = prim::GetAttr[name="level2"](%805)
  %807 : __torch__.torch.nn.modules.module.___torch_mangle_30.Module = prim::GetAttr[name="tree1"](%806)
  %808 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_22.BatchNorm2d = prim::GetAttr[name="bn1"](%807)
  %810 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %811 : __torch__.torch.nn.modules.module.___torch_mangle_47.Module = prim::GetAttr[name="level2"](%810)
  %812 : __torch__.torch.nn.modules.module.___torch_mangle_30.Module = prim::GetAttr[name="tree1"](%811)
  %813 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_21.QuantConv2d = prim::GetAttr[name="conv1"](%812)
  %815 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %816 : __torch__.torch.nn.modules.module.___torch_mangle_47.Module = prim::GetAttr[name="level2"](%815)
  %817 : __torch__.torch.nn.modules.module.___torch_mangle_18.Module = prim::GetAttr[name="project"](%816)
  %818 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_identify.QuantIdentity = prim::GetAttr[name="1_output_quant"](%817)
  %820 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %821 : __torch__.torch.nn.modules.module.___torch_mangle_47.Module = prim::GetAttr[name="level2"](%820)
  %822 : __torch__.torch.nn.modules.module.___torch_mangle_18.Module = prim::GetAttr[name="project"](%821)
  %823 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_16.BatchNorm2d = prim::GetAttr[name="1"](%822)
  %825 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %826 : __torch__.torch.nn.modules.module.___torch_mangle_47.Module = prim::GetAttr[name="level2"](%825)
  %827 : __torch__.torch.nn.modules.module.___torch_mangle_18.Module = prim::GetAttr[name="project"](%826)
  %828 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_15.QuantConv2d = prim::GetAttr[name="0"](%827)
  %830 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %831 : __torch__.torch.nn.modules.module.___torch_mangle_47.Module = prim::GetAttr[name="level2"](%830)
  %832 : __torch__.torch.nn.modules.pooling.MaxPool2d = prim::GetAttr[name="downsample"](%831)
  %834 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %835 : __torch__.torch.nn.modules.module.___torch_mangle_12.Module = prim::GetAttr[name="level1"](%834)
  %836 : __torch__.torch.nn.modules.activation.___torch_mangle_11.ReLU = prim::GetAttr[name="2"](%835)
  %838 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %839 : __torch__.torch.nn.modules.module.___torch_mangle_12.Module = prim::GetAttr[name="level1"](%838)
  %840 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_10.BatchNorm2d = prim::GetAttr[name="1"](%839)
  %842 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %843 : __torch__.torch.nn.modules.module.___torch_mangle_12.Module = prim::GetAttr[name="level1"](%842)
  %844 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_9.QuantConv2d = prim::GetAttr[name="0"](%843)
  %846 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %847 : __torch__.torch.nn.modules.module.___torch_mangle_6.Module = prim::GetAttr[name="level0"](%846)
  %848 : __torch__.torch.nn.modules.activation.___torch_mangle_5.ReLU = prim::GetAttr[name="2"](%847)
  %850 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %851 : __torch__.torch.nn.modules.module.___torch_mangle_6.Module = prim::GetAttr[name="level0"](%850)
  %852 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_4.BatchNorm2d = prim::GetAttr[name="1"](%851)
  %854 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %855 : __torch__.torch.nn.modules.module.___torch_mangle_6.Module = prim::GetAttr[name="level0"](%854)
  %856 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.___torch_mangle_3.QuantConv2d = prim::GetAttr[name="0"](%855)
  %858 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %859 : __torch__.torch.nn.modules.module.Module = prim::GetAttr[name="base_layer"](%858)
  %860 : __torch__.torch.nn.modules.activation.ReLU = prim::GetAttr[name="2"](%859)
  %862 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %863 : __torch__.torch.nn.modules.module.Module = prim::GetAttr[name="base_layer"](%862)
  %864 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name="1"](%863)
  %866 : __torch__.torch.nn.modules.module.___torch_mangle_235.Module = prim::GetAttr[name="base"](%self.1)
  %867 : __torch__.torch.nn.modules.module.Module = prim::GetAttr[name="base_layer"](%866)
  %868 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.quant_conv.QuantConv2d = prim::GetAttr[name="0"](%867)
  %1456 : Tensor = prim::Constant[value=<Tensor>]()
  %1457 : Tensor = prim::Constant[value=<Tensor>]()
  %1458 : float = prim::Constant[value=0.033028970553180367]()
  %1459 : int = prim::Constant[value=0]()
  %1460 : int = prim::Constant[value=-128]()
  %1461 : int = prim::Constant[value=127]()
  %1462 : int = prim::Constant[value=1]()
  %1463 : NoneType = prim::Constant()
  %1464 : int[] = prim::Constant[value=[1, 1]]()
  %1465 : int[] = prim::Constant[value=[3, 3]]()
  %1466 : bool = prim::Constant[value=0]()
  %1467 : int[] = prim::Constant[value=[0, 0]]()
  %1468 : bool = prim::Constant[value=1]()
  %1469 : Tensor = prim::GetAttr[name="weight"]()
  %1470 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_0.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%868)
  %1471 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%868)
  %quant_input.2 : Tensor = aten::fake_quantize_per_tensor_affine(%x.1, %1458, %1459, %1460, %1461)
  %quant_weight.2 : Tensor = aten::fake_quantize_per_channel_affine(%self.base.base_layer.0.weight, %1457, %1456, %1459, %1460, %1461)
  %input.5 : Tensor = aten::_convolution(%quant_input.2, %quant_weight.2, %1463, %1464, %1465, %1464, %1466, %1467, %1462, %1466, %1466, %1468, %1468)
  %1475 : float = prim::Constant[value=1.0000000000000001e-05]()
  %1476 : float = prim::Constant[value=0.10000000000000001]()
  %1477 : bool = prim::Constant[value=0]()
  %1478 : bool = prim::Constant[value=1]()
  %1479 : Tensor = prim::GetAttr[name="running_var"]()
  %1480 : Tensor = prim::GetAttr[name="running_mean"]()
  %1481 : Tensor = prim::GetAttr[name="bias"]()
  %1482 : Tensor = prim::GetAttr[name="weight"]()
  %input.7 : Tensor = aten::batch_norm(%input.5, %self.base.base_layer.1.weight, %self.base.base_layer.1.bias, %self.base.base_layer.1.running_mean, %self.base.base_layer.1.running_var, %1477, %1476, %1475, %1478)
  %1484 : Tensor = aten::relu_(%input.7)
  %1485 : Tensor = prim::Constant[value=<Tensor>]()
  %1486 : Tensor = prim::Constant[value=<Tensor>]()
  %1487 : float = prim::Constant[value=0.12232814999077264]()
  %1488 : int = prim::Constant[value=0]()
  %1489 : int = prim::Constant[value=-128]()
  %1490 : int = prim::Constant[value=127]()
  %1491 : int = prim::Constant[value=1]()
  %1492 : NoneType = prim::Constant()
  %1493 : int[] = prim::Constant[value=[1, 1]]()
  %1494 : bool = prim::Constant[value=0]()
  %1495 : int[] = prim::Constant[value=[0, 0]]()
  %1496 : bool = prim::Constant[value=1]()
  %1497 : Tensor = prim::GetAttr[name="weight"]()
  %1498 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_2.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%856)
  %1499 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_1.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%856)
  %quant_input.4 : Tensor = aten::fake_quantize_per_tensor_affine(%1484, %1487, %1488, %1489, %1490)
  %quant_weight.4 : Tensor = aten::fake_quantize_per_channel_affine(%self.base.level0.0.weight, %1486, %1485, %1488, %1489, %1490)
  %input.9 : Tensor = aten::_convolution(%quant_input.4, %quant_weight.4, %1492, %1493, %1493, %1493, %1494, %1495, %1491, %1494, %1494, %1496, %1496)
  %1503 : float = prim::Constant[value=1.0000000000000001e-05]()
  %1504 : float = prim::Constant[value=0.10000000000000001]()
  %1505 : bool = prim::Constant[value=0]()
  %1506 : bool = prim::Constant[value=1]()
  %1507 : Tensor = prim::GetAttr[name="running_var"]()
  %1508 : Tensor = prim::GetAttr[name="running_mean"]()
  %1509 : Tensor = prim::GetAttr[name="bias"]()
  %1510 : Tensor = prim::GetAttr[name="weight"]()
  %input.11 : Tensor = aten::batch_norm(%input.9, %self.base.level0.1.weight, %self.base.level0.1.bias, %self.base.level0.1.running_mean, %self.base.level0.1.running_var, %1505, %1504, %1503, %1506)
  %1512 : Tensor = aten::relu_(%input.11)
  %1513 : Tensor = prim::Constant[value=<Tensor>]()
  %1514 : Tensor = prim::Constant[value=<Tensor>]()
  %1515 : float = prim::Constant[value=0.24879081605926273]()
  %1516 : int = prim::Constant[value=0]()
  %1517 : int = prim::Constant[value=-128]()
  %1518 : int = prim::Constant[value=127]()
  %1519 : int = prim::Constant[value=1]()
  %1520 : NoneType = prim::Constant()
  %1521 : int[] = prim::Constant[value=[2, 2]]()
  %1522 : int[] = prim::Constant[value=[1, 1]]()
  %1523 : bool = prim::Constant[value=0]()
  %1524 : int[] = prim::Constant[value=[0, 0]]()
  %1525 : bool = prim::Constant[value=1]()
  %1526 : Tensor = prim::GetAttr[name="weight"]()
  %1527 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_8.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%844)
  %1528 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_7.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%844)
  %quant_input.6 : Tensor = aten::fake_quantize_per_tensor_affine(%1512, %1515, %1516, %1517, %1518)
  %quant_weight.6 : Tensor = aten::fake_quantize_per_channel_affine(%self.base.level1.0.weight, %1514, %1513, %1516, %1517, %1518)
  %input.13 : Tensor = aten::_convolution(%quant_input.6, %quant_weight.6, %1520, %1521, %1522, %1522, %1523, %1524, %1519, %1523, %1523, %1525, %1525)
  %1532 : float = prim::Constant[value=1.0000000000000001e-05]()
  %1533 : float = prim::Constant[value=0.10000000000000001]()
  %1534 : bool = prim::Constant[value=0]()
  %1535 : bool = prim::Constant[value=1]()
  %1536 : Tensor = prim::GetAttr[name="running_var"]()
  %1537 : Tensor = prim::GetAttr[name="running_mean"]()
  %1538 : Tensor = prim::GetAttr[name="bias"]()
  %1539 : Tensor = prim::GetAttr[name="weight"]()
  %input.15 : Tensor = aten::batch_norm(%input.13, %self.base.level1.1.weight, %self.base.level1.1.bias, %self.base.level1.1.running_mean, %self.base.level1.1.running_var, %1534, %1533, %1532, %1535)
  %1541 : Tensor = aten::relu_(%input.15)
  %1542 : int[] = prim::Constant[value=[2, 2]]()
  %1543 : int[] = prim::Constant[value=[0, 0]]()
  %1544 : int[] = prim::Constant[value=[1, 1]]()
  %1545 : bool = prim::Constant[value=0]()
  %inputs.5 : Tensor = aten::max_pool2d(%1541, %1542, %1542, %1543, %1544, %1545)
  %1547 : Tensor = prim::Constant[value=<Tensor>]()
  %1548 : Tensor = prim::Constant[value=<Tensor>]()
  %1549 : float = prim::Constant[value=0.20050177987166276]()
  %1550 : int = prim::Constant[value=0]()
  %1551 : int = prim::Constant[value=-128]()
  %1552 : int = prim::Constant[value=127]()
  %1553 : int = prim::Constant[value=1]()
  %1554 : NoneType = prim::Constant()
  %1555 : int[] = prim::Constant[value=[1, 1]]()
  %1556 : int[] = prim::Constant[value=[0, 0]]()
  %1557 : bool = prim::Constant[value=0]()
  %1558 : bool = prim::Constant[value=1]()
  %1559 : Tensor = prim::GetAttr[name="weight"]()
  %1560 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_14.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%828)
  %1561 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_13.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%828)
  %quant_input.8 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs.5, %1549, %1550, %1551, %1552)
  %quant_weight.8 : Tensor = aten::fake_quantize_per_channel_affine(%self.base.level2.project.0.weight, %1548, %1547, %1550, %1551, %1552)
  %input.17 : Tensor = aten::_convolution(%quant_input.8, %quant_weight.8, %1554, %1555, %1556, %1555, %1557, %1556, %1553, %1557, %1557, %1558, %1558)
  %1565 : float = prim::Constant[value=1.0000000000000001e-05]()
  %1566 : float = prim::Constant[value=0.10000000000000001]()
  %1567 : bool = prim::Constant[value=0]()
  %1568 : bool = prim::Constant[value=1]()
  %1569 : Tensor = prim::GetAttr[name="running_var"]()
  %1570 : Tensor = prim::GetAttr[name="running_mean"]()
  %1571 : Tensor = prim::GetAttr[name="bias"]()
  %1572 : Tensor = prim::GetAttr[name="weight"]()
  %inputs.7 : Tensor = aten::batch_norm(%input.17, %self.base.level2.project.1.weight, %self.base.level2.project.1.bias, %self.base.level2.project.1.running_mean, %self.base.level2.project.1.running_var, %1567, %1566, %1565, %1568)
  %1574 : float = prim::Constant[value=0.56899291511595718]()
  %1575 : int = prim::Constant[value=0]()
  %1576 : int = prim::Constant[value=-128]()
  %1577 : int = prim::Constant[value=127]()
  %1578 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_17.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%818)
  %base_level2_project_1_output_quant.1 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs.7, %1574, %1575, %1576, %1577)
  %1580 : Tensor = prim::Constant[value=<Tensor>]()
  %1581 : Tensor = prim::Constant[value=<Tensor>]()
  %1582 : float = prim::Constant[value=0.20050177987166276]()
  %1583 : int = prim::Constant[value=0]()
  %1584 : int = prim::Constant[value=-128]()
  %1585 : int = prim::Constant[value=127]()
  %1586 : int = prim::Constant[value=1]()
  %1587 : NoneType = prim::Constant()
  %1588 : int[] = prim::Constant[value=[2, 2]]()
  %1589 : int[] = prim::Constant[value=[1, 1]]()
  %1590 : bool = prim::Constant[value=0]()
  %1591 : int[] = prim::Constant[value=[0, 0]]()
  %1592 : bool = prim::Constant[value=1]()
  %1593 : Tensor = prim::GetAttr[name="weight"]()
  %1594 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_20.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%813)
  %1595 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_19.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%813)
  %quant_input.10 : Tensor = aten::fake_quantize_per_tensor_affine(%1541, %1582, %1583, %1584, %1585)
  %quant_weight.10 : Tensor = aten::fake_quantize_per_channel_affine(%self.base.level2.tree1.conv1.weight, %1581, %1580, %1583, %1584, %1585)
  %input.19 : Tensor = aten::_convolution(%quant_input.10, %quant_weight.10, %1587, %1588, %1589, %1589, %1590, %1591, %1586, %1590, %1590, %1592, %1592)
  %1599 : float = prim::Constant[value=1.0000000000000001e-05]()
  %1600 : float = prim::Constant[value=0.10000000000000001]()
  %1601 : bool = prim::Constant[value=0]()
  %1602 : bool = prim::Constant[value=1]()
  %1603 : Tensor = prim::GetAttr[name="running_var"]()
  %1604 : Tensor = prim::GetAttr[name="running_mean"]()
  %1605 : Tensor = prim::GetAttr[name="bias"]()
  %1606 : Tensor = prim::GetAttr[name="weight"]()
  %input.21 : Tensor = aten::batch_norm(%input.19, %self.base.level2.tree1.bn1.weight, %self.base.level2.tree1.bn1.bias, %self.base.level2.tree1.bn1.running_mean, %self.base.level2.tree1.bn1.running_var, %1601, %1600, %1599, %1602)
  %1608 : Tensor = aten::relu_(%input.21)
  %1609 : Tensor = prim::Constant[value=<Tensor>]()
  %1610 : Tensor = prim::Constant[value=<Tensor>]()
  %1611 : float = prim::Constant[value=0.087626915278397208]()
  %1612 : int = prim::Constant[value=0]()
  %1613 : int = prim::Constant[value=-128]()
  %1614 : int = prim::Constant[value=127]()
  %1615 : int = prim::Constant[value=1]()
  %1616 : NoneType = prim::Constant()
  %1617 : int[] = prim::Constant[value=[1, 1]]()
  %1618 : bool = prim::Constant[value=0]()
  %1619 : int[] = prim::Constant[value=[0, 0]]()
  %1620 : bool = prim::Constant[value=1]()
  %1621 : Tensor = prim::GetAttr[name="weight"]()
  %1622 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_25.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%798)
  %1623 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_24.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%798)
  %quant_input.12 : Tensor = aten::fake_quantize_per_tensor_affine(%1608, %1611, %1612, %1613, %1614)
  %quant_weight.12 : Tensor = aten::fake_quantize_per_channel_affine(%self.base.level2.tree1.conv2.weight, %1610, %1609, %1612, %1613, %1614)
  %input.23 : Tensor = aten::_convolution(%quant_input.12, %quant_weight.12, %1616, %1617, %1617, %1617, %1618, %1619, %1615, %1618, %1618, %1620, %1620)
  %1627 : float = prim::Constant[value=1.0000000000000001e-05]()
  %1628 : float = prim::Constant[value=0.10000000000000001]()
  %1629 : bool = prim::Constant[value=0]()
  %1630 : bool = prim::Constant[value=1]()
  %1631 : Tensor = prim::GetAttr[name="running_var"]()
  %1632 : Tensor = prim::GetAttr[name="running_mean"]()
  %1633 : Tensor = prim::GetAttr[name="bias"]()
  %1634 : Tensor = prim::GetAttr[name="weight"]()
  %base_level2_tree1_bn2.1 : Tensor = aten::batch_norm(%input.23, %self.base.level2.tree1.bn2.weight, %self.base.level2.tree1.bn2.bias, %self.base.level2.tree1.bn2.running_mean, %self.base.level2.tree1.bn2.running_var, %1629, %1628, %1627, %1630)
  %input.3 : Tensor = aten::add(%base_level2_tree1_bn2.1, %base_level2_project_1_output_quant.1, %944)
  %1636 : Tensor = aten::relu_(%input.3)
  %1637 : float = prim::Constant[value=0.064039365513118229]()
  %1638 : int = prim::Constant[value=0]()
  %1639 : int = prim::Constant[value=-128]()
  %1640 : int = prim::Constant[value=127]()
  %1641 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_28.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%788)
  %base_level2_tree1_relu_output_quant.1 : Tensor = aten::fake_quantize_per_tensor_affine(%1636, %1637, %1638, %1639, %1640)
  %1643 : Tensor = prim::Constant[value=<Tensor>]()
  %1644 : Tensor = prim::Constant[value=<Tensor>]()
  %1645 : float = prim::Constant[value=0.064039365513118229]()
  %1646 : int = prim::Constant[value=0]()
  %1647 : int = prim::Constant[value=-128]()
  %1648 : int = prim::Constant[value=127]()
  %1649 : int = prim::Constant[value=1]()
  %1650 : NoneType = prim::Constant()
  %1651 : int[] = prim::Constant[value=[1, 1]]()
  %1652 : bool = prim::Constant[value=0]()
  %1653 : int[] = prim::Constant[value=[0, 0]]()
  %1654 : bool = prim::Constant[value=1]()
  %1655 : Tensor = prim::GetAttr[name="weight"]()
  %1656 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_32.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%783)
  %1657 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_31.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%783)
  %quant_input.14 : Tensor = aten::fake_quantize_per_tensor_affine(%1636, %1645, %1646, %1647, %1648)
  %quant_weight.14 : Tensor = aten::fake_quantize_per_channel_affine(%self.base.level2.tree2.conv1.weight, %1644, %1643, %1646, %1647, %1648)
  %input.25 : Tensor = aten::_convolution(%quant_input.14, %quant_weight.14, %1650, %1651, %1651, %1651, %1652, %1653, %1649, %1652, %1652, %1654, %1654)
  %1661 : float = prim::Constant[value=1.0000000000000001e-05]()
  %1662 : float = prim::Constant[value=0.10000000000000001]()
  %1663 : bool = prim::Constant[value=0]()
  %1664 : bool = prim::Constant[value=1]()
  %1665 : Tensor = prim::GetAttr[name="running_var"]()
  %1666 : Tensor = prim::GetAttr[name="running_mean"]()
  %1667 : Tensor = prim::GetAttr[name="bias"]()
  %1668 : Tensor = prim::GetAttr[name="weight"]()
  %input.27 : Tensor = aten::batch_norm(%input.25, %self.base.level2.tree2.bn1.weight, %self.base.level2.tree2.bn1.bias, %self.base.level2.tree2.bn1.running_mean, %self.base.level2.tree2.bn1.running_var, %1663, %1662, %1661, %1664)
  %1670 : Tensor = aten::relu_(%input.27)
  %1671 : Tensor = prim::Constant[value=<Tensor>]()
  %1672 : Tensor = prim::Constant[value=<Tensor>]()
  %1673 : float = prim::Constant[value=0.047357044820710431]()
  %1674 : int = prim::Constant[value=0]()
  %1675 : int = prim::Constant[value=-128]()
  %1676 : int = prim::Constant[value=127]()
  %1677 : int = prim::Constant[value=1]()
  %1678 : NoneType = prim::Constant()
  %1679 : int[] = prim::Constant[value=[1, 1]]()
  %1680 : bool = prim::Constant[value=0]()
  %1681 : int[] = prim::Constant[value=[0, 0]]()
  %1682 : bool = prim::Constant[value=1]()
  %1683 : Tensor = prim::GetAttr[name="weight"]()
  %1684 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_37.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%768)
  %1685 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_36.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%768)
  %quant_input.16 : Tensor = aten::fake_quantize_per_tensor_affine(%1670, %1673, %1674, %1675, %1676)
  %quant_weight.16 : Tensor = aten::fake_quantize_per_channel_affine(%self.base.level2.tree2.conv2.weight, %1672, %1671, %1674, %1675, %1676)
  %input.29 : Tensor = aten::_convolution(%quant_input.16, %quant_weight.16, %1678, %1679, %1679, %1679, %1680, %1681, %1677, %1680, %1680, %1682, %1682)
  %1689 : float = prim::Constant[value=1.0000000000000001e-05]()
  %1690 : float = prim::Constant[value=0.10000000000000001]()
  %1691 : bool = prim::Constant[value=0]()
  %1692 : bool = prim::Constant[value=1]()
  %1693 : Tensor = prim::GetAttr[name="running_var"]()
  %1694 : Tensor = prim::GetAttr[name="running_mean"]()
  %1695 : Tensor = prim::GetAttr[name="bias"]()
  %1696 : Tensor = prim::GetAttr[name="weight"]()
  %base_level2_tree2_bn2.1 : Tensor = aten::batch_norm(%input.29, %self.base.level2.tree2.bn2.weight, %self.base.level2.tree2.bn2.bias, %self.base.level2.tree2.bn2.running_mean, %self.base.level2.tree2.bn2.running_var, %1691, %1690, %1689, %1692)
  %input0.1 : Tensor = aten::add(%base_level2_tree2_bn2.1, %base_level2_tree1_relu_output_quant.1, %944)
  %1698 : Tensor = aten::relu_(%input0.1)
  %943 : Tensor[] = prim::ListConstruct(%1698, %1636)
  %inputs.3 : Tensor = aten::cat(%943, %944)
  %1699 : Tensor = prim::Constant[value=<Tensor>]()
  %1700 : Tensor = prim::Constant[value=<Tensor>]()
  %1701 : float = prim::Constant[value=0.10902035330224225]()
  %1702 : int = prim::Constant[value=0]()
  %1703 : int = prim::Constant[value=-128]()
  %1704 : int = prim::Constant[value=127]()
  %1705 : int = prim::Constant[value=1]()
  %1706 : NoneType = prim::Constant()
  %1707 : int[] = prim::Constant[value=[1, 1]]()
  %1708 : int[] = prim::Constant[value=[0, 0]]()
  %1709 : bool = prim::Constant[value=0]()
  %1710 : bool = prim::Constant[value=1]()
  %1711 : Tensor = prim::GetAttr[name="weight"]()
  %1712 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_42.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%758)
  %1713 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_41.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%758)
  %quant_input.18 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs.3, %1701, %1702, %1703, %1704)
  %quant_weight.18 : Tensor = aten::fake_quantize_per_channel_affine(%self.base.level2.root.conv.weight, %1700, %1699, %1702, %1703, %1704)
  %input.31 : Tensor = aten::_convolution(%quant_input.18, %quant_weight.18, %1706, %1707, %1708, %1707, %1709, %1708, %1705, %1709, %1709, %1710, %1710)
  %1717 : float = prim::Constant[value=1.0000000000000001e-05]()
  %1718 : float = prim::Constant[value=0.10000000000000001]()
  %1719 : bool = prim::Constant[value=0]()
  %1720 : bool = prim::Constant[value=1]()
  %1721 : Tensor = prim::GetAttr[name="running_var"]()
  %1722 : Tensor = prim::GetAttr[name="running_mean"]()
  %1723 : Tensor = prim::GetAttr[name="bias"]()
  %1724 : Tensor = prim::GetAttr[name="weight"]()
  %input.33 : Tensor = aten::batch_norm(%input.31, %self.base.level2.root.bn.weight, %self.base.level2.root.bn.bias, %self.base.level2.root.bn.running_mean, %self.base.level2.root.bn.running_var, %1719, %1718, %1717, %1720)
  %1726 : Tensor = aten::relu_(%input.33)
  %1727 : int[] = prim::Constant[value=[2, 2]]()
  %1728 : int[] = prim::Constant[value=[0, 0]]()
  %1729 : int[] = prim::Constant[value=[1, 1]]()
  %1730 : bool = prim::Constant[value=0]()
  %inputs.9 : Tensor = aten::max_pool2d(%1726, %1727, %1727, %1728, %1729, %1730)
  %1732 : Tensor = prim::Constant[value=<Tensor>]()
  %1733 : Tensor = prim::Constant[value=<Tensor>]()
  %1734 : float = prim::Constant[value=0.068756268719049884]()
  %1735 : int = prim::Constant[value=0]()
  %1736 : int = prim::Constant[value=-128]()
  %1737 : int = prim::Constant[value=127]()
  %1738 : int = prim::Constant[value=1]()
  %1739 : NoneType = prim::Constant()
  %1740 : int[] = prim::Constant[value=[1, 1]]()
  %1741 : int[] = prim::Constant[value=[0, 0]]()
  %1742 : bool = prim::Constant[value=0]()
  %1743 : bool = prim::Constant[value=1]()
  %1744 : Tensor = prim::GetAttr[name="weight"]()
  %1745 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_50.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%739)
  %1746 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_49.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%739)
  %quant_input.20 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs.9, %1734, %1735, %1736, %1737)
  %quant_weight.20 : Tensor = aten::fake_quantize_per_channel_affine(%self.base.level3.project.0.weight, %1733, %1732, %1735, %1736, %1737)
  %input.35 : Tensor = aten::_convolution(%quant_input.20, %quant_weight.20, %1739, %1740, %1741, %1740, %1742, %1741, %1738, %1742, %1742, %1743, %1743)
  %1750 : NoneType = prim::Constant()
  %1751 : int[] = prim::Constant[value=[2, 2]]()
  %1752 : int[] = prim::Constant[value=[0, 0]]()
  %1753 : int[] = prim::Constant[value=[1, 1]]()
  %1754 : bool = prim::Constant[value=0]()
  %inputs.11 : Tensor = aten::max_pool2d(%1726, %1751, %1751, %1752, %1753, %1754)
  %1756 : Tensor = prim::Constant[value=<Tensor>]()
  %1757 : Tensor = prim::Constant[value=<Tensor>]()
  %1758 : float = prim::Constant[value=0.068756268719049884]()
  %1759 : int = prim::Constant[value=0]()
  %1760 : int = prim::Constant[value=-128]()
  %1761 : int = prim::Constant[value=127]()
  %1762 : int = prim::Constant[value=1]()
  %1763 : NoneType = prim::Constant()
  %1764 : int[] = prim::Constant[value=[1, 1]]()
  %1765 : int[] = prim::Constant[value=[0, 0]]()
  %1766 : bool = prim::Constant[value=0]()
  %1767 : bool = prim::Constant[value=1]()
  %1768 : Tensor = prim::GetAttr[name="weight"]()
  %1769 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_56.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%724)
  %1770 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_55.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%724)
  %quant_input.22 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs.11, %1758, %1759, %1760, %1761)
  %quant_weight.22 : Tensor = aten::fake_quantize_per_channel_affine(%self.base.level3.tree1.project.0.weight, %1757, %1756, %1759, %1760, %1761)
  %input.37 : Tensor = aten::_convolution(%quant_input.22, %quant_weight.22, %1763, %1764, %1765, %1764, %1766, %1765, %1762, %1766, %1766, %1767, %1767)
  %1774 : float = prim::Constant[value=1.0000000000000001e-05]()
  %1775 : float = prim::Constant[value=0.10000000000000001]()
  %1776 : bool = prim::Constant[value=0]()
  %1777 : bool = prim::Constant[value=1]()
  %1778 : Tensor = prim::GetAttr[name="running_var"]()
  %1779 : Tensor = prim::GetAttr[name="running_mean"]()
  %1780 : Tensor = prim::GetAttr[name="bias"]()
  %1781 : Tensor = prim::GetAttr[name="weight"]()
  %inputs.13 : Tensor = aten::batch_norm(%input.37, %self.base.level3.tree1.project.1.weight, %self.base.level3.tree1.project.1.bias, %self.base.level3.tree1.project.1.running_mean, %self.base.level3.tree1.project.1.running_var, %1776, %1775, %1774, %1777)
  %1783 : float = prim::Constant[value=0.035285611790934888]()
  %1784 : int = prim::Constant[value=0]()
  %1785 : int = prim::Constant[value=-128]()
  %1786 : int = prim::Constant[value=127]()
  %1787 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_59.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%712)
  %base_level3_tree1_project_1_output_quant.1 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs.13, %1783, %1784, %1785, %1786)
  %1789 : Tensor = prim::Constant[value=<Tensor>]()
  %1790 : Tensor = prim::Constant[value=<Tensor>]()
  %1791 : float = prim::Constant[value=0.068756268719049884]()
  %1792 : int = prim::Constant[value=0]()
  %1793 : int = prim::Constant[value=-128]()
  %1794 : int = prim::Constant[value=127]()
  %1795 : int = prim::Constant[value=1]()
  %1796 : NoneType = prim::Constant()
  %1797 : int[] = prim::Constant[value=[2, 2]]()
  %1798 : int[] = prim::Constant[value=[1, 1]]()
  %1799 : bool = prim::Constant[value=0]()
  %1800 : int[] = prim::Constant[value=[0, 0]]()
  %1801 : bool = prim::Constant[value=1]()
  %1802 : Tensor = prim::GetAttr[name="weight"]()
  %1803 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_63.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%706)
  %1804 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_62.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%706)
  %quant_input.24 : Tensor = aten::fake_quantize_per_tensor_affine(%1726, %1791, %1792, %1793, %1794)
  %quant_weight.24 : Tensor = aten::fake_quantize_per_channel_affine(%self.base.level3.tree1.tree1.conv1.weight, %1790, %1789, %1792, %1793, %1794)
  %input.39 : Tensor = aten::_convolution(%quant_input.24, %quant_weight.24, %1796, %1797, %1798, %1798, %1799, %1800, %1795, %1799, %1799, %1801, %1801)
  %1808 : float = prim::Constant[value=1.0000000000000001e-05]()
  %1809 : float = prim::Constant[value=0.10000000000000001]()
  %1810 : bool = prim::Constant[value=0]()
  %1811 : bool = prim::Constant[value=1]()
  %1812 : Tensor = prim::GetAttr[name="running_var"]()
  %1813 : Tensor = prim::GetAttr[name="running_mean"]()
  %1814 : Tensor = prim::GetAttr[name="bias"]()
  %1815 : Tensor = prim::GetAttr[name="weight"]()
  %input.41 : Tensor = aten::batch_norm(%input.39, %self.base.level3.tree1.tree1.bn1.weight, %self.base.level3.tree1.tree1.bn1.bias, %self.base.level3.tree1.tree1.bn1.running_mean, %self.base.level3.tree1.tree1.bn1.running_var, %1810, %1809, %1808, %1811)
  %1817 : Tensor = aten::relu_(%input.41)
  %1818 : Tensor = prim::Constant[value=<Tensor>]()
  %1819 : Tensor = prim::Constant[value=<Tensor>]()
  %1820 : float = prim::Constant[value=0.037216479384054348]()
  %1821 : int = prim::Constant[value=0]()
  %1822 : int = prim::Constant[value=-128]()
  %1823 : int = prim::Constant[value=127]()
  %1824 : int = prim::Constant[value=1]()
  %1825 : NoneType = prim::Constant()
  %1826 : int[] = prim::Constant[value=[1, 1]]()
  %1827 : bool = prim::Constant[value=0]()
  %1828 : int[] = prim::Constant[value=[0, 0]]()
  %1829 : bool = prim::Constant[value=1]()
  %1830 : Tensor = prim::GetAttr[name="weight"]()
  %1831 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_68.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%688)
  %1832 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_67.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%688)
  %quant_input.26 : Tensor = aten::fake_quantize_per_tensor_affine(%1817, %1820, %1821, %1822, %1823)
  %quant_weight.26 : Tensor = aten::fake_quantize_per_channel_affine(%self.base.level3.tree1.tree1.conv2.weight, %1819, %1818, %1821, %1822, %1823)
  %input.43 : Tensor = aten::_convolution(%quant_input.26, %quant_weight.26, %1825, %1826, %1826, %1826, %1827, %1828, %1824, %1827, %1827, %1829, %1829)
  %1836 : float = prim::Constant[value=1.0000000000000001e-05]()
  %1837 : float = prim::Constant[value=0.10000000000000001]()
  %1838 : bool = prim::Constant[value=0]()
  %1839 : bool = prim::Constant[value=1]()
  %1840 : Tensor = prim::GetAttr[name="running_var"]()
  %1841 : Tensor = prim::GetAttr[name="running_mean"]()
  %1842 : Tensor = prim::GetAttr[name="bias"]()
  %1843 : Tensor = prim::GetAttr[name="weight"]()
  %base_level3_tree1_tree1_bn2.1 : Tensor = aten::batch_norm(%input.43, %self.base.level3.tree1.tree1.bn2.weight, %self.base.level3.tree1.tree1.bn2.bias, %self.base.level3.tree1.tree1.bn2.running_mean, %self.base.level3.tree1.tree1.bn2.running_var, %1838, %1837, %1836, %1839)
  %input1.1 : Tensor = aten::add(%base_level3_tree1_tree1_bn2.1, %base_level3_tree1_project_1_output_quant.1, %944)
  %1845 : Tensor = aten::relu_(%input1.1)
  %1846 : float = prim::Constant[value=0.04412779470128337]()
  %1847 : int = prim::Constant[value=0]()
  %1848 : int = prim::Constant[value=-128]()
  %1849 : int = prim::Constant[value=127]()
  %1850 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_71.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%676)
  %base_level3_tree1_tree1_relu_output_quant.1 : Tensor = aten::fake_quantize_per_tensor_affine(%1845, %1846, %1847, %1848, %1849)
  %1852 : Tensor = prim::Constant[value=<Tensor>]()
  %1853 : Tensor = prim::Constant[value=<Tensor>]()
  %1854 : float = prim::Constant[value=0.04412779470128337]()
  %1855 : int = prim::Constant[value=0]()
  %1856 : int = prim::Constant[value=-128]()
  %1857 : int = prim::Constant[value=127]()
  %1858 : int = prim::Constant[value=1]()
  %1859 : NoneType = prim::Constant()
  %1860 : int[] = prim::Constant[value=[1, 1]]()
  %1861 : bool = prim::Constant[value=0]()
  %1862 : int[] = prim::Constant[value=[0, 0]]()
  %1863 : bool = prim::Constant[value=1]()
  %1864 : Tensor = prim::GetAttr[name="weight"]()
  %1865 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_75.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%670)
  %1866 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_74.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%670)
  %quant_input.28 : Tensor = aten::fake_quantize_per_tensor_affine(%1845, %1854, %1855, %1856, %1857)
  %quant_weight.28 : Tensor = aten::fake_quantize_per_channel_affine(%self.base.level3.tree1.tree2.conv1.weight, %1853, %1852, %1855, %1856, %1857)
  %input.45 : Tensor = aten::_convolution(%quant_input.28, %quant_weight.28, %1859, %1860, %1860, %1860, %1861, %1862, %1858, %1861, %1861, %1863, %1863)
  %1870 : float = prim::Constant[value=1.0000000000000001e-05]()
  %1871 : float = prim::Constant[value=0.10000000000000001]()
  %1872 : bool = prim::Constant[value=0]()
  %1873 : bool = prim::Constant[value=1]()
  %1874 : Tensor = prim::GetAttr[name="running_var"]()
  %1875 : Tensor = prim::GetAttr[name="running_mean"]()
  %1876 : Tensor = prim::GetAttr[name="bias"]()
  %1877 : Tensor = prim::GetAttr[name="weight"]()
  %input.47 : Tensor = aten::batch_norm(%input.45, %self.base.level3.tree1.tree2.bn1.weight, %self.base.level3.tree1.tree2.bn1.bias, %self.base.level3.tree1.tree2.bn1.running_mean, %self.base.level3.tree1.tree2.bn1.running_var, %1872, %1871, %1870, %1873)
  %1879 : Tensor = aten::relu_(%input.47)
  %1880 : Tensor = prim::Constant[value=<Tensor>]()
  %1881 : Tensor = prim::Constant[value=<Tensor>]()
  %1882 : float = prim::Constant[value=0.024860139906875731]()
  %1883 : int = prim::Constant[value=0]()
  %1884 : int = prim::Constant[value=-128]()
  %1885 : int = prim::Constant[value=127]()
  %1886 : int = prim::Constant[value=1]()
  %1887 : NoneType = prim::Constant()
  %1888 : int[] = prim::Constant[value=[1, 1]]()
  %1889 : bool = prim::Constant[value=0]()
  %1890 : int[] = prim::Constant[value=[0, 0]]()
  %1891 : bool = prim::Constant[value=1]()
  %1892 : Tensor = prim::GetAttr[name="weight"]()
  %1893 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_80.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%652)
  %1894 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_79.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%652)
  %quant_input.30 : Tensor = aten::fake_quantize_per_tensor_affine(%1879, %1882, %1883, %1884, %1885)
  %quant_weight.30 : Tensor = aten::fake_quantize_per_channel_affine(%self.base.level3.tree1.tree2.conv2.weight, %1881, %1880, %1883, %1884, %1885)
  %input.49 : Tensor = aten::_convolution(%quant_input.30, %quant_weight.30, %1887, %1888, %1888, %1888, %1889, %1890, %1886, %1889, %1889, %1891, %1891)
  %1898 : float = prim::Constant[value=1.0000000000000001e-05]()
  %1899 : float = prim::Constant[value=0.10000000000000001]()
  %1900 : bool = prim::Constant[value=0]()
  %1901 : bool = prim::Constant[value=1]()
  %1902 : Tensor = prim::GetAttr[name="running_var"]()
  %1903 : Tensor = prim::GetAttr[name="running_mean"]()
  %1904 : Tensor = prim::GetAttr[name="bias"]()
  %1905 : Tensor = prim::GetAttr[name="weight"]()
  %base_level3_tree1_tree2_bn2.1 : Tensor = aten::batch_norm(%input.49, %self.base.level3.tree1.tree2.bn2.weight, %self.base.level3.tree1.tree2.bn2.bias, %self.base.level3.tree1.tree2.bn2.running_mean, %self.base.level3.tree1.tree2.bn2.running_var, %1900, %1899, %1898, %1901)
  %input2.1 : Tensor = aten::add(%base_level3_tree1_tree2_bn2.1, %base_level3_tree1_tree1_relu_output_quant.1, %944)
  %1907 : Tensor = aten::relu_(%input2.1)
  %1014 : Tensor[] = prim::ListConstruct(%1907, %1845)
  %inputs0.1 : Tensor = aten::cat(%1014, %944)
  %1908 : Tensor = prim::Constant[value=<Tensor>]()
  %1909 : Tensor = prim::Constant[value=<Tensor>]()
  %1910 : float = prim::Constant[value=0.055899556227556366]()
  %1911 : int = prim::Constant[value=0]()
  %1912 : int = prim::Constant[value=-128]()
  %1913 : int = prim::Constant[value=127]()
  %1914 : int = prim::Constant[value=1]()
  %1915 : NoneType = prim::Constant()
  %1916 : int[] = prim::Constant[value=[1, 1]]()
  %1917 : int[] = prim::Constant[value=[0, 0]]()
  %1918 : bool = prim::Constant[value=0]()
  %1919 : bool = prim::Constant[value=1]()
  %1920 : Tensor = prim::GetAttr[name="weight"]()
  %1921 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_85.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%640)
  %1922 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_84.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%640)
  %quant_input.32 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs0.1, %1910, %1911, %1912, %1913)
  %quant_weight.32 : Tensor = aten::fake_quantize_per_channel_affine(%self.base.level3.tree1.root.conv.weight, %1909, %1908, %1911, %1912, %1913)
  %input.51 : Tensor = aten::_convolution(%quant_input.32, %quant_weight.32, %1915, %1916, %1917, %1916, %1918, %1917, %1914, %1918, %1918, %1919, %1919)
  %1926 : float = prim::Constant[value=1.0000000000000001e-05]()
  %1927 : float = prim::Constant[value=0.10000000000000001]()
  %1928 : bool = prim::Constant[value=0]()
  %1929 : bool = prim::Constant[value=1]()
  %1930 : Tensor = prim::GetAttr[name="running_var"]()
  %1931 : Tensor = prim::GetAttr[name="running_mean"]()
  %1932 : Tensor = prim::GetAttr[name="bias"]()
  %1933 : Tensor = prim::GetAttr[name="weight"]()
  %input.53 : Tensor = aten::batch_norm(%input.51, %self.base.level3.tree1.root.bn.weight, %self.base.level3.tree1.root.bn.bias, %self.base.level3.tree1.root.bn.running_mean, %self.base.level3.tree1.root.bn.running_var, %1928, %1927, %1926, %1929)
  %1935 : Tensor = aten::relu_(%input.53)
  %1936 : float = prim::Constant[value=0.034769336069662739]()
  %1937 : int = prim::Constant[value=0]()
  %1938 : int = prim::Constant[value=-128]()
  %1939 : int = prim::Constant[value=127]()
  %1940 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_89.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%622)
  %base_level3_tree1_root_relu_output_quant.1 : Tensor = aten::fake_quantize_per_tensor_affine(%1935, %1936, %1937, %1938, %1939)
  %1942 : Tensor = prim::Constant[value=<Tensor>]()
  %1943 : Tensor = prim::Constant[value=<Tensor>]()
  %1944 : float = prim::Constant[value=0.034769336069662739]()
  %1945 : int = prim::Constant[value=0]()
  %1946 : int = prim::Constant[value=-128]()
  %1947 : int = prim::Constant[value=127]()
  %1948 : int = prim::Constant[value=1]()
  %1949 : NoneType = prim::Constant()
  %1950 : int[] = prim::Constant[value=[1, 1]]()
  %1951 : bool = prim::Constant[value=0]()
  %1952 : int[] = prim::Constant[value=[0, 0]]()
  %1953 : bool = prim::Constant[value=1]()
  %1954 : Tensor = prim::GetAttr[name="weight"]()
  %1955 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_94.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%616)
  %1956 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_93.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%616)
  %quant_input.34 : Tensor = aten::fake_quantize_per_tensor_affine(%1935, %1944, %1945, %1946, %1947)
  %quant_weight.34 : Tensor = aten::fake_quantize_per_channel_affine(%self.base.level3.tree2.tree1.conv1.weight, %1943, %1942, %1945, %1946, %1947)
  %input.55 : Tensor = aten::_convolution(%quant_input.34, %quant_weight.34, %1949, %1950, %1950, %1950, %1951, %1952, %1948, %1951, %1951, %1953, %1953)
  %1960 : float = prim::Constant[value=1.0000000000000001e-05]()
  %1961 : float = prim::Constant[value=0.10000000000000001]()
  %1962 : bool = prim::Constant[value=0]()
  %1963 : bool = prim::Constant[value=1]()
  %1964 : Tensor = prim::GetAttr[name="running_var"]()
  %1965 : Tensor = prim::GetAttr[name="running_mean"]()
  %1966 : Tensor = prim::GetAttr[name="bias"]()
  %1967 : Tensor = prim::GetAttr[name="weight"]()
  %input.57 : Tensor = aten::batch_norm(%input.55, %self.base.level3.tree2.tree1.bn1.weight, %self.base.level3.tree2.tree1.bn1.bias, %self.base.level3.tree2.tree1.bn1.running_mean, %self.base.level3.tree2.tree1.bn1.running_var, %1962, %1961, %1960, %1963)
  %1969 : Tensor = aten::relu_(%input.57)
  %1970 : Tensor = prim::Constant[value=<Tensor>]()
  %1971 : Tensor = prim::Constant[value=<Tensor>]()
  %1972 : float = prim::Constant[value=0.036814873612771824]()
  %1973 : int = prim::Constant[value=0]()
  %1974 : int = prim::Constant[value=-128]()
  %1975 : int = prim::Constant[value=127]()
  %1976 : int = prim::Constant[value=1]()
  %1977 : NoneType = prim::Constant()
  %1978 : int[] = prim::Constant[value=[1, 1]]()
  %1979 : bool = prim::Constant[value=0]()
  %1980 : int[] = prim::Constant[value=[0, 0]]()
  %1981 : bool = prim::Constant[value=1]()
  %1982 : Tensor = prim::GetAttr[name="weight"]()
  %1983 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_99.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%598)
  %1984 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_98.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%598)
  %quant_input.36 : Tensor = aten::fake_quantize_per_tensor_affine(%1969, %1972, %1973, %1974, %1975)
  %quant_weight.36 : Tensor = aten::fake_quantize_per_channel_affine(%self.base.level3.tree2.tree1.conv2.weight, %1971, %1970, %1973, %1974, %1975)
  %input.59 : Tensor = aten::_convolution(%quant_input.36, %quant_weight.36, %1977, %1978, %1978, %1978, %1979, %1980, %1976, %1979, %1979, %1981, %1981)
  %1988 : float = prim::Constant[value=1.0000000000000001e-05]()
  %1989 : float = prim::Constant[value=0.10000000000000001]()
  %1990 : bool = prim::Constant[value=0]()
  %1991 : bool = prim::Constant[value=1]()
  %1992 : Tensor = prim::GetAttr[name="running_var"]()
  %1993 : Tensor = prim::GetAttr[name="running_mean"]()
  %1994 : Tensor = prim::GetAttr[name="bias"]()
  %1995 : Tensor = prim::GetAttr[name="weight"]()
  %base_level3_tree2_tree1_bn2.1 : Tensor = aten::batch_norm(%input.59, %self.base.level3.tree2.tree1.bn2.weight, %self.base.level3.tree2.tree1.bn2.bias, %self.base.level3.tree2.tree1.bn2.running_mean, %self.base.level3.tree2.tree1.bn2.running_var, %1990, %1989, %1988, %1991)
  %input3.1 : Tensor = aten::add(%base_level3_tree2_tree1_bn2.1, %base_level3_tree1_root_relu_output_quant.1, %944)
  %1997 : Tensor = aten::relu_(%input3.1)
  %1998 : float = prim::Constant[value=0.04428145265954686]()
  %1999 : int = prim::Constant[value=0]()
  %2000 : int = prim::Constant[value=-128]()
  %2001 : int = prim::Constant[value=127]()
  %2002 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_102.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%586)
  %base_level3_tree2_tree1_relu_output_quant.1 : Tensor = aten::fake_quantize_per_tensor_affine(%1997, %1998, %1999, %2000, %2001)
  %2004 : Tensor = prim::Constant[value=<Tensor>]()
  %2005 : Tensor = prim::Constant[value=<Tensor>]()
  %2006 : float = prim::Constant[value=0.04428145265954686]()
  %2007 : int = prim::Constant[value=0]()
  %2008 : int = prim::Constant[value=-128]()
  %2009 : int = prim::Constant[value=127]()
  %2010 : int = prim::Constant[value=1]()
  %2011 : NoneType = prim::Constant()
  %2012 : int[] = prim::Constant[value=[1, 1]]()
  %2013 : bool = prim::Constant[value=0]()
  %2014 : int[] = prim::Constant[value=[0, 0]]()
  %2015 : bool = prim::Constant[value=1]()
  %2016 : Tensor = prim::GetAttr[name="weight"]()
  %2017 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_106.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%580)
  %2018 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_105.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%580)
  %quant_input.38 : Tensor = aten::fake_quantize_per_tensor_affine(%1997, %2006, %2007, %2008, %2009)
  %quant_weight.38 : Tensor = aten::fake_quantize_per_channel_affine(%self.base.level3.tree2.tree2.conv1.weight, %2005, %2004, %2007, %2008, %2009)
  %input.61 : Tensor = aten::_convolution(%quant_input.38, %quant_weight.38, %2011, %2012, %2012, %2012, %2013, %2014, %2010, %2013, %2013, %2015, %2015)
  %2022 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2023 : float = prim::Constant[value=0.10000000000000001]()
  %2024 : bool = prim::Constant[value=0]()
  %2025 : bool = prim::Constant[value=1]()
  %2026 : Tensor = prim::GetAttr[name="running_var"]()
  %2027 : Tensor = prim::GetAttr[name="running_mean"]()
  %2028 : Tensor = prim::GetAttr[name="bias"]()
  %2029 : Tensor = prim::GetAttr[name="weight"]()
  %input.63 : Tensor = aten::batch_norm(%input.61, %self.base.level3.tree2.tree2.bn1.weight, %self.base.level3.tree2.tree2.bn1.bias, %self.base.level3.tree2.tree2.bn1.running_mean, %self.base.level3.tree2.tree2.bn1.running_var, %2024, %2023, %2022, %2025)
  %2031 : Tensor = aten::relu_(%input.63)
  %2032 : Tensor = prim::Constant[value=<Tensor>]()
  %2033 : Tensor = prim::Constant[value=<Tensor>]()
  %2034 : float = prim::Constant[value=0.038084904978594444]()
  %2035 : int = prim::Constant[value=0]()
  %2036 : int = prim::Constant[value=-128]()
  %2037 : int = prim::Constant[value=127]()
  %2038 : int = prim::Constant[value=1]()
  %2039 : NoneType = prim::Constant()
  %2040 : int[] = prim::Constant[value=[1, 1]]()
  %2041 : bool = prim::Constant[value=0]()
  %2042 : int[] = prim::Constant[value=[0, 0]]()
  %2043 : bool = prim::Constant[value=1]()
  %2044 : Tensor = prim::GetAttr[name="weight"]()
  %2045 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_111.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%562)
  %2046 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_110.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%562)
  %quant_input.40 : Tensor = aten::fake_quantize_per_tensor_affine(%2031, %2034, %2035, %2036, %2037)
  %quant_weight.40 : Tensor = aten::fake_quantize_per_channel_affine(%self.base.level3.tree2.tree2.conv2.weight, %2033, %2032, %2035, %2036, %2037)
  %input.65 : Tensor = aten::_convolution(%quant_input.40, %quant_weight.40, %2039, %2040, %2040, %2040, %2041, %2042, %2038, %2041, %2041, %2043, %2043)
  %2050 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2051 : float = prim::Constant[value=0.10000000000000001]()
  %2052 : bool = prim::Constant[value=0]()
  %2053 : bool = prim::Constant[value=1]()
  %2054 : Tensor = prim::GetAttr[name="running_var"]()
  %2055 : Tensor = prim::GetAttr[name="running_mean"]()
  %2056 : Tensor = prim::GetAttr[name="bias"]()
  %2057 : Tensor = prim::GetAttr[name="weight"]()
  %base_level3_tree2_tree2_bn2.1 : Tensor = aten::batch_norm(%input.65, %self.base.level3.tree2.tree2.bn2.weight, %self.base.level3.tree2.tree2.bn2.bias, %self.base.level3.tree2.tree2.bn2.running_mean, %self.base.level3.tree2.tree2.bn2.running_var, %2052, %2051, %2050, %2053)
  %input4.1 : Tensor = aten::add(%base_level3_tree2_tree2_bn2.1, %base_level3_tree2_tree1_relu_output_quant.1, %944)
  %2059 : Tensor = aten::relu_(%input4.1)
  %1071 : Tensor[] = prim::ListConstruct(%2059, %1997, %inputs.9, %1935)
  %inputs1.1 : Tensor = aten::cat(%1071, %944)
  %2060 : Tensor = prim::Constant[value=<Tensor>]()
  %2061 : Tensor = prim::Constant[value=<Tensor>]()
  %2062 : float = prim::Constant[value=0.076764474703570992]()
  %2063 : int = prim::Constant[value=0]()
  %2064 : int = prim::Constant[value=-128]()
  %2065 : int = prim::Constant[value=127]()
  %2066 : int = prim::Constant[value=1]()
  %2067 : NoneType = prim::Constant()
  %2068 : int[] = prim::Constant[value=[1, 1]]()
  %2069 : int[] = prim::Constant[value=[0, 0]]()
  %2070 : bool = prim::Constant[value=0]()
  %2071 : bool = prim::Constant[value=1]()
  %2072 : Tensor = prim::GetAttr[name="weight"]()
  %2073 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_116.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%550)
  %2074 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_115.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%550)
  %quant_input.42 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs1.1, %2062, %2063, %2064, %2065)
  %quant_weight.42 : Tensor = aten::fake_quantize_per_channel_affine(%self.base.level3.tree2.root.conv.weight, %2061, %2060, %2063, %2064, %2065)
  %input.67 : Tensor = aten::_convolution(%quant_input.42, %quant_weight.42, %2067, %2068, %2069, %2068, %2070, %2069, %2066, %2070, %2070, %2071, %2071)
  %2078 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2079 : float = prim::Constant[value=0.10000000000000001]()
  %2080 : bool = prim::Constant[value=0]()
  %2081 : bool = prim::Constant[value=1]()
  %2082 : Tensor = prim::GetAttr[name="running_var"]()
  %2083 : Tensor = prim::GetAttr[name="running_mean"]()
  %2084 : Tensor = prim::GetAttr[name="bias"]()
  %2085 : Tensor = prim::GetAttr[name="weight"]()
  %input.69 : Tensor = aten::batch_norm(%input.67, %self.base.level3.tree2.root.bn.weight, %self.base.level3.tree2.root.bn.bias, %self.base.level3.tree2.root.bn.running_mean, %self.base.level3.tree2.root.bn.running_var, %2080, %2079, %2078, %2081)
  %2087 : Tensor = aten::relu_(%input.69)
  %2088 : int[] = prim::Constant[value=[2, 2]]()
  %2089 : int[] = prim::Constant[value=[0, 0]]()
  %2090 : int[] = prim::Constant[value=[1, 1]]()
  %2091 : bool = prim::Constant[value=0]()
  %inputs.15 : Tensor = aten::max_pool2d(%2087, %2088, %2088, %2089, %2090, %2091)
  %2093 : Tensor = prim::Constant[value=<Tensor>]()
  %2094 : Tensor = prim::Constant[value=<Tensor>]()
  %2095 : float = prim::Constant[value=0.046685489143912247]()
  %2096 : int = prim::Constant[value=0]()
  %2097 : int = prim::Constant[value=-128]()
  %2098 : int = prim::Constant[value=127]()
  %2099 : int = prim::Constant[value=1]()
  %2100 : NoneType = prim::Constant()
  %2101 : int[] = prim::Constant[value=[1, 1]]()
  %2102 : int[] = prim::Constant[value=[0, 0]]()
  %2103 : bool = prim::Constant[value=0]()
  %2104 : bool = prim::Constant[value=1]()
  %2105 : Tensor = prim::GetAttr[name="weight"]()
  %2106 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_125.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%528)
  %2107 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_124.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%528)
  %quant_input.44 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs.15, %2095, %2096, %2097, %2098)
  %quant_weight.44 : Tensor = aten::fake_quantize_per_channel_affine(%self.base.level4.project.0.weight, %2094, %2093, %2096, %2097, %2098)
  %input.71 : Tensor = aten::_convolution(%quant_input.44, %quant_weight.44, %2100, %2101, %2102, %2101, %2103, %2102, %2099, %2103, %2103, %2104, %2104)
  %2111 : NoneType = prim::Constant()
  %2112 : int[] = prim::Constant[value=[2, 2]]()
  %2113 : int[] = prim::Constant[value=[0, 0]]()
  %2114 : int[] = prim::Constant[value=[1, 1]]()
  %2115 : bool = prim::Constant[value=0]()
  %inputs.17 : Tensor = aten::max_pool2d(%2087, %2112, %2112, %2113, %2114, %2115)
  %2117 : Tensor = prim::Constant[value=<Tensor>]()
  %2118 : Tensor = prim::Constant[value=<Tensor>]()
  %2119 : float = prim::Constant[value=0.046685489143912247]()
  %2120 : int = prim::Constant[value=0]()
  %2121 : int = prim::Constant[value=-128]()
  %2122 : int = prim::Constant[value=127]()
  %2123 : int = prim::Constant[value=1]()
  %2124 : NoneType = prim::Constant()
  %2125 : int[] = prim::Constant[value=[1, 1]]()
  %2126 : int[] = prim::Constant[value=[0, 0]]()
  %2127 : bool = prim::Constant[value=0]()
  %2128 : bool = prim::Constant[value=1]()
  %2129 : Tensor = prim::GetAttr[name="weight"]()
  %2130 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_131.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%513)
  %2131 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_130.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%513)
  %quant_input.46 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs.17, %2119, %2120, %2121, %2122)
  %quant_weight.46 : Tensor = aten::fake_quantize_per_channel_affine(%self.base.level4.tree1.project.0.weight, %2118, %2117, %2120, %2121, %2122)
  %input.73 : Tensor = aten::_convolution(%quant_input.46, %quant_weight.46, %2124, %2125, %2126, %2125, %2127, %2126, %2123, %2127, %2127, %2128, %2128)
  %2135 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2136 : float = prim::Constant[value=0.10000000000000001]()
  %2137 : bool = prim::Constant[value=0]()
  %2138 : bool = prim::Constant[value=1]()
  %2139 : Tensor = prim::GetAttr[name="running_var"]()
  %2140 : Tensor = prim::GetAttr[name="running_mean"]()
  %2141 : Tensor = prim::GetAttr[name="bias"]()
  %2142 : Tensor = prim::GetAttr[name="weight"]()
  %inputs.19 : Tensor = aten::batch_norm(%input.73, %self.base.level4.tree1.project.1.weight, %self.base.level4.tree1.project.1.bias, %self.base.level4.tree1.project.1.running_mean, %self.base.level4.tree1.project.1.running_var, %2137, %2136, %2135, %2138)
  %2144 : float = prim::Constant[value=0.038747915132777901]()
  %2145 : int = prim::Constant[value=0]()
  %2146 : int = prim::Constant[value=-128]()
  %2147 : int = prim::Constant[value=127]()
  %2148 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_134.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%501)
  %base_level4_tree1_project_1_output_quant.1 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs.19, %2144, %2145, %2146, %2147)
  %2150 : Tensor = prim::Constant[value=<Tensor>]()
  %2151 : Tensor = prim::Constant[value=<Tensor>]()
  %2152 : float = prim::Constant[value=0.046685489143912247]()
  %2153 : int = prim::Constant[value=0]()
  %2154 : int = prim::Constant[value=-128]()
  %2155 : int = prim::Constant[value=127]()
  %2156 : int = prim::Constant[value=1]()
  %2157 : NoneType = prim::Constant()
  %2158 : int[] = prim::Constant[value=[2, 2]]()
  %2159 : int[] = prim::Constant[value=[1, 1]]()
  %2160 : bool = prim::Constant[value=0]()
  %2161 : int[] = prim::Constant[value=[0, 0]]()
  %2162 : bool = prim::Constant[value=1]()
  %2163 : Tensor = prim::GetAttr[name="weight"]()
  %2164 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_138.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%495)
  %2165 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_137.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%495)
  %quant_input.48 : Tensor = aten::fake_quantize_per_tensor_affine(%2087, %2152, %2153, %2154, %2155)
  %quant_weight.48 : Tensor = aten::fake_quantize_per_channel_affine(%self.base.level4.tree1.tree1.conv1.weight, %2151, %2150, %2153, %2154, %2155)
  %input.75 : Tensor = aten::_convolution(%quant_input.48, %quant_weight.48, %2157, %2158, %2159, %2159, %2160, %2161, %2156, %2160, %2160, %2162, %2162)
  %2169 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2170 : float = prim::Constant[value=0.10000000000000001]()
  %2171 : bool = prim::Constant[value=0]()
  %2172 : bool = prim::Constant[value=1]()
  %2173 : Tensor = prim::GetAttr[name="running_var"]()
  %2174 : Tensor = prim::GetAttr[name="running_mean"]()
  %2175 : Tensor = prim::GetAttr[name="bias"]()
  %2176 : Tensor = prim::GetAttr[name="weight"]()
  %input.77 : Tensor = aten::batch_norm(%input.75, %self.base.level4.tree1.tree1.bn1.weight, %self.base.level4.tree1.tree1.bn1.bias, %self.base.level4.tree1.tree1.bn1.running_mean, %self.base.level4.tree1.tree1.bn1.running_var, %2171, %2170, %2169, %2172)
  %2178 : Tensor = aten::relu_(%input.77)
  %2179 : Tensor = prim::Constant[value=<Tensor>]()
  %2180 : Tensor = prim::Constant[value=<Tensor>]()
  %2181 : float = prim::Constant[value=0.032149284843384751]()
  %2182 : int = prim::Constant[value=0]()
  %2183 : int = prim::Constant[value=-128]()
  %2184 : int = prim::Constant[value=127]()
  %2185 : int = prim::Constant[value=1]()
  %2186 : NoneType = prim::Constant()
  %2187 : int[] = prim::Constant[value=[1, 1]]()
  %2188 : bool = prim::Constant[value=0]()
  %2189 : int[] = prim::Constant[value=[0, 0]]()
  %2190 : bool = prim::Constant[value=1]()
  %2191 : Tensor = prim::GetAttr[name="weight"]()
  %2192 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_143.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%477)
  %2193 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_142.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%477)
  %quant_input.50 : Tensor = aten::fake_quantize_per_tensor_affine(%2178, %2181, %2182, %2183, %2184)
  %quant_weight.50 : Tensor = aten::fake_quantize_per_channel_affine(%self.base.level4.tree1.tree1.conv2.weight, %2180, %2179, %2182, %2183, %2184)
  %input.79 : Tensor = aten::_convolution(%quant_input.50, %quant_weight.50, %2186, %2187, %2187, %2187, %2188, %2189, %2185, %2188, %2188, %2190, %2190)
  %2197 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2198 : float = prim::Constant[value=0.10000000000000001]()
  %2199 : bool = prim::Constant[value=0]()
  %2200 : bool = prim::Constant[value=1]()
  %2201 : Tensor = prim::GetAttr[name="running_var"]()
  %2202 : Tensor = prim::GetAttr[name="running_mean"]()
  %2203 : Tensor = prim::GetAttr[name="bias"]()
  %2204 : Tensor = prim::GetAttr[name="weight"]()
  %base_level4_tree1_tree1_bn2.1 : Tensor = aten::batch_norm(%input.79, %self.base.level4.tree1.tree1.bn2.weight, %self.base.level4.tree1.tree1.bn2.bias, %self.base.level4.tree1.tree1.bn2.running_mean, %self.base.level4.tree1.tree1.bn2.running_var, %2199, %2198, %2197, %2200)
  %input5.1 : Tensor = aten::add(%base_level4_tree1_tree1_bn2.1, %base_level4_tree1_project_1_output_quant.1, %944)
  %2206 : Tensor = aten::relu_(%input5.1)
  %2207 : float = prim::Constant[value=0.041200863094780389]()
  %2208 : int = prim::Constant[value=0]()
  %2209 : int = prim::Constant[value=-128]()
  %2210 : int = prim::Constant[value=127]()
  %2211 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_146.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%465)
  %base_level4_tree1_tree1_relu_output_quant.1 : Tensor = aten::fake_quantize_per_tensor_affine(%2206, %2207, %2208, %2209, %2210)
  %2213 : Tensor = prim::Constant[value=<Tensor>]()
  %2214 : Tensor = prim::Constant[value=<Tensor>]()
  %2215 : float = prim::Constant[value=0.041200863094780389]()
  %2216 : int = prim::Constant[value=0]()
  %2217 : int = prim::Constant[value=-128]()
  %2218 : int = prim::Constant[value=127]()
  %2219 : int = prim::Constant[value=1]()
  %2220 : NoneType = prim::Constant()
  %2221 : int[] = prim::Constant[value=[1, 1]]()
  %2222 : bool = prim::Constant[value=0]()
  %2223 : int[] = prim::Constant[value=[0, 0]]()
  %2224 : bool = prim::Constant[value=1]()
  %2225 : Tensor = prim::GetAttr[name="weight"]()
  %2226 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_150.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%459)
  %2227 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_149.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%459)
  %quant_input.52 : Tensor = aten::fake_quantize_per_tensor_affine(%2206, %2215, %2216, %2217, %2218)
  %quant_weight.52 : Tensor = aten::fake_quantize_per_channel_affine(%self.base.level4.tree1.tree2.conv1.weight, %2214, %2213, %2216, %2217, %2218)
  %input.81 : Tensor = aten::_convolution(%quant_input.52, %quant_weight.52, %2220, %2221, %2221, %2221, %2222, %2223, %2219, %2222, %2222, %2224, %2224)
  %2231 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2232 : float = prim::Constant[value=0.10000000000000001]()
  %2233 : bool = prim::Constant[value=0]()
  %2234 : bool = prim::Constant[value=1]()
  %2235 : Tensor = prim::GetAttr[name="running_var"]()
  %2236 : Tensor = prim::GetAttr[name="running_mean"]()
  %2237 : Tensor = prim::GetAttr[name="bias"]()
  %2238 : Tensor = prim::GetAttr[name="weight"]()
  %input.83 : Tensor = aten::batch_norm(%input.81, %self.base.level4.tree1.tree2.bn1.weight, %self.base.level4.tree1.tree2.bn1.bias, %self.base.level4.tree1.tree2.bn1.running_mean, %self.base.level4.tree1.tree2.bn1.running_var, %2233, %2232, %2231, %2234)
  %2240 : Tensor = aten::relu_(%input.83)
  %2241 : Tensor = prim::Constant[value=<Tensor>]()
  %2242 : Tensor = prim::Constant[value=<Tensor>]()
  %2243 : float = prim::Constant[value=0.041013927910271593]()
  %2244 : int = prim::Constant[value=0]()
  %2245 : int = prim::Constant[value=-128]()
  %2246 : int = prim::Constant[value=127]()
  %2247 : int = prim::Constant[value=1]()
  %2248 : NoneType = prim::Constant()
  %2249 : int[] = prim::Constant[value=[1, 1]]()
  %2250 : bool = prim::Constant[value=0]()
  %2251 : int[] = prim::Constant[value=[0, 0]]()
  %2252 : bool = prim::Constant[value=1]()
  %2253 : Tensor = prim::GetAttr[name="weight"]()
  %2254 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_155.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%441)
  %2255 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_154.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%441)
  %quant_input.54 : Tensor = aten::fake_quantize_per_tensor_affine(%2240, %2243, %2244, %2245, %2246)
  %quant_weight.54 : Tensor = aten::fake_quantize_per_channel_affine(%self.base.level4.tree1.tree2.conv2.weight, %2242, %2241, %2244, %2245, %2246)
  %input.85 : Tensor = aten::_convolution(%quant_input.54, %quant_weight.54, %2248, %2249, %2249, %2249, %2250, %2251, %2247, %2250, %2250, %2252, %2252)
  %2259 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2260 : float = prim::Constant[value=0.10000000000000001]()
  %2261 : bool = prim::Constant[value=0]()
  %2262 : bool = prim::Constant[value=1]()
  %2263 : Tensor = prim::GetAttr[name="running_var"]()
  %2264 : Tensor = prim::GetAttr[name="running_mean"]()
  %2265 : Tensor = prim::GetAttr[name="bias"]()
  %2266 : Tensor = prim::GetAttr[name="weight"]()
  %base_level4_tree1_tree2_bn2.1 : Tensor = aten::batch_norm(%input.85, %self.base.level4.tree1.tree2.bn2.weight, %self.base.level4.tree1.tree2.bn2.bias, %self.base.level4.tree1.tree2.bn2.running_mean, %self.base.level4.tree1.tree2.bn2.running_var, %2261, %2260, %2259, %2262)
  %input6.1 : Tensor = aten::add(%base_level4_tree1_tree2_bn2.1, %base_level4_tree1_tree1_relu_output_quant.1, %944)
  %2268 : Tensor = aten::relu_(%input6.1)
  %1142 : Tensor[] = prim::ListConstruct(%2268, %2206)
  %inputs2.1 : Tensor = aten::cat(%1142, %944)
  %2269 : Tensor = prim::Constant[value=<Tensor>]()
  %2270 : Tensor = prim::Constant[value=<Tensor>]()
  %2271 : float = prim::Constant[value=0.050051929443839967]()
  %2272 : int = prim::Constant[value=0]()
  %2273 : int = prim::Constant[value=-128]()
  %2274 : int = prim::Constant[value=127]()
  %2275 : int = prim::Constant[value=1]()
  %2276 : NoneType = prim::Constant()
  %2277 : int[] = prim::Constant[value=[1, 1]]()
  %2278 : int[] = prim::Constant[value=[0, 0]]()
  %2279 : bool = prim::Constant[value=0]()
  %2280 : bool = prim::Constant[value=1]()
  %2281 : Tensor = prim::GetAttr[name="weight"]()
  %2282 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_160.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%429)
  %2283 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_159.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%429)
  %quant_input.56 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs2.1, %2271, %2272, %2273, %2274)
  %quant_weight.56 : Tensor = aten::fake_quantize_per_channel_affine(%self.base.level4.tree1.root.conv.weight, %2270, %2269, %2272, %2273, %2274)
  %input.87 : Tensor = aten::_convolution(%quant_input.56, %quant_weight.56, %2276, %2277, %2278, %2277, %2279, %2278, %2275, %2279, %2279, %2280, %2280)
  %2287 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2288 : float = prim::Constant[value=0.10000000000000001]()
  %2289 : bool = prim::Constant[value=0]()
  %2290 : bool = prim::Constant[value=1]()
  %2291 : Tensor = prim::GetAttr[name="running_var"]()
  %2292 : Tensor = prim::GetAttr[name="running_mean"]()
  %2293 : Tensor = prim::GetAttr[name="bias"]()
  %2294 : Tensor = prim::GetAttr[name="weight"]()
  %input.89 : Tensor = aten::batch_norm(%input.87, %self.base.level4.tree1.root.bn.weight, %self.base.level4.tree1.root.bn.bias, %self.base.level4.tree1.root.bn.running_mean, %self.base.level4.tree1.root.bn.running_var, %2289, %2288, %2287, %2290)
  %2296 : Tensor = aten::relu_(%input.89)
  %2297 : float = prim::Constant[value=0.034652248142272468]()
  %2298 : int = prim::Constant[value=0]()
  %2299 : int = prim::Constant[value=-128]()
  %2300 : int = prim::Constant[value=127]()
  %2301 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_164.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%411)
  %base_level4_tree1_root_relu_output_quant.1 : Tensor = aten::fake_quantize_per_tensor_affine(%2296, %2297, %2298, %2299, %2300)
  %2303 : Tensor = prim::Constant[value=<Tensor>]()
  %2304 : Tensor = prim::Constant[value=<Tensor>]()
  %2305 : float = prim::Constant[value=0.034652248142272468]()
  %2306 : int = prim::Constant[value=0]()
  %2307 : int = prim::Constant[value=-128]()
  %2308 : int = prim::Constant[value=127]()
  %2309 : int = prim::Constant[value=1]()
  %2310 : NoneType = prim::Constant()
  %2311 : int[] = prim::Constant[value=[1, 1]]()
  %2312 : bool = prim::Constant[value=0]()
  %2313 : int[] = prim::Constant[value=[0, 0]]()
  %2314 : bool = prim::Constant[value=1]()
  %2315 : Tensor = prim::GetAttr[name="weight"]()
  %2316 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_169.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%405)
  %2317 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_168.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%405)
  %quant_input.58 : Tensor = aten::fake_quantize_per_tensor_affine(%2296, %2305, %2306, %2307, %2308)
  %quant_weight.58 : Tensor = aten::fake_quantize_per_channel_affine(%self.base.level4.tree2.tree1.conv1.weight, %2304, %2303, %2306, %2307, %2308)
  %input.91 : Tensor = aten::_convolution(%quant_input.58, %quant_weight.58, %2310, %2311, %2311, %2311, %2312, %2313, %2309, %2312, %2312, %2314, %2314)
  %2321 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2322 : float = prim::Constant[value=0.10000000000000001]()
  %2323 : bool = prim::Constant[value=0]()
  %2324 : bool = prim::Constant[value=1]()
  %2325 : Tensor = prim::GetAttr[name="running_var"]()
  %2326 : Tensor = prim::GetAttr[name="running_mean"]()
  %2327 : Tensor = prim::GetAttr[name="bias"]()
  %2328 : Tensor = prim::GetAttr[name="weight"]()
  %input.93 : Tensor = aten::batch_norm(%input.91, %self.base.level4.tree2.tree1.bn1.weight, %self.base.level4.tree2.tree1.bn1.bias, %self.base.level4.tree2.tree1.bn1.running_mean, %self.base.level4.tree2.tree1.bn1.running_var, %2323, %2322, %2321, %2324)
  %2330 : Tensor = aten::relu_(%input.93)
  %2331 : Tensor = prim::Constant[value=<Tensor>]()
  %2332 : Tensor = prim::Constant[value=<Tensor>]()
  %2333 : float = prim::Constant[value=0.029836707227812037]()
  %2334 : int = prim::Constant[value=0]()
  %2335 : int = prim::Constant[value=-128]()
  %2336 : int = prim::Constant[value=127]()
  %2337 : int = prim::Constant[value=1]()
  %2338 : NoneType = prim::Constant()
  %2339 : int[] = prim::Constant[value=[1, 1]]()
  %2340 : bool = prim::Constant[value=0]()
  %2341 : int[] = prim::Constant[value=[0, 0]]()
  %2342 : bool = prim::Constant[value=1]()
  %2343 : Tensor = prim::GetAttr[name="weight"]()
  %2344 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_174.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%387)
  %2345 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_173.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%387)
  %quant_input.60 : Tensor = aten::fake_quantize_per_tensor_affine(%2330, %2333, %2334, %2335, %2336)
  %quant_weight.60 : Tensor = aten::fake_quantize_per_channel_affine(%self.base.level4.tree2.tree1.conv2.weight, %2332, %2331, %2334, %2335, %2336)
  %input.95 : Tensor = aten::_convolution(%quant_input.60, %quant_weight.60, %2338, %2339, %2339, %2339, %2340, %2341, %2337, %2340, %2340, %2342, %2342)
  %2349 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2350 : float = prim::Constant[value=0.10000000000000001]()
  %2351 : bool = prim::Constant[value=0]()
  %2352 : bool = prim::Constant[value=1]()
  %2353 : Tensor = prim::GetAttr[name="running_var"]()
  %2354 : Tensor = prim::GetAttr[name="running_mean"]()
  %2355 : Tensor = prim::GetAttr[name="bias"]()
  %2356 : Tensor = prim::GetAttr[name="weight"]()
  %base_level4_tree2_tree1_bn2.1 : Tensor = aten::batch_norm(%input.95, %self.base.level4.tree2.tree1.bn2.weight, %self.base.level4.tree2.tree1.bn2.bias, %self.base.level4.tree2.tree1.bn2.running_mean, %self.base.level4.tree2.tree1.bn2.running_var, %2351, %2350, %2349, %2352)
  %input7.1 : Tensor = aten::add(%base_level4_tree2_tree1_bn2.1, %base_level4_tree1_root_relu_output_quant.1, %944)
  %2358 : Tensor = aten::relu_(%input7.1)
  %2359 : float = prim::Constant[value=0.048360787038728009]()
  %2360 : int = prim::Constant[value=0]()
  %2361 : int = prim::Constant[value=-128]()
  %2362 : int = prim::Constant[value=127]()
  %2363 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_177.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%375)
  %base_level4_tree2_tree1_relu_output_quant.1 : Tensor = aten::fake_quantize_per_tensor_affine(%2358, %2359, %2360, %2361, %2362)
  %2365 : Tensor = prim::Constant[value=<Tensor>]()
  %2366 : Tensor = prim::Constant[value=<Tensor>]()
  %2367 : float = prim::Constant[value=0.048360787038728009]()
  %2368 : int = prim::Constant[value=0]()
  %2369 : int = prim::Constant[value=-128]()
  %2370 : int = prim::Constant[value=127]()
  %2371 : int = prim::Constant[value=1]()
  %2372 : NoneType = prim::Constant()
  %2373 : int[] = prim::Constant[value=[1, 1]]()
  %2374 : bool = prim::Constant[value=0]()
  %2375 : int[] = prim::Constant[value=[0, 0]]()
  %2376 : bool = prim::Constant[value=1]()
  %2377 : Tensor = prim::GetAttr[name="weight"]()
  %2378 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_181.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%369)
  %2379 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_180.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%369)
  %quant_input.62 : Tensor = aten::fake_quantize_per_tensor_affine(%2358, %2367, %2368, %2369, %2370)
  %quant_weight.62 : Tensor = aten::fake_quantize_per_channel_affine(%self.base.level4.tree2.tree2.conv1.weight, %2366, %2365, %2368, %2369, %2370)
  %input.97 : Tensor = aten::_convolution(%quant_input.62, %quant_weight.62, %2372, %2373, %2373, %2373, %2374, %2375, %2371, %2374, %2374, %2376, %2376)
  %2383 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2384 : float = prim::Constant[value=0.10000000000000001]()
  %2385 : bool = prim::Constant[value=0]()
  %2386 : bool = prim::Constant[value=1]()
  %2387 : Tensor = prim::GetAttr[name="running_var"]()
  %2388 : Tensor = prim::GetAttr[name="running_mean"]()
  %2389 : Tensor = prim::GetAttr[name="bias"]()
  %2390 : Tensor = prim::GetAttr[name="weight"]()
  %input.99 : Tensor = aten::batch_norm(%input.97, %self.base.level4.tree2.tree2.bn1.weight, %self.base.level4.tree2.tree2.bn1.bias, %self.base.level4.tree2.tree2.bn1.running_mean, %self.base.level4.tree2.tree2.bn1.running_var, %2385, %2384, %2383, %2386)
  %2392 : Tensor = aten::relu_(%input.99)
  %2393 : Tensor = prim::Constant[value=<Tensor>]()
  %2394 : Tensor = prim::Constant[value=<Tensor>]()
  %2395 : float = prim::Constant[value=0.038877584802822801]()
  %2396 : int = prim::Constant[value=0]()
  %2397 : int = prim::Constant[value=-128]()
  %2398 : int = prim::Constant[value=127]()
  %2399 : int = prim::Constant[value=1]()
  %2400 : NoneType = prim::Constant()
  %2401 : int[] = prim::Constant[value=[1, 1]]()
  %2402 : bool = prim::Constant[value=0]()
  %2403 : int[] = prim::Constant[value=[0, 0]]()
  %2404 : bool = prim::Constant[value=1]()
  %2405 : Tensor = prim::GetAttr[name="weight"]()
  %2406 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_186.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%351)
  %2407 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_185.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%351)
  %quant_input.64 : Tensor = aten::fake_quantize_per_tensor_affine(%2392, %2395, %2396, %2397, %2398)
  %quant_weight.64 : Tensor = aten::fake_quantize_per_channel_affine(%self.base.level4.tree2.tree2.conv2.weight, %2394, %2393, %2396, %2397, %2398)
  %input.101 : Tensor = aten::_convolution(%quant_input.64, %quant_weight.64, %2400, %2401, %2401, %2401, %2402, %2403, %2399, %2402, %2402, %2404, %2404)
  %2411 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2412 : float = prim::Constant[value=0.10000000000000001]()
  %2413 : bool = prim::Constant[value=0]()
  %2414 : bool = prim::Constant[value=1]()
  %2415 : Tensor = prim::GetAttr[name="running_var"]()
  %2416 : Tensor = prim::GetAttr[name="running_mean"]()
  %2417 : Tensor = prim::GetAttr[name="bias"]()
  %2418 : Tensor = prim::GetAttr[name="weight"]()
  %base_level4_tree2_tree2_bn2.1 : Tensor = aten::batch_norm(%input.101, %self.base.level4.tree2.tree2.bn2.weight, %self.base.level4.tree2.tree2.bn2.bias, %self.base.level4.tree2.tree2.bn2.running_mean, %self.base.level4.tree2.tree2.bn2.running_var, %2413, %2412, %2411, %2414)
  %input8.1 : Tensor = aten::add(%base_level4_tree2_tree2_bn2.1, %base_level4_tree2_tree1_relu_output_quant.1, %944)
  %2420 : Tensor = aten::relu_(%input8.1)
  %1199 : Tensor[] = prim::ListConstruct(%2420, %2358, %inputs.15, %2296)
  %inputs3.1 : Tensor = aten::cat(%1199, %944)
  %2421 : Tensor = prim::Constant[value=<Tensor>]()
  %2422 : Tensor = prim::Constant[value=<Tensor>]()
  %2423 : float = prim::Constant[value=0.059606807438407357]()
  %2424 : int = prim::Constant[value=0]()
  %2425 : int = prim::Constant[value=-128]()
  %2426 : int = prim::Constant[value=127]()
  %2427 : int = prim::Constant[value=1]()
  %2428 : NoneType = prim::Constant()
  %2429 : int[] = prim::Constant[value=[1, 1]]()
  %2430 : int[] = prim::Constant[value=[0, 0]]()
  %2431 : bool = prim::Constant[value=0]()
  %2432 : bool = prim::Constant[value=1]()
  %2433 : Tensor = prim::GetAttr[name="weight"]()
  %2434 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_191.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%339)
  %2435 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_190.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%339)
  %quant_input.66 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs3.1, %2423, %2424, %2425, %2426)
  %quant_weight.66 : Tensor = aten::fake_quantize_per_channel_affine(%self.base.level4.tree2.root.conv.weight, %2422, %2421, %2424, %2425, %2426)
  %input.103 : Tensor = aten::_convolution(%quant_input.66, %quant_weight.66, %2428, %2429, %2430, %2429, %2431, %2430, %2427, %2431, %2431, %2432, %2432)
  %2439 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2440 : float = prim::Constant[value=0.10000000000000001]()
  %2441 : bool = prim::Constant[value=0]()
  %2442 : bool = prim::Constant[value=1]()
  %2443 : Tensor = prim::GetAttr[name="running_var"]()
  %2444 : Tensor = prim::GetAttr[name="running_mean"]()
  %2445 : Tensor = prim::GetAttr[name="bias"]()
  %2446 : Tensor = prim::GetAttr[name="weight"]()
  %input.105 : Tensor = aten::batch_norm(%input.103, %self.base.level4.tree2.root.bn.weight, %self.base.level4.tree2.root.bn.bias, %self.base.level4.tree2.root.bn.running_mean, %self.base.level4.tree2.root.bn.running_var, %2441, %2440, %2439, %2442)
  %2448 : Tensor = aten::relu_(%input.105)
  %2449 : int[] = prim::Constant[value=[2, 2]]()
  %2450 : int[] = prim::Constant[value=[0, 0]]()
  %2451 : int[] = prim::Constant[value=[1, 1]]()
  %2452 : bool = prim::Constant[value=0]()
  %inputs.21 : Tensor = aten::max_pool2d(%2448, %2449, %2449, %2450, %2451, %2452)
  %2454 : Tensor = prim::Constant[value=<Tensor>]()
  %2455 : Tensor = prim::Constant[value=<Tensor>]()
  %2456 : float = prim::Constant[value=0.041764908888208586]()
  %2457 : int = prim::Constant[value=0]()
  %2458 : int = prim::Constant[value=-128]()
  %2459 : int = prim::Constant[value=127]()
  %2460 : int = prim::Constant[value=1]()
  %2461 : NoneType = prim::Constant()
  %2462 : int[] = prim::Constant[value=[1, 1]]()
  %2463 : int[] = prim::Constant[value=[0, 0]]()
  %2464 : bool = prim::Constant[value=0]()
  %2465 : bool = prim::Constant[value=1]()
  %2466 : Tensor = prim::GetAttr[name="weight"]()
  %2467 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_200.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%317)
  %2468 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_199.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%317)
  %quant_input.68 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs.21, %2456, %2457, %2458, %2459)
  %quant_weight.68 : Tensor = aten::fake_quantize_per_channel_affine(%self.base.level5.project.0.weight, %2455, %2454, %2457, %2458, %2459)
  %input.107 : Tensor = aten::_convolution(%quant_input.68, %quant_weight.68, %2461, %2462, %2463, %2462, %2464, %2463, %2460, %2464, %2464, %2465, %2465)
  %2472 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2473 : float = prim::Constant[value=0.10000000000000001]()
  %2474 : bool = prim::Constant[value=0]()
  %2475 : bool = prim::Constant[value=1]()
  %2476 : Tensor = prim::GetAttr[name="running_var"]()
  %2477 : Tensor = prim::GetAttr[name="running_mean"]()
  %2478 : Tensor = prim::GetAttr[name="bias"]()
  %2479 : Tensor = prim::GetAttr[name="weight"]()
  %inputs.1 : Tensor = aten::batch_norm(%input.107, %self.base.level5.project.1.weight, %self.base.level5.project.1.bias, %self.base.level5.project.1.running_mean, %self.base.level5.project.1.running_var, %2474, %2473, %2472, %2475)
  %2481 : float = prim::Constant[value=0.040352085443932241]()
  %2482 : int = prim::Constant[value=0]()
  %2483 : int = prim::Constant[value=-128]()
  %2484 : int = prim::Constant[value=127]()
  %2485 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_203.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%307)
  %base_level5_project_1_output_quant.1 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs.1, %2481, %2482, %2483, %2484)
  %2487 : Tensor = prim::Constant[value=<Tensor>]()
  %2488 : Tensor = prim::Constant[value=<Tensor>]()
  %2489 : float = prim::Constant[value=0.041764908888208586]()
  %2490 : int = prim::Constant[value=0]()
  %2491 : int = prim::Constant[value=-128]()
  %2492 : int = prim::Constant[value=127]()
  %2493 : int = prim::Constant[value=1]()
  %2494 : NoneType = prim::Constant()
  %2495 : int[] = prim::Constant[value=[2, 2]]()
  %2496 : int[] = prim::Constant[value=[1, 1]]()
  %2497 : bool = prim::Constant[value=0]()
  %2498 : int[] = prim::Constant[value=[0, 0]]()
  %2499 : bool = prim::Constant[value=1]()
  %2500 : Tensor = prim::GetAttr[name="weight"]()
  %2501 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_207.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%302)
  %2502 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_206.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%302)
  %quant_input.70 : Tensor = aten::fake_quantize_per_tensor_affine(%2448, %2489, %2490, %2491, %2492)
  %quant_weight.70 : Tensor = aten::fake_quantize_per_channel_affine(%self.base.level5.tree1.conv1.weight, %2488, %2487, %2490, %2491, %2492)
  %input.109 : Tensor = aten::_convolution(%quant_input.70, %quant_weight.70, %2494, %2495, %2496, %2496, %2497, %2498, %2493, %2497, %2497, %2499, %2499)
  %2506 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2507 : float = prim::Constant[value=0.10000000000000001]()
  %2508 : bool = prim::Constant[value=0]()
  %2509 : bool = prim::Constant[value=1]()
  %2510 : Tensor = prim::GetAttr[name="running_var"]()
  %2511 : Tensor = prim::GetAttr[name="running_mean"]()
  %2512 : Tensor = prim::GetAttr[name="bias"]()
  %2513 : Tensor = prim::GetAttr[name="weight"]()
  %input.111 : Tensor = aten::batch_norm(%input.109, %self.base.level5.tree1.bn1.weight, %self.base.level5.tree1.bn1.bias, %self.base.level5.tree1.bn1.running_mean, %self.base.level5.tree1.bn1.running_var, %2508, %2507, %2506, %2509)
  %2515 : Tensor = aten::relu_(%input.111)
  %2516 : Tensor = prim::Constant[value=<Tensor>]()
  %2517 : Tensor = prim::Constant[value=<Tensor>]()
  %2518 : float = prim::Constant[value=0.024328402646883265]()
  %2519 : int = prim::Constant[value=0]()
  %2520 : int = prim::Constant[value=-128]()
  %2521 : int = prim::Constant[value=127]()
  %2522 : int = prim::Constant[value=1]()
  %2523 : NoneType = prim::Constant()
  %2524 : int[] = prim::Constant[value=[1, 1]]()
  %2525 : bool = prim::Constant[value=0]()
  %2526 : int[] = prim::Constant[value=[0, 0]]()
  %2527 : bool = prim::Constant[value=1]()
  %2528 : Tensor = prim::GetAttr[name="weight"]()
  %2529 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_212.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%287)
  %2530 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_211.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%287)
  %quant_input.72 : Tensor = aten::fake_quantize_per_tensor_affine(%2515, %2518, %2519, %2520, %2521)
  %quant_weight.72 : Tensor = aten::fake_quantize_per_channel_affine(%self.base.level5.tree1.conv2.weight, %2517, %2516, %2519, %2520, %2521)
  %input.113 : Tensor = aten::_convolution(%quant_input.72, %quant_weight.72, %2523, %2524, %2524, %2524, %2525, %2526, %2522, %2525, %2525, %2527, %2527)
  %2534 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2535 : float = prim::Constant[value=0.10000000000000001]()
  %2536 : bool = prim::Constant[value=0]()
  %2537 : bool = prim::Constant[value=1]()
  %2538 : Tensor = prim::GetAttr[name="running_var"]()
  %2539 : Tensor = prim::GetAttr[name="running_mean"]()
  %2540 : Tensor = prim::GetAttr[name="bias"]()
  %2541 : Tensor = prim::GetAttr[name="weight"]()
  %base_level5_tree1_bn2.1 : Tensor = aten::batch_norm(%input.113, %self.base.level5.tree1.bn2.weight, %self.base.level5.tree1.bn2.bias, %self.base.level5.tree1.bn2.running_mean, %self.base.level5.tree1.bn2.running_var, %2536, %2535, %2534, %2537)
  %input9.1 : Tensor = aten::add(%base_level5_tree1_bn2.1, %base_level5_project_1_output_quant.1, %944)
  %2543 : Tensor = aten::relu_(%input9.1)
  %2544 : float = prim::Constant[value=0.039226884917011408]()
  %2545 : int = prim::Constant[value=0]()
  %2546 : int = prim::Constant[value=-128]()
  %2547 : int = prim::Constant[value=127]()
  %2548 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_215.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%277)
  %base_level5_tree1_relu_output_quant.1 : Tensor = aten::fake_quantize_per_tensor_affine(%2543, %2544, %2545, %2546, %2547)
  %2550 : Tensor = prim::Constant[value=<Tensor>]()
  %2551 : Tensor = prim::Constant[value=<Tensor>]()
  %2552 : float = prim::Constant[value=0.039226884917011408]()
  %2553 : int = prim::Constant[value=0]()
  %2554 : int = prim::Constant[value=-128]()
  %2555 : int = prim::Constant[value=127]()
  %2556 : int = prim::Constant[value=1]()
  %2557 : NoneType = prim::Constant()
  %2558 : int[] = prim::Constant[value=[1, 1]]()
  %2559 : bool = prim::Constant[value=0]()
  %2560 : int[] = prim::Constant[value=[0, 0]]()
  %2561 : bool = prim::Constant[value=1]()
  %2562 : Tensor = prim::GetAttr[name="weight"]()
  %2563 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_219.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%272)
  %2564 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_218.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%272)
  %quant_input.74 : Tensor = aten::fake_quantize_per_tensor_affine(%2543, %2552, %2553, %2554, %2555)
  %quant_weight.74 : Tensor = aten::fake_quantize_per_channel_affine(%self.base.level5.tree2.conv1.weight, %2551, %2550, %2553, %2554, %2555)
  %input.115 : Tensor = aten::_convolution(%quant_input.74, %quant_weight.74, %2557, %2558, %2558, %2558, %2559, %2560, %2556, %2559, %2559, %2561, %2561)
  %2568 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2569 : float = prim::Constant[value=0.10000000000000001]()
  %2570 : bool = prim::Constant[value=0]()
  %2571 : bool = prim::Constant[value=1]()
  %2572 : Tensor = prim::GetAttr[name="running_var"]()
  %2573 : Tensor = prim::GetAttr[name="running_mean"]()
  %2574 : Tensor = prim::GetAttr[name="bias"]()
  %2575 : Tensor = prim::GetAttr[name="weight"]()
  %input.117 : Tensor = aten::batch_norm(%input.115, %self.base.level5.tree2.bn1.weight, %self.base.level5.tree2.bn1.bias, %self.base.level5.tree2.bn1.running_mean, %self.base.level5.tree2.bn1.running_var, %2570, %2569, %2568, %2571)
  %2577 : Tensor = aten::relu_(%input.117)
  %2578 : Tensor = prim::Constant[value=<Tensor>]()
  %2579 : Tensor = prim::Constant[value=<Tensor>]()
  %2580 : float = prim::Constant[value=0.032877809419406684]()
  %2581 : int = prim::Constant[value=0]()
  %2582 : int = prim::Constant[value=-128]()
  %2583 : int = prim::Constant[value=127]()
  %2584 : int = prim::Constant[value=1]()
  %2585 : NoneType = prim::Constant()
  %2586 : int[] = prim::Constant[value=[1, 1]]()
  %2587 : bool = prim::Constant[value=0]()
  %2588 : int[] = prim::Constant[value=[0, 0]]()
  %2589 : bool = prim::Constant[value=1]()
  %2590 : Tensor = prim::GetAttr[name="weight"]()
  %2591 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_224.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%257)
  %2592 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_223.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%257)
  %quant_input.76 : Tensor = aten::fake_quantize_per_tensor_affine(%2577, %2580, %2581, %2582, %2583)
  %quant_weight.76 : Tensor = aten::fake_quantize_per_channel_affine(%self.base.level5.tree2.conv2.weight, %2579, %2578, %2581, %2582, %2583)
  %input.119 : Tensor = aten::_convolution(%quant_input.76, %quant_weight.76, %2585, %2586, %2586, %2586, %2587, %2588, %2584, %2587, %2587, %2589, %2589)
  %2596 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2597 : float = prim::Constant[value=0.10000000000000001]()
  %2598 : bool = prim::Constant[value=0]()
  %2599 : bool = prim::Constant[value=1]()
  %2600 : Tensor = prim::GetAttr[name="running_var"]()
  %2601 : Tensor = prim::GetAttr[name="running_mean"]()
  %2602 : Tensor = prim::GetAttr[name="bias"]()
  %2603 : Tensor = prim::GetAttr[name="weight"]()
  %base_level5_tree2_bn2.1 : Tensor = aten::batch_norm(%input.119, %self.base.level5.tree2.bn2.weight, %self.base.level5.tree2.bn2.bias, %self.base.level5.tree2.bn2.running_mean, %self.base.level5.tree2.bn2.running_var, %2598, %2597, %2596, %2599)
  %input10.1 : Tensor = aten::add(%base_level5_tree2_bn2.1, %base_level5_tree1_relu_output_quant.1, %944)
  %2605 : Tensor = aten::relu_(%input10.1)
  %1264 : Tensor[] = prim::ListConstruct(%2605, %2543, %inputs.21)
  %inputs4.1 : Tensor = aten::cat(%1264, %944)
  %2606 : Tensor = prim::Constant[value=<Tensor>]()
  %2607 : Tensor = prim::Constant[value=<Tensor>]()
  %2608 : float = prim::Constant[value=0.081878324193278632]()
  %2609 : int = prim::Constant[value=0]()
  %2610 : int = prim::Constant[value=-128]()
  %2611 : int = prim::Constant[value=127]()
  %2612 : int = prim::Constant[value=1]()
  %2613 : NoneType = prim::Constant()
  %2614 : int[] = prim::Constant[value=[1, 1]]()
  %2615 : int[] = prim::Constant[value=[0, 0]]()
  %2616 : bool = prim::Constant[value=0]()
  %2617 : bool = prim::Constant[value=1]()
  %2618 : Tensor = prim::GetAttr[name="weight"]()
  %2619 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_229.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%247)
  %2620 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_228.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%247)
  %quant_input.78 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs4.1, %2608, %2609, %2610, %2611)
  %quant_weight.78 : Tensor = aten::fake_quantize_per_channel_affine(%self.base.level5.root.conv.weight, %2607, %2606, %2609, %2610, %2611)
  %input.121 : Tensor = aten::_convolution(%quant_input.78, %quant_weight.78, %2613, %2614, %2615, %2614, %2616, %2615, %2612, %2616, %2616, %2617, %2617)
  %2624 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2625 : float = prim::Constant[value=0.10000000000000001]()
  %2626 : bool = prim::Constant[value=0]()
  %2627 : bool = prim::Constant[value=1]()
  %2628 : Tensor = prim::GetAttr[name="running_var"]()
  %2629 : Tensor = prim::GetAttr[name="running_mean"]()
  %2630 : Tensor = prim::GetAttr[name="bias"]()
  %2631 : Tensor = prim::GetAttr[name="weight"]()
  %input.123 : Tensor = aten::batch_norm(%input.121, %self.base.level5.root.bn.weight, %self.base.level5.root.bn.bias, %self.base.level5.root.bn.running_mean, %self.base.level5.root.bn.running_var, %2626, %2625, %2624, %2627)
  %2633 : Tensor = aten::relu_(%input.123)
  %2634 : Tensor = prim::Constant[value=<Tensor>]()
  %2635 : Tensor = prim::Constant[value=<Tensor>]()
  %2636 : float = prim::Constant[value=0.28054608322503999]()
  %2637 : int = prim::Constant[value=0]()
  %2638 : int = prim::Constant[value=-128]()
  %2639 : int = prim::Constant[value=127]()
  %2640 : int = prim::Constant[value=1]()
  %2641 : NoneType = prim::Constant()
  %2642 : int[] = prim::Constant[value=[1, 1]]()
  %2643 : int[] = prim::Constant[value=[0, 0]]()
  %2644 : bool = prim::Constant[value=0]()
  %2645 : bool = prim::Constant[value=1]()
  %2646 : Tensor = prim::GetAttr[name="weight"]()
  %2647 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_237.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%232)
  %2648 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_236.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%232)
  %quant_input.80 : Tensor = aten::fake_quantize_per_tensor_affine(%2633, %2636, %2637, %2638, %2639)
  %quant_weight.80 : Tensor = aten::fake_quantize_per_channel_affine(%self.dla_up.ida_0.proj_1.0.weight, %2635, %2634, %2637, %2638, %2639)
  %input.125 : Tensor = aten::_convolution(%quant_input.80, %quant_weight.80, %2641, %2642, %2643, %2642, %2644, %2643, %2640, %2644, %2644, %2645, %2645)
  %2652 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2653 : float = prim::Constant[value=0.10000000000000001]()
  %2654 : bool = prim::Constant[value=0]()
  %2655 : bool = prim::Constant[value=1]()
  %2656 : Tensor = prim::GetAttr[name="running_var"]()
  %2657 : Tensor = prim::GetAttr[name="running_mean"]()
  %2658 : Tensor = prim::GetAttr[name="bias"]()
  %2659 : Tensor = prim::GetAttr[name="weight"]()
  %input.127 : Tensor = aten::batch_norm(%input.125, %self.dla_up.ida_0.proj_1.1.weight, %self.dla_up.ida_0.proj_1.1.bias, %self.dla_up.ida_0.proj_1.1.running_mean, %self.dla_up.ida_0.proj_1.1.running_var, %2654, %2653, %2652, %2655)
  %2661 : Tensor = aten::relu_(%input.127)
  %2662 : int = prim::Constant[value=256]()
  %2663 : NoneType = prim::Constant()
  %2664 : int[] = prim::Constant[value=[2, 2]]()
  %2665 : int[] = prim::Constant[value=[1, 1]]()
  %2666 : bool = prim::Constant[value=1]()
  %2667 : int[] = prim::Constant[value=[0, 0]]()
  %2668 : bool = prim::Constant[value=0]()
  %2669 : Tensor = prim::GetAttr[name="weight"]()
  %dla_up_ida_0_up_1.1 : Tensor = aten::_convolution(%2661, %self.dla_up.ida_0.up_1.weight, %2663, %2664, %2665, %2665, %2666, %2667, %2662, %2668, %2668, %2666, %2666)
  %1286 : Tensor[] = prim::ListConstruct(%2448, %dla_up_ida_0_up_1.1)
  %inputs5.1 : Tensor = aten::cat(%1286, %944)
  %2671 : Tensor = prim::Constant[value=<Tensor>]()
  %2672 : Tensor = prim::Constant[value=<Tensor>]()
  %2673 : float = prim::Constant[value=0.17380840571846548]()
  %2674 : int = prim::Constant[value=0]()
  %2675 : int = prim::Constant[value=-128]()
  %2676 : int = prim::Constant[value=127]()
  %2677 : int = prim::Constant[value=1]()
  %2678 : NoneType = prim::Constant()
  %2679 : int[] = prim::Constant[value=[1, 1]]()
  %2680 : bool = prim::Constant[value=0]()
  %2681 : int[] = prim::Constant[value=[0, 0]]()
  %2682 : bool = prim::Constant[value=1]()
  %2683 : Tensor = prim::GetAttr[name="weight"]()
  %2684 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_243.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%213)
  %2685 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_242.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%213)
  %quant_input.82 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs5.1, %2673, %2674, %2675, %2676)
  %quant_weight.82 : Tensor = aten::fake_quantize_per_channel_affine(%self.dla_up.ida_0.node_1.0.weight, %2672, %2671, %2674, %2675, %2676)
  %input.129 : Tensor = aten::_convolution(%quant_input.82, %quant_weight.82, %2678, %2679, %2679, %2679, %2680, %2681, %2677, %2680, %2680, %2682, %2682)
  %2689 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2690 : float = prim::Constant[value=0.10000000000000001]()
  %2691 : bool = prim::Constant[value=0]()
  %2692 : bool = prim::Constant[value=1]()
  %2693 : Tensor = prim::GetAttr[name="running_var"]()
  %2694 : Tensor = prim::GetAttr[name="running_mean"]()
  %2695 : Tensor = prim::GetAttr[name="bias"]()
  %2696 : Tensor = prim::GetAttr[name="weight"]()
  %input.131 : Tensor = aten::batch_norm(%input.129, %self.dla_up.ida_0.node_1.1.weight, %self.dla_up.ida_0.node_1.1.bias, %self.dla_up.ida_0.node_1.1.running_mean, %self.dla_up.ida_0.node_1.1.running_var, %2691, %2690, %2689, %2692)
  %2698 : Tensor = aten::relu_(%input.131)
  %2699 : Tensor = prim::Constant[value=<Tensor>]()
  %2700 : Tensor = prim::Constant[value=<Tensor>]()
  %2701 : float = prim::Constant[value=0.041764908888208586]()
  %2702 : int = prim::Constant[value=0]()
  %2703 : int = prim::Constant[value=-128]()
  %2704 : int = prim::Constant[value=127]()
  %2705 : int = prim::Constant[value=1]()
  %2706 : NoneType = prim::Constant()
  %2707 : int[] = prim::Constant[value=[1, 1]]()
  %2708 : int[] = prim::Constant[value=[0, 0]]()
  %2709 : bool = prim::Constant[value=0]()
  %2710 : bool = prim::Constant[value=1]()
  %2711 : Tensor = prim::GetAttr[name="weight"]()
  %2712 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_250.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%198)
  %2713 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_249.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%198)
  %quant_input.84 : Tensor = aten::fake_quantize_per_tensor_affine(%2448, %2701, %2702, %2703, %2704)
  %quant_weight.84 : Tensor = aten::fake_quantize_per_channel_affine(%self.dla_up.ida_1.proj_1.0.weight, %2700, %2699, %2702, %2703, %2704)
  %input.133 : Tensor = aten::_convolution(%quant_input.84, %quant_weight.84, %2706, %2707, %2708, %2707, %2709, %2708, %2705, %2709, %2709, %2710, %2710)
  %2717 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2718 : float = prim::Constant[value=0.10000000000000001]()
  %2719 : bool = prim::Constant[value=0]()
  %2720 : bool = prim::Constant[value=1]()
  %2721 : Tensor = prim::GetAttr[name="running_var"]()
  %2722 : Tensor = prim::GetAttr[name="running_mean"]()
  %2723 : Tensor = prim::GetAttr[name="bias"]()
  %2724 : Tensor = prim::GetAttr[name="weight"]()
  %input.135 : Tensor = aten::batch_norm(%input.133, %self.dla_up.ida_1.proj_1.1.weight, %self.dla_up.ida_1.proj_1.1.bias, %self.dla_up.ida_1.proj_1.1.running_mean, %self.dla_up.ida_1.proj_1.1.running_var, %2719, %2718, %2717, %2720)
  %2726 : Tensor = aten::relu_(%input.135)
  %2727 : int = prim::Constant[value=128]()
  %2728 : NoneType = prim::Constant()
  %2729 : int[] = prim::Constant[value=[2, 2]]()
  %2730 : int[] = prim::Constant[value=[1, 1]]()
  %2731 : bool = prim::Constant[value=1]()
  %2732 : int[] = prim::Constant[value=[0, 0]]()
  %2733 : bool = prim::Constant[value=0]()
  %2734 : Tensor = prim::GetAttr[name="weight"]()
  %dla_up_ida_1_up_1.1 : Tensor = aten::_convolution(%2726, %self.dla_up.ida_1.up_1.weight, %2728, %2729, %2730, %2730, %2731, %2732, %2727, %2733, %2733, %2731, %2731)
  %2736 : Tensor = prim::Constant[value=<Tensor>]()
  %2737 : Tensor = prim::Constant[value=<Tensor>]()
  %2738 : float = prim::Constant[value=0.12328381425752415]()
  %2739 : int = prim::Constant[value=0]()
  %2740 : int = prim::Constant[value=-128]()
  %2741 : int = prim::Constant[value=127]()
  %2742 : int = prim::Constant[value=1]()
  %2743 : NoneType = prim::Constant()
  %2744 : int[] = prim::Constant[value=[1, 1]]()
  %2745 : int[] = prim::Constant[value=[0, 0]]()
  %2746 : bool = prim::Constant[value=0]()
  %2747 : bool = prim::Constant[value=1]()
  %2748 : Tensor = prim::GetAttr[name="weight"]()
  %2749 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_257.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%179)
  %2750 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_256.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%179)
  %quant_input.86 : Tensor = aten::fake_quantize_per_tensor_affine(%2698, %2738, %2739, %2740, %2741)
  %quant_weight.86 : Tensor = aten::fake_quantize_per_channel_affine(%self.dla_up.ida_1.proj_2.0.weight, %2737, %2736, %2739, %2740, %2741)
  %input.137 : Tensor = aten::_convolution(%quant_input.86, %quant_weight.86, %2743, %2744, %2745, %2744, %2746, %2745, %2742, %2746, %2746, %2747, %2747)
  %2754 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2755 : float = prim::Constant[value=0.10000000000000001]()
  %2756 : bool = prim::Constant[value=0]()
  %2757 : bool = prim::Constant[value=1]()
  %2758 : Tensor = prim::GetAttr[name="running_var"]()
  %2759 : Tensor = prim::GetAttr[name="running_mean"]()
  %2760 : Tensor = prim::GetAttr[name="bias"]()
  %2761 : Tensor = prim::GetAttr[name="weight"]()
  %input.139 : Tensor = aten::batch_norm(%input.137, %self.dla_up.ida_1.proj_2.1.weight, %self.dla_up.ida_1.proj_2.1.bias, %self.dla_up.ida_1.proj_2.1.running_mean, %self.dla_up.ida_1.proj_2.1.running_var, %2756, %2755, %2754, %2757)
  %2763 : Tensor = aten::relu_(%input.139)
  %2764 : int = prim::Constant[value=128]()
  %2765 : NoneType = prim::Constant()
  %2766 : int[] = prim::Constant[value=[2, 2]]()
  %2767 : int[] = prim::Constant[value=[1, 1]]()
  %2768 : bool = prim::Constant[value=1]()
  %2769 : int[] = prim::Constant[value=[0, 0]]()
  %2770 : bool = prim::Constant[value=0]()
  %2771 : Tensor = prim::GetAttr[name="weight"]()
  %dla_up_ida_1_up_2.1 : Tensor = aten::_convolution(%2763, %self.dla_up.ida_1.up_2.weight, %2765, %2766, %2767, %2767, %2768, %2769, %2764, %2770, %2770, %2768, %2768)
  %1318 : Tensor[] = prim::ListConstruct(%2087, %dla_up_ida_1_up_1.1)
  %inputs6.1 : Tensor = aten::cat(%1318, %944)
  %2773 : Tensor = prim::Constant[value=<Tensor>]()
  %2774 : Tensor = prim::Constant[value=<Tensor>]()
  %2775 : float = prim::Constant[value=0.13152006855161172]()
  %2776 : int = prim::Constant[value=0]()
  %2777 : int = prim::Constant[value=-128]()
  %2778 : int = prim::Constant[value=127]()
  %2779 : int = prim::Constant[value=1]()
  %2780 : NoneType = prim::Constant()
  %2781 : int[] = prim::Constant[value=[1, 1]]()
  %2782 : bool = prim::Constant[value=0]()
  %2783 : int[] = prim::Constant[value=[0, 0]]()
  %2784 : bool = prim::Constant[value=1]()
  %2785 : Tensor = prim::GetAttr[name="weight"]()
  %2786 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_264.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%160)
  %2787 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_263.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%160)
  %quant_input.88 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs6.1, %2775, %2776, %2777, %2778)
  %quant_weight.88 : Tensor = aten::fake_quantize_per_channel_affine(%self.dla_up.ida_1.node_1.0.weight, %2774, %2773, %2776, %2777, %2778)
  %input.141 : Tensor = aten::_convolution(%quant_input.88, %quant_weight.88, %2780, %2781, %2781, %2781, %2782, %2783, %2779, %2782, %2782, %2784, %2784)
  %2791 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2792 : float = prim::Constant[value=0.10000000000000001]()
  %2793 : bool = prim::Constant[value=0]()
  %2794 : bool = prim::Constant[value=1]()
  %2795 : Tensor = prim::GetAttr[name="running_var"]()
  %2796 : Tensor = prim::GetAttr[name="running_mean"]()
  %2797 : Tensor = prim::GetAttr[name="bias"]()
  %2798 : Tensor = prim::GetAttr[name="weight"]()
  %input.143 : Tensor = aten::batch_norm(%input.141, %self.dla_up.ida_1.node_1.1.weight, %self.dla_up.ida_1.node_1.1.bias, %self.dla_up.ida_1.node_1.1.running_mean, %self.dla_up.ida_1.node_1.1.running_var, %2793, %2792, %2791, %2794)
  %2800 : Tensor = aten::relu_(%input.143)
  %1330 : Tensor[] = prim::ListConstruct(%2800, %dla_up_ida_1_up_2.1)
  %inputs7.1 : Tensor = aten::cat(%1330, %944)
  %2801 : Tensor = prim::Constant[value=<Tensor>]()
  %2802 : Tensor = prim::Constant[value=<Tensor>]()
  %2803 : float = prim::Constant[value=0.23995789955920122]()
  %2804 : int = prim::Constant[value=0]()
  %2805 : int = prim::Constant[value=-128]()
  %2806 : int = prim::Constant[value=127]()
  %2807 : int = prim::Constant[value=1]()
  %2808 : NoneType = prim::Constant()
  %2809 : int[] = prim::Constant[value=[1, 1]]()
  %2810 : bool = prim::Constant[value=0]()
  %2811 : int[] = prim::Constant[value=[0, 0]]()
  %2812 : bool = prim::Constant[value=1]()
  %2813 : Tensor = prim::GetAttr[name="weight"]()
  %2814 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_270.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%145)
  %2815 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_269.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%145)
  %quant_input.90 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs7.1, %2803, %2804, %2805, %2806)
  %quant_weight.90 : Tensor = aten::fake_quantize_per_channel_affine(%self.dla_up.ida_1.node_2.0.weight, %2802, %2801, %2804, %2805, %2806)
  %input.145 : Tensor = aten::_convolution(%quant_input.90, %quant_weight.90, %2808, %2809, %2809, %2809, %2810, %2811, %2807, %2810, %2810, %2812, %2812)
  %2819 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2820 : float = prim::Constant[value=0.10000000000000001]()
  %2821 : bool = prim::Constant[value=0]()
  %2822 : bool = prim::Constant[value=1]()
  %2823 : Tensor = prim::GetAttr[name="running_var"]()
  %2824 : Tensor = prim::GetAttr[name="running_mean"]()
  %2825 : Tensor = prim::GetAttr[name="bias"]()
  %2826 : Tensor = prim::GetAttr[name="weight"]()
  %input.147 : Tensor = aten::batch_norm(%input.145, %self.dla_up.ida_1.node_2.1.weight, %self.dla_up.ida_1.node_2.1.bias, %self.dla_up.ida_1.node_2.1.running_mean, %self.dla_up.ida_1.node_2.1.running_var, %2821, %2820, %2819, %2822)
  %2828 : Tensor = aten::relu_(%input.147)
  %2829 : Tensor = prim::Constant[value=<Tensor>]()
  %2830 : Tensor = prim::Constant[value=<Tensor>]()
  %2831 : float = prim::Constant[value=0.046685489143912247]()
  %2832 : int = prim::Constant[value=0]()
  %2833 : int = prim::Constant[value=-128]()
  %2834 : int = prim::Constant[value=127]()
  %2835 : int = prim::Constant[value=1]()
  %2836 : NoneType = prim::Constant()
  %2837 : int[] = prim::Constant[value=[1, 1]]()
  %2838 : int[] = prim::Constant[value=[0, 0]]()
  %2839 : bool = prim::Constant[value=0]()
  %2840 : bool = prim::Constant[value=1]()
  %2841 : Tensor = prim::GetAttr[name="weight"]()
  %2842 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_277.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%130)
  %2843 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_276.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%130)
  %quant_input.92 : Tensor = aten::fake_quantize_per_tensor_affine(%2087, %2831, %2832, %2833, %2834)
  %quant_weight.92 : Tensor = aten::fake_quantize_per_channel_affine(%self.dla_up.ida_2.proj_1.0.weight, %2830, %2829, %2832, %2833, %2834)
  %input.149 : Tensor = aten::_convolution(%quant_input.92, %quant_weight.92, %2836, %2837, %2838, %2837, %2839, %2838, %2835, %2839, %2839, %2840, %2840)
  %2847 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2848 : float = prim::Constant[value=0.10000000000000001]()
  %2849 : bool = prim::Constant[value=0]()
  %2850 : bool = prim::Constant[value=1]()
  %2851 : Tensor = prim::GetAttr[name="running_var"]()
  %2852 : Tensor = prim::GetAttr[name="running_mean"]()
  %2853 : Tensor = prim::GetAttr[name="bias"]()
  %2854 : Tensor = prim::GetAttr[name="weight"]()
  %input.151 : Tensor = aten::batch_norm(%input.149, %self.dla_up.ida_2.proj_1.1.weight, %self.dla_up.ida_2.proj_1.1.bias, %self.dla_up.ida_2.proj_1.1.running_mean, %self.dla_up.ida_2.proj_1.1.running_var, %2849, %2848, %2847, %2850)
  %2856 : Tensor = aten::relu_(%input.151)
  %2857 : int = prim::Constant[value=64]()
  %2858 : NoneType = prim::Constant()
  %2859 : int[] = prim::Constant[value=[2, 2]]()
  %2860 : int[] = prim::Constant[value=[1, 1]]()
  %2861 : bool = prim::Constant[value=1]()
  %2862 : int[] = prim::Constant[value=[0, 0]]()
  %2863 : bool = prim::Constant[value=0]()
  %2864 : Tensor = prim::GetAttr[name="weight"]()
  %dla_up_ida_2_up_1.1 : Tensor = aten::_convolution(%2856, %self.dla_up.ida_2.up_1.weight, %2858, %2859, %2860, %2860, %2861, %2862, %2857, %2863, %2863, %2861, %2861)
  %2866 : Tensor = prim::Constant[value=<Tensor>]()
  %2867 : Tensor = prim::Constant[value=<Tensor>]()
  %2868 : float = prim::Constant[value=0.12479710766649622]()
  %2869 : int = prim::Constant[value=0]()
  %2870 : int = prim::Constant[value=-128]()
  %2871 : int = prim::Constant[value=127]()
  %2872 : int = prim::Constant[value=1]()
  %2873 : NoneType = prim::Constant()
  %2874 : int[] = prim::Constant[value=[1, 1]]()
  %2875 : int[] = prim::Constant[value=[0, 0]]()
  %2876 : bool = prim::Constant[value=0]()
  %2877 : bool = prim::Constant[value=1]()
  %2878 : Tensor = prim::GetAttr[name="weight"]()
  %2879 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_284.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%111)
  %2880 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_283.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%111)
  %quant_input.94 : Tensor = aten::fake_quantize_per_tensor_affine(%2800, %2868, %2869, %2870, %2871)
  %quant_weight.94 : Tensor = aten::fake_quantize_per_channel_affine(%self.dla_up.ida_2.proj_2.0.weight, %2867, %2866, %2869, %2870, %2871)
  %input.153 : Tensor = aten::_convolution(%quant_input.94, %quant_weight.94, %2873, %2874, %2875, %2874, %2876, %2875, %2872, %2876, %2876, %2877, %2877)
  %2884 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2885 : float = prim::Constant[value=0.10000000000000001]()
  %2886 : bool = prim::Constant[value=0]()
  %2887 : bool = prim::Constant[value=1]()
  %2888 : Tensor = prim::GetAttr[name="running_var"]()
  %2889 : Tensor = prim::GetAttr[name="running_mean"]()
  %2890 : Tensor = prim::GetAttr[name="bias"]()
  %2891 : Tensor = prim::GetAttr[name="weight"]()
  %input.155 : Tensor = aten::batch_norm(%input.153, %self.dla_up.ida_2.proj_2.1.weight, %self.dla_up.ida_2.proj_2.1.bias, %self.dla_up.ida_2.proj_2.1.running_mean, %self.dla_up.ida_2.proj_2.1.running_var, %2886, %2885, %2884, %2887)
  %2893 : Tensor = aten::relu_(%input.155)
  %2894 : int = prim::Constant[value=64]()
  %2895 : NoneType = prim::Constant()
  %2896 : int[] = prim::Constant[value=[2, 2]]()
  %2897 : int[] = prim::Constant[value=[1, 1]]()
  %2898 : bool = prim::Constant[value=1]()
  %2899 : int[] = prim::Constant[value=[0, 0]]()
  %2900 : bool = prim::Constant[value=0]()
  %2901 : Tensor = prim::GetAttr[name="weight"]()
  %dla_up_ida_2_up_2.1 : Tensor = aten::_convolution(%2893, %self.dla_up.ida_2.up_2.weight, %2895, %2896, %2897, %2897, %2898, %2899, %2894, %2900, %2900, %2898, %2898)
  %2903 : Tensor = prim::Constant[value=<Tensor>]()
  %2904 : Tensor = prim::Constant[value=<Tensor>]()
  %2905 : float = prim::Constant[value=0.12283295909250815]()
  %2906 : int = prim::Constant[value=0]()
  %2907 : int = prim::Constant[value=-128]()
  %2908 : int = prim::Constant[value=127]()
  %2909 : int = prim::Constant[value=1]()
  %2910 : NoneType = prim::Constant()
  %2911 : int[] = prim::Constant[value=[1, 1]]()
  %2912 : int[] = prim::Constant[value=[0, 0]]()
  %2913 : bool = prim::Constant[value=0]()
  %2914 : bool = prim::Constant[value=1]()
  %2915 : Tensor = prim::GetAttr[name="weight"]()
  %2916 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_291.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%92)
  %2917 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_290.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%92)
  %quant_input.96 : Tensor = aten::fake_quantize_per_tensor_affine(%2828, %2905, %2906, %2907, %2908)
  %quant_weight.96 : Tensor = aten::fake_quantize_per_channel_affine(%self.dla_up.ida_2.proj_3.0.weight, %2904, %2903, %2906, %2907, %2908)
  %input.157 : Tensor = aten::_convolution(%quant_input.96, %quant_weight.96, %2910, %2911, %2912, %2911, %2913, %2912, %2909, %2913, %2913, %2914, %2914)
  %2921 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2922 : float = prim::Constant[value=0.10000000000000001]()
  %2923 : bool = prim::Constant[value=0]()
  %2924 : bool = prim::Constant[value=1]()
  %2925 : Tensor = prim::GetAttr[name="running_var"]()
  %2926 : Tensor = prim::GetAttr[name="running_mean"]()
  %2927 : Tensor = prim::GetAttr[name="bias"]()
  %2928 : Tensor = prim::GetAttr[name="weight"]()
  %input.159 : Tensor = aten::batch_norm(%input.157, %self.dla_up.ida_2.proj_3.1.weight, %self.dla_up.ida_2.proj_3.1.bias, %self.dla_up.ida_2.proj_3.1.running_mean, %self.dla_up.ida_2.proj_3.1.running_var, %2923, %2922, %2921, %2924)
  %2930 : Tensor = aten::relu_(%input.159)
  %2931 : int = prim::Constant[value=64]()
  %2932 : NoneType = prim::Constant()
  %2933 : int[] = prim::Constant[value=[2, 2]]()
  %2934 : int[] = prim::Constant[value=[1, 1]]()
  %2935 : bool = prim::Constant[value=1]()
  %2936 : int[] = prim::Constant[value=[0, 0]]()
  %2937 : bool = prim::Constant[value=0]()
  %2938 : Tensor = prim::GetAttr[name="weight"]()
  %dla_up_ida_2_up_3.1 : Tensor = aten::_convolution(%2930, %self.dla_up.ida_2.up_3.weight, %2932, %2933, %2934, %2934, %2935, %2936, %2931, %2937, %2937, %2935, %2935)
  %1372 : Tensor[] = prim::ListConstruct(%1726, %dla_up_ida_2_up_1.1)
  %inputs8.1 : Tensor = aten::cat(%1372, %944)
  %2940 : Tensor = prim::Constant[value=<Tensor>]()
  %2941 : Tensor = prim::Constant[value=<Tensor>]()
  %2942 : float = prim::Constant[value=0.072918140982079688]()
  %2943 : int = prim::Constant[value=0]()
  %2944 : int = prim::Constant[value=-128]()
  %2945 : int = prim::Constant[value=127]()
  %2946 : int = prim::Constant[value=1]()
  %2947 : NoneType = prim::Constant()
  %2948 : int[] = prim::Constant[value=[1, 1]]()
  %2949 : bool = prim::Constant[value=0]()
  %2950 : int[] = prim::Constant[value=[0, 0]]()
  %2951 : bool = prim::Constant[value=1]()
  %2952 : Tensor = prim::GetAttr[name="weight"]()
  %2953 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_298.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%73)
  %2954 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_297.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%73)
  %quant_input.98 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs8.1, %2942, %2943, %2944, %2945)
  %quant_weight.98 : Tensor = aten::fake_quantize_per_channel_affine(%self.dla_up.ida_2.node_1.0.weight, %2941, %2940, %2943, %2944, %2945)
  %input.161 : Tensor = aten::_convolution(%quant_input.98, %quant_weight.98, %2947, %2948, %2948, %2948, %2949, %2950, %2946, %2949, %2949, %2951, %2951)
  %2958 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2959 : float = prim::Constant[value=0.10000000000000001]()
  %2960 : bool = prim::Constant[value=0]()
  %2961 : bool = prim::Constant[value=1]()
  %2962 : Tensor = prim::GetAttr[name="running_var"]()
  %2963 : Tensor = prim::GetAttr[name="running_mean"]()
  %2964 : Tensor = prim::GetAttr[name="bias"]()
  %2965 : Tensor = prim::GetAttr[name="weight"]()
  %input.163 : Tensor = aten::batch_norm(%input.161, %self.dla_up.ida_2.node_1.1.weight, %self.dla_up.ida_2.node_1.1.bias, %self.dla_up.ida_2.node_1.1.running_mean, %self.dla_up.ida_2.node_1.1.running_var, %2960, %2959, %2958, %2961)
  %2967 : Tensor = aten::relu_(%input.163)
  %1383 : Tensor[] = prim::ListConstruct(%2967, %dla_up_ida_2_up_2.1)
  %inputs9.1 : Tensor = aten::cat(%1383, %944)
  %2968 : Tensor = prim::Constant[value=<Tensor>]()
  %2969 : Tensor = prim::Constant[value=<Tensor>]()
  %2970 : float = prim::Constant[value=0.16638957046148345]()
  %2971 : int = prim::Constant[value=0]()
  %2972 : int = prim::Constant[value=-128]()
  %2973 : int = prim::Constant[value=127]()
  %2974 : int = prim::Constant[value=1]()
  %2975 : NoneType = prim::Constant()
  %2976 : int[] = prim::Constant[value=[1, 1]]()
  %2977 : bool = prim::Constant[value=0]()
  %2978 : int[] = prim::Constant[value=[0, 0]]()
  %2979 : bool = prim::Constant[value=1]()
  %2980 : Tensor = prim::GetAttr[name="weight"]()
  %2981 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_304.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%58)
  %2982 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_303.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%58)
  %quant_input.100 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs9.1, %2970, %2971, %2972, %2973)
  %quant_weight.100 : Tensor = aten::fake_quantize_per_channel_affine(%self.dla_up.ida_2.node_2.0.weight, %2969, %2968, %2971, %2972, %2973)
  %input.165 : Tensor = aten::_convolution(%quant_input.100, %quant_weight.100, %2975, %2976, %2976, %2976, %2977, %2978, %2974, %2977, %2977, %2979, %2979)
  %2986 : float = prim::Constant[value=1.0000000000000001e-05]()
  %2987 : float = prim::Constant[value=0.10000000000000001]()
  %2988 : bool = prim::Constant[value=0]()
  %2989 : bool = prim::Constant[value=1]()
  %2990 : Tensor = prim::GetAttr[name="running_var"]()
  %2991 : Tensor = prim::GetAttr[name="running_mean"]()
  %2992 : Tensor = prim::GetAttr[name="bias"]()
  %2993 : Tensor = prim::GetAttr[name="weight"]()
  %input.167 : Tensor = aten::batch_norm(%input.165, %self.dla_up.ida_2.node_2.1.weight, %self.dla_up.ida_2.node_2.1.bias, %self.dla_up.ida_2.node_2.1.running_mean, %self.dla_up.ida_2.node_2.1.running_var, %2988, %2987, %2986, %2989)
  %2995 : Tensor = aten::relu_(%input.167)
  %1394 : Tensor[] = prim::ListConstruct(%2995, %dla_up_ida_2_up_3.1)
  %inputs10.1 : Tensor = aten::cat(%1394, %944)
  %2996 : Tensor = prim::Constant[value=<Tensor>]()
  %2997 : Tensor = prim::Constant[value=<Tensor>]()
  %2998 : float = prim::Constant[value=0.11740763356366496]()
  %2999 : int = prim::Constant[value=0]()
  %3000 : int = prim::Constant[value=-128]()
  %3001 : int = prim::Constant[value=127]()
  %3002 : int = prim::Constant[value=1]()
  %3003 : NoneType = prim::Constant()
  %3004 : int[] = prim::Constant[value=[1, 1]]()
  %3005 : bool = prim::Constant[value=0]()
  %3006 : int[] = prim::Constant[value=[0, 0]]()
  %3007 : bool = prim::Constant[value=1]()
  %3008 : Tensor = prim::GetAttr[name="weight"]()
  %3009 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_310.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%43)
  %3010 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_309.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%43)
  %quant_input.102 : Tensor = aten::fake_quantize_per_tensor_affine(%inputs10.1, %2998, %2999, %3000, %3001)
  %quant_weight.102 : Tensor = aten::fake_quantize_per_channel_affine(%self.dla_up.ida_2.node_3.0.weight, %2997, %2996, %2999, %3000, %3001)
  %input.169 : Tensor = aten::_convolution(%quant_input.102, %quant_weight.102, %3003, %3004, %3004, %3004, %3005, %3006, %3002, %3005, %3005, %3007, %3007)
  %3014 : float = prim::Constant[value=1.0000000000000001e-05]()
  %3015 : float = prim::Constant[value=0.10000000000000001]()
  %3016 : bool = prim::Constant[value=0]()
  %3017 : bool = prim::Constant[value=1]()
  %3018 : Tensor = prim::GetAttr[name="running_var"]()
  %3019 : Tensor = prim::GetAttr[name="running_mean"]()
  %3020 : Tensor = prim::GetAttr[name="bias"]()
  %3021 : Tensor = prim::GetAttr[name="weight"]()
  %input.171 : Tensor = aten::batch_norm(%input.169, %self.dla_up.ida_2.node_3.1.weight, %self.dla_up.ida_2.node_3.1.bias, %self.dla_up.ida_2.node_3.1.running_mean, %self.dla_up.ida_2.node_3.1.running_var, %3016, %3015, %3014, %3017)
  %3023 : Tensor = aten::relu_(%input.171)
  %3024 : Tensor = prim::Constant[value=<Tensor>]()
  %3025 : Tensor = prim::Constant[value=<Tensor>]()
  %3026 : float = prim::Constant[value=0.079049185505063513]()
  %3027 : int = prim::Constant[value=0]()
  %3028 : int = prim::Constant[value=-128]()
  %3029 : int = prim::Constant[value=127]()
  %3030 : int = prim::Constant[value=1]()
  %3031 : int[] = prim::Constant[value=[1, 1]]()
  %3032 : bool = prim::Constant[value=0]()
  %3033 : int[] = prim::Constant[value=[0, 0]]()
  %3034 : bool = prim::Constant[value=1]()
  %3035 : Tensor = prim::GetAttr[name="bias"]()
  %3036 : Tensor = prim::GetAttr[name="weight"]()
  %3037 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_318.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%28)
  %3038 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_317.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%28)
  %quant_input.104 : Tensor = aten::fake_quantize_per_tensor_affine(%3023, %3026, %3027, %3028, %3029)
  %quant_weight.104 : Tensor = aten::fake_quantize_per_channel_affine(%self.hm.0.weight, %3025, %3024, %3027, %3028, %3029)
  %input.173 : Tensor = aten::_convolution(%quant_input.104, %quant_weight.104, %self.hm.0.bias, %3031, %3031, %3031, %3032, %3033, %3030, %3032, %3032, %3034, %3034)
  %3042 : Tensor = aten::relu_(%input.173)
  %3043 : Tensor = prim::Constant[value=<Tensor>]()
  %3044 : Tensor = prim::Constant[value=<Tensor>]()
  %3045 : float = prim::Constant[value=0.32104384054349161]()
  %3046 : int = prim::Constant[value=0]()
  %3047 : int = prim::Constant[value=-128]()
  %3048 : int = prim::Constant[value=127]()
  %3049 : int = prim::Constant[value=1]()
  %3050 : int[] = prim::Constant[value=[1, 1]]()
  %3051 : int[] = prim::Constant[value=[0, 0]]()
  %3052 : bool = prim::Constant[value=0]()
  %3053 : bool = prim::Constant[value=1]()
  %3054 : Tensor = prim::GetAttr[name="bias"]()
  %3055 : Tensor = prim::GetAttr[name="weight"]()
  %3056 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_322.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%22)
  %3057 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_321.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%22)
  %quant_input.106 : Tensor = aten::fake_quantize_per_tensor_affine(%3042, %3045, %3046, %3047, %3048)
  %quant_weight.106 : Tensor = aten::fake_quantize_per_channel_affine(%self.hm.2.weight, %3044, %3043, %3046, %3047, %3048)
  %hm_2.1 : Tensor = aten::_convolution(%quant_input.106, %quant_weight.106, %self.hm.2.bias, %3050, %3051, %3050, %3052, %3051, %3049, %3052, %3052, %3053, %3053)
  %input11.1 : Tensor = aten::sigmoid_(%hm_2.1)
  %max_pool2d.1 : Tensor = aten::max_pool2d(%input11.1, %1452, %1453, %1453, %1453, %1419)
  %3061 : Tensor = prim::Constant[value=<Tensor>]()
  %3062 : Tensor = prim::Constant[value=<Tensor>]()
  %3063 : float = prim::Constant[value=0.079049185505063513]()
  %3064 : int = prim::Constant[value=0]()
  %3065 : int = prim::Constant[value=-128]()
  %3066 : int = prim::Constant[value=127]()
  %3067 : int = prim::Constant[value=1]()
  %3068 : int[] = prim::Constant[value=[1, 1]]()
  %3069 : bool = prim::Constant[value=0]()
  %3070 : int[] = prim::Constant[value=[0, 0]]()
  %3071 : bool = prim::Constant[value=1]()
  %3072 : Tensor = prim::GetAttr[name="bias"]()
  %3073 : Tensor = prim::GetAttr[name="weight"]()
  %3074 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_326.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%19)
  %3075 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_325.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%19)
  %quant_input.108 : Tensor = aten::fake_quantize_per_tensor_affine(%3023, %3063, %3064, %3065, %3066)
  %quant_weight.108 : Tensor = aten::fake_quantize_per_channel_affine(%self.wh.0.weight, %3062, %3061, %3064, %3065, %3066)
  %input.175 : Tensor = aten::_convolution(%quant_input.108, %quant_weight.108, %self.wh.0.bias, %3068, %3068, %3068, %3069, %3070, %3067, %3069, %3069, %3071, %3071)
  %3079 : Tensor = aten::relu_(%input.175)
  %3080 : Tensor = prim::Constant[value= 0  0 [ CUDALongType{2} ]]()
  %3081 : Tensor = prim::Constant[value=0.001 *  2.8907  3.4067 [ CUDAFloatType{2} ]]()
  %3082 : float = prim::Constant[value=0.34322086844857286]()
  %3083 : int = prim::Constant[value=0]()
  %3084 : int = prim::Constant[value=-128]()
  %3085 : int = prim::Constant[value=127]()
  %3086 : int = prim::Constant[value=1]()
  %3087 : int[] = prim::Constant[value=[1, 1]]()
  %3088 : int[] = prim::Constant[value=[0, 0]]()
  %3089 : bool = prim::Constant[value=0]()
  %3090 : bool = prim::Constant[value=1]()
  %3091 : Tensor = prim::GetAttr[name="bias"]()
  %3092 : Tensor = prim::GetAttr[name="weight"]()
  %3093 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_330.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%13)
  %3094 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_329.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%13)
  %quant_input.110 : Tensor = aten::fake_quantize_per_tensor_affine(%3079, %3082, %3083, %3084, %3085)
  %quant_weight.110 : Tensor = aten::fake_quantize_per_channel_affine(%self.wh.2.weight, %3081, %3080, %3083, %3084, %3085)
  %wh_2.1 : Tensor = aten::_convolution(%quant_input.110, %quant_weight.110, %self.wh.2.bias, %3087, %3088, %3087, %3089, %3088, %3086, %3089, %3089, %3090, %3090)
  %3098 : Tensor = prim::Constant[value=<Tensor>]()
  %3099 : Tensor = prim::Constant[value=<Tensor>]()
  %3100 : float = prim::Constant[value=0.079049185505063513]()
  %3101 : int = prim::Constant[value=0]()
  %3102 : int = prim::Constant[value=-128]()
  %3103 : int = prim::Constant[value=127]()
  %3104 : int = prim::Constant[value=1]()
  %3105 : int[] = prim::Constant[value=[1, 1]]()
  %3106 : bool = prim::Constant[value=0]()
  %3107 : int[] = prim::Constant[value=[0, 0]]()
  %3108 : bool = prim::Constant[value=1]()
  %3109 : Tensor = prim::GetAttr[name="bias"]()
  %3110 : Tensor = prim::GetAttr[name="weight"]()
  %3111 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_334.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%10)
  %3112 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_333.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%10)
  %quant_input.112 : Tensor = aten::fake_quantize_per_tensor_affine(%3023, %3100, %3101, %3102, %3103)
  %quant_weight.112 : Tensor = aten::fake_quantize_per_channel_affine(%self.reg.0.weight, %3099, %3098, %3101, %3102, %3103)
  %input.1 : Tensor = aten::_convolution(%quant_input.112, %quant_weight.112, %self.reg.0.bias, %3105, %3105, %3105, %3106, %3107, %3104, %3106, %3106, %3108, %3108)
  %3116 : Tensor = aten::relu_(%input.1)
  %3117 : Tensor = prim::Constant[value= 0  0 [ CUDALongType{2} ]]()
  %3118 : Tensor = prim::Constant[value=0.0001 *  7.2973  9.0729 [ CUDAFloatType{2} ]]()
  %3119 : float = prim::Constant[value=0.12249077774408296]()
  %3120 : int = prim::Constant[value=0]()
  %3121 : int = prim::Constant[value=-128]()
  %3122 : int = prim::Constant[value=127]()
  %3123 : int = prim::Constant[value=1]()
  %3124 : int[] = prim::Constant[value=[1, 1]]()
  %3125 : int[] = prim::Constant[value=[0, 0]]()
  %3126 : bool = prim::Constant[value=0]()
  %3127 : bool = prim::Constant[value=1]()
  %3128 : Tensor = prim::GetAttr[name="bias"]()
  %3129 : Tensor = prim::GetAttr[name="weight"]()
  %3130 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_338.TensorQuantizer = prim::GetAttr[name="_weight_quantizer"](%4)
  %3131 : __torch__.tiacc_training.quant.pytorch_quantization.nn.modules.tensor_quantizer.___torch_mangle_337.TensorQuantizer = prim::GetAttr[name="_input_quantizer"](%4)
  %quant_input.1 : Tensor = aten::fake_quantize_per_tensor_affine(%3116, %3119, %3120, %3121, %3122)
  %quant_weight.1 : Tensor = aten::fake_quantize_per_channel_affine(%self.reg.2.weight, %3118, %3117, %3120, %3121, %3122)
  %reg_2.1 : Tensor = aten::_convolution(%quant_input.1, %quant_weight.1, %self.reg.2.bias, %3124, %3125, %3124, %3126, %3125, %3123, %3126, %3126, %3127, %3127)
  %1440 : Tensor[] = prim::ListConstruct(%input11.1, %max_pool2d.1, %wh_2.1, %reg_2.1)
  %1442 : Tensor = aten::cat(%1440, %944)
  return (%1442)

DOT Graph:
digraph G {
"x.1" -> "aten::fake_quantize_per_tensor_affine---280";
"aten::fake_quantize_per_tensor_affine---280" -> "quant_input.2";
"aten::fake_quantize_per_channel_affine---281" -> "quant_weight.2";
"quant_input.2" -> "aten::_convolution---282";
"quant_weight.2" -> "aten::_convolution---282";
"aten::_convolution---282" -> "input.5";
"input.5" -> "aten::batch_norm---287";
"aten::batch_norm---287" -> "input.7";
"input.7" -> "aten::relu_---288";
"aten::relu_---288" -> "1484";
"1484" -> "aten::fake_quantize_per_tensor_affine---301";
"aten::fake_quantize_per_tensor_affine---301" -> "quant_input.4";
"aten::fake_quantize_per_channel_affine---302" -> "quant_weight.4";
"quant_input.4" -> "aten::_convolution---303";
"quant_weight.4" -> "aten::_convolution---303";
"aten::_convolution---303" -> "input.9";
"input.9" -> "aten::batch_norm---308";
"aten::batch_norm---308" -> "input.11";
"input.11" -> "aten::relu_---309";
"aten::relu_---309" -> "1512";
"1512" -> "aten::fake_quantize_per_tensor_affine---323";
"aten::fake_quantize_per_tensor_affine---323" -> "quant_input.6";
"aten::fake_quantize_per_channel_affine---324" -> "quant_weight.6";
"quant_input.6" -> "aten::_convolution---325";
"quant_weight.6" -> "aten::_convolution---325";
"aten::_convolution---325" -> "input.13";
"input.13" -> "aten::batch_norm---330";
"aten::batch_norm---330" -> "input.15";
"input.15" -> "aten::relu_---331";
"aten::relu_---331" -> "1541";
"1541" -> "aten::max_pool2d---336";
"aten::max_pool2d---336" -> "inputs.5";
"inputs.5" -> "aten::fake_quantize_per_tensor_affine---349";
"aten::fake_quantize_per_tensor_affine---349" -> "quant_input.8";
"aten::fake_quantize_per_channel_affine---350" -> "quant_weight.8";
"quant_input.8" -> "aten::_convolution---351";
"quant_weight.8" -> "aten::_convolution---351";
"aten::_convolution---351" -> "input.17";
"input.17" -> "aten::batch_norm---356";
"aten::batch_norm---356" -> "inputs.7";
"inputs.7" -> "aten::fake_quantize_per_tensor_affine---361";
"aten::fake_quantize_per_tensor_affine---361" -> "base_level2_project_1_output_quant.1";
"1541" -> "aten::fake_quantize_per_tensor_affine---375";
"aten::fake_quantize_per_tensor_affine---375" -> "quant_input.10";
"aten::fake_quantize_per_channel_affine---376" -> "quant_weight.10";
"quant_input.10" -> "aten::_convolution---377";
"quant_weight.10" -> "aten::_convolution---377";
"aten::_convolution---377" -> "input.19";
"input.19" -> "aten::batch_norm---382";
"aten::batch_norm---382" -> "input.21";
"input.21" -> "aten::relu_---383";
"aten::relu_---383" -> "1608";
"1608" -> "aten::fake_quantize_per_tensor_affine---396";
"aten::fake_quantize_per_tensor_affine---396" -> "quant_input.12";
"aten::fake_quantize_per_channel_affine---397" -> "quant_weight.12";
"quant_input.12" -> "aten::_convolution---398";
"quant_weight.12" -> "aten::_convolution---398";
"aten::_convolution---398" -> "input.23";
"input.23" -> "aten::batch_norm---403";
"aten::batch_norm---403" -> "base_level2_tree1_bn2.1";
"base_level2_tree1_bn2.1" -> "aten::add---404";
"base_level2_project_1_output_quant.1" -> "aten::add---404";
"aten::add---404" -> "input.3";
"input.3" -> "aten::relu_---405";
"aten::relu_---405" -> "1636";
"1636" -> "aten::fake_quantize_per_tensor_affine---410";
"aten::fake_quantize_per_tensor_affine---410" -> "base_level2_tree1_relu_output_quant.1";
"1636" -> "aten::fake_quantize_per_tensor_affine---423";
"aten::fake_quantize_per_tensor_affine---423" -> "quant_input.14";
"aten::fake_quantize_per_channel_affine---424" -> "quant_weight.14";
"quant_input.14" -> "aten::_convolution---425";
"quant_weight.14" -> "aten::_convolution---425";
"aten::_convolution---425" -> "input.25";
"input.25" -> "aten::batch_norm---430";
"aten::batch_norm---430" -> "input.27";
"input.27" -> "aten::relu_---431";
"aten::relu_---431" -> "1670";
"1670" -> "aten::fake_quantize_per_tensor_affine---444";
"aten::fake_quantize_per_tensor_affine---444" -> "quant_input.16";
"aten::fake_quantize_per_channel_affine---445" -> "quant_weight.16";
"quant_input.16" -> "aten::_convolution---446";
"quant_weight.16" -> "aten::_convolution---446";
"aten::_convolution---446" -> "input.29";
"input.29" -> "aten::batch_norm---451";
"aten::batch_norm---451" -> "base_level2_tree2_bn2.1";
"base_level2_tree2_bn2.1" -> "aten::add---452";
"base_level2_tree1_relu_output_quant.1" -> "aten::add---452";
"aten::add---452" -> "input0.1";
"input0.1" -> "aten::relu_---453";
"aten::relu_---453" -> "1698";
"1698" -> "prim::ListConstruct---454";
"1636" -> "prim::ListConstruct---454";
"prim::ListConstruct---454" -> "943";
"943" -> "aten::cat---455";
"aten::cat---455" -> "inputs.3";
"inputs.3" -> "aten::fake_quantize_per_tensor_affine---468";
"aten::fake_quantize_per_tensor_affine---468" -> "quant_input.18";
"aten::fake_quantize_per_channel_affine---469" -> "quant_weight.18";
"quant_input.18" -> "aten::_convolution---470";
"quant_weight.18" -> "aten::_convolution---470";
"aten::_convolution---470" -> "input.31";
"input.31" -> "aten::batch_norm---475";
"aten::batch_norm---475" -> "input.33";
"input.33" -> "aten::relu_---476";
"aten::relu_---476" -> "1726";
"1726" -> "aten::max_pool2d---481";
"aten::max_pool2d---481" -> "inputs.9";
"1726" -> "aten::max_pool2d---486";
"aten::max_pool2d---486" -> "inputs.11";
"inputs.11" -> "aten::fake_quantize_per_tensor_affine---499";
"aten::fake_quantize_per_tensor_affine---499" -> "quant_input.22";
"aten::fake_quantize_per_channel_affine---500" -> "quant_weight.22";
"quant_input.22" -> "aten::_convolution---501";
"quant_weight.22" -> "aten::_convolution---501";
"aten::_convolution---501" -> "input.37";
"input.37" -> "aten::batch_norm---506";
"aten::batch_norm---506" -> "inputs.13";
"inputs.13" -> "aten::fake_quantize_per_tensor_affine---511";
"aten::fake_quantize_per_tensor_affine---511" -> "base_level3_tree1_project_1_output_quant.1";
"1726" -> "aten::fake_quantize_per_tensor_affine---525";
"aten::fake_quantize_per_tensor_affine---525" -> "quant_input.24";
"aten::fake_quantize_per_channel_affine---526" -> "quant_weight.24";
"quant_input.24" -> "aten::_convolution---527";
"quant_weight.24" -> "aten::_convolution---527";
"aten::_convolution---527" -> "input.39";
"input.39" -> "aten::batch_norm---532";
"aten::batch_norm---532" -> "input.41";
"input.41" -> "aten::relu_---533";
"aten::relu_---533" -> "1817";
"1817" -> "aten::fake_quantize_per_tensor_affine---546";
"aten::fake_quantize_per_tensor_affine---546" -> "quant_input.26";
"aten::fake_quantize_per_channel_affine---547" -> "quant_weight.26";
"quant_input.26" -> "aten::_convolution---548";
"quant_weight.26" -> "aten::_convolution---548";
"aten::_convolution---548" -> "input.43";
"input.43" -> "aten::batch_norm---553";
"aten::batch_norm---553" -> "base_level3_tree1_tree1_bn2.1";
"base_level3_tree1_tree1_bn2.1" -> "aten::add---554";
"base_level3_tree1_project_1_output_quant.1" -> "aten::add---554";
"aten::add---554" -> "input1.1";
"input1.1" -> "aten::relu_---555";
"aten::relu_---555" -> "1845";
"1845" -> "aten::fake_quantize_per_tensor_affine---560";
"aten::fake_quantize_per_tensor_affine---560" -> "base_level3_tree1_tree1_relu_output_quant.1";
"1845" -> "aten::fake_quantize_per_tensor_affine---573";
"aten::fake_quantize_per_tensor_affine---573" -> "quant_input.28";
"aten::fake_quantize_per_channel_affine---574" -> "quant_weight.28";
"quant_input.28" -> "aten::_convolution---575";
"quant_weight.28" -> "aten::_convolution---575";
"aten::_convolution---575" -> "input.45";
"input.45" -> "aten::batch_norm---580";
"aten::batch_norm---580" -> "input.47";
"input.47" -> "aten::relu_---581";
"aten::relu_---581" -> "1879";
"1879" -> "aten::fake_quantize_per_tensor_affine---594";
"aten::fake_quantize_per_tensor_affine---594" -> "quant_input.30";
"aten::fake_quantize_per_channel_affine---595" -> "quant_weight.30";
"quant_input.30" -> "aten::_convolution---596";
"quant_weight.30" -> "aten::_convolution---596";
"aten::_convolution---596" -> "input.49";
"input.49" -> "aten::batch_norm---601";
"aten::batch_norm---601" -> "base_level3_tree1_tree2_bn2.1";
"base_level3_tree1_tree2_bn2.1" -> "aten::add---602";
"base_level3_tree1_tree1_relu_output_quant.1" -> "aten::add---602";
"aten::add---602" -> "input2.1";
"input2.1" -> "aten::relu_---603";
"aten::relu_---603" -> "1907";
"1907" -> "prim::ListConstruct---604";
"1845" -> "prim::ListConstruct---604";
"prim::ListConstruct---604" -> "1014";
"1014" -> "aten::cat---605";
"aten::cat---605" -> "inputs0.1";
"inputs0.1" -> "aten::fake_quantize_per_tensor_affine---618";
"aten::fake_quantize_per_tensor_affine---618" -> "quant_input.32";
"aten::fake_quantize_per_channel_affine---619" -> "quant_weight.32";
"quant_input.32" -> "aten::_convolution---620";
"quant_weight.32" -> "aten::_convolution---620";
"aten::_convolution---620" -> "input.51";
"input.51" -> "aten::batch_norm---625";
"aten::batch_norm---625" -> "input.53";
"input.53" -> "aten::relu_---626";
"aten::relu_---626" -> "1935";
"1935" -> "aten::fake_quantize_per_tensor_affine---631";
"aten::fake_quantize_per_tensor_affine---631" -> "base_level3_tree1_root_relu_output_quant.1";
"1935" -> "aten::fake_quantize_per_tensor_affine---644";
"aten::fake_quantize_per_tensor_affine---644" -> "quant_input.34";
"aten::fake_quantize_per_channel_affine---645" -> "quant_weight.34";
"quant_input.34" -> "aten::_convolution---646";
"quant_weight.34" -> "aten::_convolution---646";
"aten::_convolution---646" -> "input.55";
"input.55" -> "aten::batch_norm---651";
"aten::batch_norm---651" -> "input.57";
"input.57" -> "aten::relu_---652";
"aten::relu_---652" -> "1969";
"1969" -> "aten::fake_quantize_per_tensor_affine---665";
"aten::fake_quantize_per_tensor_affine---665" -> "quant_input.36";
"aten::fake_quantize_per_channel_affine---666" -> "quant_weight.36";
"quant_input.36" -> "aten::_convolution---667";
"quant_weight.36" -> "aten::_convolution---667";
"aten::_convolution---667" -> "input.59";
"input.59" -> "aten::batch_norm---672";
"aten::batch_norm---672" -> "base_level3_tree2_tree1_bn2.1";
"base_level3_tree2_tree1_bn2.1" -> "aten::add---673";
"base_level3_tree1_root_relu_output_quant.1" -> "aten::add---673";
"aten::add---673" -> "input3.1";
"input3.1" -> "aten::relu_---674";
"aten::relu_---674" -> "1997";
"1997" -> "aten::fake_quantize_per_tensor_affine---679";
"aten::fake_quantize_per_tensor_affine---679" -> "base_level3_tree2_tree1_relu_output_quant.1";
"1997" -> "aten::fake_quantize_per_tensor_affine---692";
"aten::fake_quantize_per_tensor_affine---692" -> "quant_input.38";
"aten::fake_quantize_per_channel_affine---693" -> "quant_weight.38";
"quant_input.38" -> "aten::_convolution---694";
"quant_weight.38" -> "aten::_convolution---694";
"aten::_convolution---694" -> "input.61";
"input.61" -> "aten::batch_norm---699";
"aten::batch_norm---699" -> "input.63";
"input.63" -> "aten::relu_---700";
"aten::relu_---700" -> "2031";
"2031" -> "aten::fake_quantize_per_tensor_affine---713";
"aten::fake_quantize_per_tensor_affine---713" -> "quant_input.40";
"aten::fake_quantize_per_channel_affine---714" -> "quant_weight.40";
"quant_input.40" -> "aten::_convolution---715";
"quant_weight.40" -> "aten::_convolution---715";
"aten::_convolution---715" -> "input.65";
"input.65" -> "aten::batch_norm---720";
"aten::batch_norm---720" -> "base_level3_tree2_tree2_bn2.1";
"base_level3_tree2_tree2_bn2.1" -> "aten::add---721";
"base_level3_tree2_tree1_relu_output_quant.1" -> "aten::add---721";
"aten::add---721" -> "input4.1";
"input4.1" -> "aten::relu_---722";
"aten::relu_---722" -> "2059";
"2059" -> "prim::ListConstruct---723";
"1997" -> "prim::ListConstruct---723";
"inputs.9" -> "prim::ListConstruct---723";
"1935" -> "prim::ListConstruct---723";
"prim::ListConstruct---723" -> "1071";
"1071" -> "aten::cat---724";
"aten::cat---724" -> "inputs1.1";
"inputs1.1" -> "aten::fake_quantize_per_tensor_affine---737";
"aten::fake_quantize_per_tensor_affine---737" -> "quant_input.42";
"aten::fake_quantize_per_channel_affine---738" -> "quant_weight.42";
"quant_input.42" -> "aten::_convolution---739";
"quant_weight.42" -> "aten::_convolution---739";
"aten::_convolution---739" -> "input.67";
"input.67" -> "aten::batch_norm---744";
"aten::batch_norm---744" -> "input.69";
"input.69" -> "aten::relu_---745";
"aten::relu_---745" -> "2087";
"2087" -> "aten::max_pool2d---750";
"aten::max_pool2d---750" -> "inputs.15";
"2087" -> "aten::max_pool2d---755";
"aten::max_pool2d---755" -> "inputs.17";
"inputs.17" -> "aten::fake_quantize_per_tensor_affine---768";
"aten::fake_quantize_per_tensor_affine---768" -> "quant_input.46";
"aten::fake_quantize_per_channel_affine---769" -> "quant_weight.46";
"quant_input.46" -> "aten::_convolution---770";
"quant_weight.46" -> "aten::_convolution---770";
"aten::_convolution---770" -> "input.73";
"input.73" -> "aten::batch_norm---775";
"aten::batch_norm---775" -> "inputs.19";
"inputs.19" -> "aten::fake_quantize_per_tensor_affine---780";
"aten::fake_quantize_per_tensor_affine---780" -> "base_level4_tree1_project_1_output_quant.1";
"2087" -> "aten::fake_quantize_per_tensor_affine---794";
"aten::fake_quantize_per_tensor_affine---794" -> "quant_input.48";
"aten::fake_quantize_per_channel_affine---795" -> "quant_weight.48";
"quant_input.48" -> "aten::_convolution---796";
"quant_weight.48" -> "aten::_convolution---796";
"aten::_convolution---796" -> "input.75";
"input.75" -> "aten::batch_norm---801";
"aten::batch_norm---801" -> "input.77";
"input.77" -> "aten::relu_---802";
"aten::relu_---802" -> "2178";
"2178" -> "aten::fake_quantize_per_tensor_affine---815";
"aten::fake_quantize_per_tensor_affine---815" -> "quant_input.50";
"aten::fake_quantize_per_channel_affine---816" -> "quant_weight.50";
"quant_input.50" -> "aten::_convolution---817";
"quant_weight.50" -> "aten::_convolution---817";
"aten::_convolution---817" -> "input.79";
"input.79" -> "aten::batch_norm---822";
"aten::batch_norm---822" -> "base_level4_tree1_tree1_bn2.1";
"base_level4_tree1_tree1_bn2.1" -> "aten::add---823";
"base_level4_tree1_project_1_output_quant.1" -> "aten::add---823";
"aten::add---823" -> "input5.1";
"input5.1" -> "aten::relu_---824";
"aten::relu_---824" -> "2206";
"2206" -> "aten::fake_quantize_per_tensor_affine---829";
"aten::fake_quantize_per_tensor_affine---829" -> "base_level4_tree1_tree1_relu_output_quant.1";
"2206" -> "aten::fake_quantize_per_tensor_affine---842";
"aten::fake_quantize_per_tensor_affine---842" -> "quant_input.52";
"aten::fake_quantize_per_channel_affine---843" -> "quant_weight.52";
"quant_input.52" -> "aten::_convolution---844";
"quant_weight.52" -> "aten::_convolution---844";
"aten::_convolution---844" -> "input.81";
"input.81" -> "aten::batch_norm---849";
"aten::batch_norm---849" -> "input.83";
"input.83" -> "aten::relu_---850";
"aten::relu_---850" -> "2240";
"2240" -> "aten::fake_quantize_per_tensor_affine---863";
"aten::fake_quantize_per_tensor_affine---863" -> "quant_input.54";
"aten::fake_quantize_per_channel_affine---864" -> "quant_weight.54";
"quant_input.54" -> "aten::_convolution---865";
"quant_weight.54" -> "aten::_convolution---865";
"aten::_convolution---865" -> "input.85";
"input.85" -> "aten::batch_norm---870";
"aten::batch_norm---870" -> "base_level4_tree1_tree2_bn2.1";
"base_level4_tree1_tree2_bn2.1" -> "aten::add---871";
"base_level4_tree1_tree1_relu_output_quant.1" -> "aten::add---871";
"aten::add---871" -> "input6.1";
"input6.1" -> "aten::relu_---872";
"aten::relu_---872" -> "2268";
"2268" -> "prim::ListConstruct---873";
"2206" -> "prim::ListConstruct---873";
"prim::ListConstruct---873" -> "1142";
"1142" -> "aten::cat---874";
"aten::cat---874" -> "inputs2.1";
"inputs2.1" -> "aten::fake_quantize_per_tensor_affine---887";
"aten::fake_quantize_per_tensor_affine---887" -> "quant_input.56";
"aten::fake_quantize_per_channel_affine---888" -> "quant_weight.56";
"quant_input.56" -> "aten::_convolution---889";
"quant_weight.56" -> "aten::_convolution---889";
"aten::_convolution---889" -> "input.87";
"input.87" -> "aten::batch_norm---894";
"aten::batch_norm---894" -> "input.89";
"input.89" -> "aten::relu_---895";
"aten::relu_---895" -> "2296";
"2296" -> "aten::fake_quantize_per_tensor_affine---900";
"aten::fake_quantize_per_tensor_affine---900" -> "base_level4_tree1_root_relu_output_quant.1";
"2296" -> "aten::fake_quantize_per_tensor_affine---913";
"aten::fake_quantize_per_tensor_affine---913" -> "quant_input.58";
"aten::fake_quantize_per_channel_affine---914" -> "quant_weight.58";
"quant_input.58" -> "aten::_convolution---915";
"quant_weight.58" -> "aten::_convolution---915";
"aten::_convolution---915" -> "input.91";
"input.91" -> "aten::batch_norm---920";
"aten::batch_norm---920" -> "input.93";
"input.93" -> "aten::relu_---921";
"aten::relu_---921" -> "2330";
"2330" -> "aten::fake_quantize_per_tensor_affine---934";
"aten::fake_quantize_per_tensor_affine---934" -> "quant_input.60";
"aten::fake_quantize_per_channel_affine---935" -> "quant_weight.60";
"quant_input.60" -> "aten::_convolution---936";
"quant_weight.60" -> "aten::_convolution---936";
"aten::_convolution---936" -> "input.95";
"input.95" -> "aten::batch_norm---941";
"aten::batch_norm---941" -> "base_level4_tree2_tree1_bn2.1";
"base_level4_tree2_tree1_bn2.1" -> "aten::add---942";
"base_level4_tree1_root_relu_output_quant.1" -> "aten::add---942";
"aten::add---942" -> "input7.1";
"input7.1" -> "aten::relu_---943";
"aten::relu_---943" -> "2358";
"2358" -> "aten::fake_quantize_per_tensor_affine---948";
"aten::fake_quantize_per_tensor_affine---948" -> "base_level4_tree2_tree1_relu_output_quant.1";
"2358" -> "aten::fake_quantize_per_tensor_affine---961";
"aten::fake_quantize_per_tensor_affine---961" -> "quant_input.62";
"aten::fake_quantize_per_channel_affine---962" -> "quant_weight.62";
"quant_input.62" -> "aten::_convolution---963";
"quant_weight.62" -> "aten::_convolution---963";
"aten::_convolution---963" -> "input.97";
"input.97" -> "aten::batch_norm---968";
"aten::batch_norm---968" -> "input.99";
"input.99" -> "aten::relu_---969";
"aten::relu_---969" -> "2392";
"2392" -> "aten::fake_quantize_per_tensor_affine---982";
"aten::fake_quantize_per_tensor_affine---982" -> "quant_input.64";
"aten::fake_quantize_per_channel_affine---983" -> "quant_weight.64";
"quant_input.64" -> "aten::_convolution---984";
"quant_weight.64" -> "aten::_convolution---984";
"aten::_convolution---984" -> "input.101";
"input.101" -> "aten::batch_norm---989";
"aten::batch_norm---989" -> "base_level4_tree2_tree2_bn2.1";
"base_level4_tree2_tree2_bn2.1" -> "aten::add---990";
"base_level4_tree2_tree1_relu_output_quant.1" -> "aten::add---990";
"aten::add---990" -> "input8.1";
"input8.1" -> "aten::relu_---991";
"aten::relu_---991" -> "2420";
"2420" -> "prim::ListConstruct---992";
"2358" -> "prim::ListConstruct---992";
"inputs.15" -> "prim::ListConstruct---992";
"2296" -> "prim::ListConstruct---992";
"prim::ListConstruct---992" -> "1199";
"1199" -> "aten::cat---993";
"aten::cat---993" -> "inputs3.1";
"inputs3.1" -> "aten::fake_quantize_per_tensor_affine---1006";
"aten::fake_quantize_per_tensor_affine---1006" -> "quant_input.66";
"aten::fake_quantize_per_channel_affine---1007" -> "quant_weight.66";
"quant_input.66" -> "aten::_convolution---1008";
"quant_weight.66" -> "aten::_convolution---1008";
"aten::_convolution---1008" -> "input.103";
"input.103" -> "aten::batch_norm---1013";
"aten::batch_norm---1013" -> "input.105";
"input.105" -> "aten::relu_---1014";
"aten::relu_---1014" -> "2448";
"2448" -> "aten::max_pool2d---1019";
"aten::max_pool2d---1019" -> "inputs.21";
"inputs.21" -> "aten::fake_quantize_per_tensor_affine---1032";
"aten::fake_quantize_per_tensor_affine---1032" -> "quant_input.68";
"aten::fake_quantize_per_channel_affine---1033" -> "quant_weight.68";
"quant_input.68" -> "aten::_convolution---1034";
"quant_weight.68" -> "aten::_convolution---1034";
"aten::_convolution---1034" -> "input.107";
"input.107" -> "aten::batch_norm---1039";
"aten::batch_norm---1039" -> "inputs.1";
"inputs.1" -> "aten::fake_quantize_per_tensor_affine---1044";
"aten::fake_quantize_per_tensor_affine---1044" -> "base_level5_project_1_output_quant.1";
"2448" -> "aten::fake_quantize_per_tensor_affine---1058";
"aten::fake_quantize_per_tensor_affine---1058" -> "quant_input.70";
"aten::fake_quantize_per_channel_affine---1059" -> "quant_weight.70";
"quant_input.70" -> "aten::_convolution---1060";
"quant_weight.70" -> "aten::_convolution---1060";
"aten::_convolution---1060" -> "input.109";
"input.109" -> "aten::batch_norm---1065";
"aten::batch_norm---1065" -> "input.111";
"input.111" -> "aten::relu_---1066";
"aten::relu_---1066" -> "2515";
"2515" -> "aten::fake_quantize_per_tensor_affine---1079";
"aten::fake_quantize_per_tensor_affine---1079" -> "quant_input.72";
"aten::fake_quantize_per_channel_affine---1080" -> "quant_weight.72";
"quant_input.72" -> "aten::_convolution---1081";
"quant_weight.72" -> "aten::_convolution---1081";
"aten::_convolution---1081" -> "input.113";
"input.113" -> "aten::batch_norm---1086";
"aten::batch_norm---1086" -> "base_level5_tree1_bn2.1";
"base_level5_tree1_bn2.1" -> "aten::add---1087";
"base_level5_project_1_output_quant.1" -> "aten::add---1087";
"aten::add---1087" -> "input9.1";
"input9.1" -> "aten::relu_---1088";
"aten::relu_---1088" -> "2543";
"2543" -> "aten::fake_quantize_per_tensor_affine---1093";
"aten::fake_quantize_per_tensor_affine---1093" -> "base_level5_tree1_relu_output_quant.1";
"2543" -> "aten::fake_quantize_per_tensor_affine---1106";
"aten::fake_quantize_per_tensor_affine---1106" -> "quant_input.74";
"aten::fake_quantize_per_channel_affine---1107" -> "quant_weight.74";
"quant_input.74" -> "aten::_convolution---1108";
"quant_weight.74" -> "aten::_convolution---1108";
"aten::_convolution---1108" -> "input.115";
"input.115" -> "aten::batch_norm---1113";
"aten::batch_norm---1113" -> "input.117";
"input.117" -> "aten::relu_---1114";
"aten::relu_---1114" -> "2577";
"2577" -> "aten::fake_quantize_per_tensor_affine---1127";
"aten::fake_quantize_per_tensor_affine---1127" -> "quant_input.76";
"aten::fake_quantize_per_channel_affine---1128" -> "quant_weight.76";
"quant_input.76" -> "aten::_convolution---1129";
"quant_weight.76" -> "aten::_convolution---1129";
"aten::_convolution---1129" -> "input.119";
"input.119" -> "aten::batch_norm---1134";
"aten::batch_norm---1134" -> "base_level5_tree2_bn2.1";
"base_level5_tree2_bn2.1" -> "aten::add---1135";
"base_level5_tree1_relu_output_quant.1" -> "aten::add---1135";
"aten::add---1135" -> "input10.1";
"input10.1" -> "aten::relu_---1136";
"aten::relu_---1136" -> "2605";
"2605" -> "prim::ListConstruct---1137";
"2543" -> "prim::ListConstruct---1137";
"inputs.21" -> "prim::ListConstruct---1137";
"prim::ListConstruct---1137" -> "1264";
"1264" -> "aten::cat---1138";
"aten::cat---1138" -> "inputs4.1";
"inputs4.1" -> "aten::fake_quantize_per_tensor_affine---1151";
"aten::fake_quantize_per_tensor_affine---1151" -> "quant_input.78";
"aten::fake_quantize_per_channel_affine---1152" -> "quant_weight.78";
"quant_input.78" -> "aten::_convolution---1153";
"quant_weight.78" -> "aten::_convolution---1153";
"aten::_convolution---1153" -> "input.121";
"input.121" -> "aten::batch_norm---1158";
"aten::batch_norm---1158" -> "input.123";
"input.123" -> "aten::relu_---1159";
"aten::relu_---1159" -> "2633";
"2633" -> "aten::fake_quantize_per_tensor_affine---1172";
"aten::fake_quantize_per_tensor_affine---1172" -> "quant_input.80";
"aten::fake_quantize_per_channel_affine---1173" -> "quant_weight.80";
"quant_input.80" -> "aten::_convolution---1174";
"quant_weight.80" -> "aten::_convolution---1174";
"aten::_convolution---1174" -> "input.125";
"input.125" -> "aten::batch_norm---1179";
"aten::batch_norm---1179" -> "input.127";
"input.127" -> "aten::relu_---1180";
"aten::relu_---1180" -> "2661";
"2661" -> "aten::_convolution---1188";
"aten::_convolution---1188" -> "dla_up_ida_0_up_1.1";
"2448" -> "prim::ListConstruct---1189";
"dla_up_ida_0_up_1.1" -> "prim::ListConstruct---1189";
"prim::ListConstruct---1189" -> "1286";
"1286" -> "aten::cat---1190";
"aten::cat---1190" -> "inputs5.1";
"inputs5.1" -> "aten::fake_quantize_per_tensor_affine---1203";
"aten::fake_quantize_per_tensor_affine---1203" -> "quant_input.82";
"aten::fake_quantize_per_channel_affine---1204" -> "quant_weight.82";
"quant_input.82" -> "aten::_convolution---1205";
"quant_weight.82" -> "aten::_convolution---1205";
"aten::_convolution---1205" -> "input.129";
"input.129" -> "aten::batch_norm---1210";
"aten::batch_norm---1210" -> "input.131";
"input.131" -> "aten::relu_---1211";
"aten::relu_---1211" -> "2698";
"2448" -> "aten::fake_quantize_per_tensor_affine---1224";
"aten::fake_quantize_per_tensor_affine---1224" -> "quant_input.84";
"aten::fake_quantize_per_channel_affine---1225" -> "quant_weight.84";
"quant_input.84" -> "aten::_convolution---1226";
"quant_weight.84" -> "aten::_convolution---1226";
"aten::_convolution---1226" -> "input.133";
"input.133" -> "aten::batch_norm---1231";
"aten::batch_norm---1231" -> "input.135";
"input.135" -> "aten::relu_---1232";
"aten::relu_---1232" -> "2726";
"2726" -> "aten::_convolution---1240";
"aten::_convolution---1240" -> "dla_up_ida_1_up_1.1";
"2698" -> "aten::fake_quantize_per_tensor_affine---1253";
"aten::fake_quantize_per_tensor_affine---1253" -> "quant_input.86";
"aten::fake_quantize_per_channel_affine---1254" -> "quant_weight.86";
"quant_input.86" -> "aten::_convolution---1255";
"quant_weight.86" -> "aten::_convolution---1255";
"aten::_convolution---1255" -> "input.137";
"input.137" -> "aten::batch_norm---1260";
"aten::batch_norm---1260" -> "input.139";
"input.139" -> "aten::relu_---1261";
"aten::relu_---1261" -> "2763";
"2763" -> "aten::_convolution---1269";
"aten::_convolution---1269" -> "dla_up_ida_1_up_2.1";
"2087" -> "prim::ListConstruct---1270";
"dla_up_ida_1_up_1.1" -> "prim::ListConstruct---1270";
"prim::ListConstruct---1270" -> "1318";
"1318" -> "aten::cat---1271";
"aten::cat---1271" -> "inputs6.1";
"inputs6.1" -> "aten::fake_quantize_per_tensor_affine---1284";
"aten::fake_quantize_per_tensor_affine---1284" -> "quant_input.88";
"aten::fake_quantize_per_channel_affine---1285" -> "quant_weight.88";
"quant_input.88" -> "aten::_convolution---1286";
"quant_weight.88" -> "aten::_convolution---1286";
"aten::_convolution---1286" -> "input.141";
"input.141" -> "aten::batch_norm---1291";
"aten::batch_norm---1291" -> "input.143";
"input.143" -> "aten::relu_---1292";
"aten::relu_---1292" -> "2800";
"2800" -> "prim::ListConstruct---1293";
"dla_up_ida_1_up_2.1" -> "prim::ListConstruct---1293";
"prim::ListConstruct---1293" -> "1330";
"1330" -> "aten::cat---1294";
"aten::cat---1294" -> "inputs7.1";
"inputs7.1" -> "aten::fake_quantize_per_tensor_affine---1307";
"aten::fake_quantize_per_tensor_affine---1307" -> "quant_input.90";
"aten::fake_quantize_per_channel_affine---1308" -> "quant_weight.90";
"quant_input.90" -> "aten::_convolution---1309";
"quant_weight.90" -> "aten::_convolution---1309";
"aten::_convolution---1309" -> "input.145";
"input.145" -> "aten::batch_norm---1314";
"aten::batch_norm---1314" -> "input.147";
"input.147" -> "aten::relu_---1315";
"aten::relu_---1315" -> "2828";
"2087" -> "aten::fake_quantize_per_tensor_affine---1328";
"aten::fake_quantize_per_tensor_affine---1328" -> "quant_input.92";
"aten::fake_quantize_per_channel_affine---1329" -> "quant_weight.92";
"quant_input.92" -> "aten::_convolution---1330";
"quant_weight.92" -> "aten::_convolution---1330";
"aten::_convolution---1330" -> "input.149";
"input.149" -> "aten::batch_norm---1335";
"aten::batch_norm---1335" -> "input.151";
"input.151" -> "aten::relu_---1336";
"aten::relu_---1336" -> "2856";
"2856" -> "aten::_convolution---1344";
"aten::_convolution---1344" -> "dla_up_ida_2_up_1.1";
"2800" -> "aten::fake_quantize_per_tensor_affine---1357";
"aten::fake_quantize_per_tensor_affine---1357" -> "quant_input.94";
"aten::fake_quantize_per_channel_affine---1358" -> "quant_weight.94";
"quant_input.94" -> "aten::_convolution---1359";
"quant_weight.94" -> "aten::_convolution---1359";
"aten::_convolution---1359" -> "input.153";
"input.153" -> "aten::batch_norm---1364";
"aten::batch_norm---1364" -> "input.155";
"input.155" -> "aten::relu_---1365";
"aten::relu_---1365" -> "2893";
"2893" -> "aten::_convolution---1373";
"aten::_convolution---1373" -> "dla_up_ida_2_up_2.1";
"2828" -> "aten::fake_quantize_per_tensor_affine---1386";
"aten::fake_quantize_per_tensor_affine---1386" -> "quant_input.96";
"aten::fake_quantize_per_channel_affine---1387" -> "quant_weight.96";
"quant_input.96" -> "aten::_convolution---1388";
"quant_weight.96" -> "aten::_convolution---1388";
"aten::_convolution---1388" -> "input.157";
"input.157" -> "aten::batch_norm---1393";
"aten::batch_norm---1393" -> "input.159";
"input.159" -> "aten::relu_---1394";
"aten::relu_---1394" -> "2930";
"2930" -> "aten::_convolution---1402";
"aten::_convolution---1402" -> "dla_up_ida_2_up_3.1";
"1726" -> "prim::ListConstruct---1403";
"dla_up_ida_2_up_1.1" -> "prim::ListConstruct---1403";
"prim::ListConstruct---1403" -> "1372";
"1372" -> "aten::cat---1404";
"aten::cat---1404" -> "inputs8.1";
"inputs8.1" -> "aten::fake_quantize_per_tensor_affine---1417";
"aten::fake_quantize_per_tensor_affine---1417" -> "quant_input.98";
"aten::fake_quantize_per_channel_affine---1418" -> "quant_weight.98";
"quant_input.98" -> "aten::_convolution---1419";
"quant_weight.98" -> "aten::_convolution---1419";
"aten::_convolution---1419" -> "input.161";
"input.161" -> "aten::batch_norm---1424";
"aten::batch_norm---1424" -> "input.163";
"input.163" -> "aten::relu_---1425";
"aten::relu_---1425" -> "2967";
"2967" -> "prim::ListConstruct---1426";
"dla_up_ida_2_up_2.1" -> "prim::ListConstruct---1426";
"prim::ListConstruct---1426" -> "1383";
"1383" -> "aten::cat---1427";
"aten::cat---1427" -> "inputs9.1";
"inputs9.1" -> "aten::fake_quantize_per_tensor_affine---1440";
"aten::fake_quantize_per_tensor_affine---1440" -> "quant_input.100";
"aten::fake_quantize_per_channel_affine---1441" -> "quant_weight.100";
"quant_input.100" -> "aten::_convolution---1442";
"quant_weight.100" -> "aten::_convolution---1442";
"aten::_convolution---1442" -> "input.165";
"input.165" -> "aten::batch_norm---1447";
"aten::batch_norm---1447" -> "input.167";
"input.167" -> "aten::relu_---1448";
"aten::relu_---1448" -> "2995";
"2995" -> "prim::ListConstruct---1449";
"dla_up_ida_2_up_3.1" -> "prim::ListConstruct---1449";
"prim::ListConstruct---1449" -> "1394";
"1394" -> "aten::cat---1450";
"aten::cat---1450" -> "inputs10.1";
"inputs10.1" -> "aten::fake_quantize_per_tensor_affine---1463";
"aten::fake_quantize_per_tensor_affine---1463" -> "quant_input.102";
"aten::fake_quantize_per_channel_affine---1464" -> "quant_weight.102";
"quant_input.102" -> "aten::_convolution---1465";
"quant_weight.102" -> "aten::_convolution---1465";
"aten::_convolution---1465" -> "input.169";
"input.169" -> "aten::batch_norm---1470";
"aten::batch_norm---1470" -> "input.171";
"input.171" -> "aten::relu_---1471";
"aten::relu_---1471" -> "3023";
"3023" -> "aten::fake_quantize_per_tensor_affine---1483";
"aten::fake_quantize_per_tensor_affine---1483" -> "quant_input.104";
"aten::fake_quantize_per_channel_affine---1484" -> "quant_weight.104";
"quant_input.104" -> "aten::_convolution---1485";
"quant_weight.104" -> "aten::_convolution---1485";
"aten::_convolution---1485" -> "input.173";
"input.173" -> "aten::relu_---1486";
"aten::relu_---1486" -> "3042";
"3042" -> "aten::fake_quantize_per_tensor_affine---1498";
"aten::fake_quantize_per_tensor_affine---1498" -> "quant_input.106";
"aten::fake_quantize_per_channel_affine---1499" -> "quant_weight.106";
"quant_input.106" -> "aten::_convolution---1500";
"quant_weight.106" -> "aten::_convolution---1500";
"aten::_convolution---1500" -> "hm_2.1";
"hm_2.1" -> "aten::sigmoid_---1501";
"aten::sigmoid_---1501" -> "input11.1";
"input11.1" -> "aten::max_pool2d---1502";
"aten::max_pool2d---1502" -> "max_pool2d.1";
"3023" -> "aten::fake_quantize_per_tensor_affine---1514";
"aten::fake_quantize_per_tensor_affine---1514" -> "quant_input.108";
"aten::fake_quantize_per_channel_affine---1515" -> "quant_weight.108";
"quant_input.108" -> "aten::_convolution---1516";
"quant_weight.108" -> "aten::_convolution---1516";
"aten::_convolution---1516" -> "input.175";
"input.175" -> "aten::relu_---1517";
"aten::relu_---1517" -> "3079";
"3079" -> "aten::fake_quantize_per_tensor_affine---1529";
"aten::fake_quantize_per_tensor_affine---1529" -> "quant_input.110";
"aten::fake_quantize_per_channel_affine---1530" -> "quant_weight.110";
"quant_input.110" -> "aten::_convolution---1531";
"quant_weight.110" -> "aten::_convolution---1531";
"aten::_convolution---1531" -> "wh_2.1";
"3023" -> "aten::fake_quantize_per_tensor_affine---1543";
"aten::fake_quantize_per_tensor_affine---1543" -> "quant_input.112";
"aten::fake_quantize_per_channel_affine---1544" -> "quant_weight.112";
"quant_input.112" -> "aten::_convolution---1545";
"quant_weight.112" -> "aten::_convolution---1545";
"aten::_convolution---1545" -> "input.1";
"input.1" -> "aten::relu_---1546";
"aten::relu_---1546" -> "3116";
"3116" -> "aten::fake_quantize_per_tensor_affine---1558";
"aten::fake_quantize_per_tensor_affine---1558" -> "quant_input.1";
"aten::fake_quantize_per_channel_affine---1559" -> "quant_weight.1";
"quant_input.1" -> "aten::_convolution---1560";
"quant_weight.1" -> "aten::_convolution---1560";
"aten::_convolution---1560" -> "reg_2.1";
"input11.1" -> "prim::ListConstruct---1561";
"max_pool2d.1" -> "prim::ListConstruct---1561";
"wh_2.1" -> "prim::ListConstruct---1561";
"reg_2.1" -> "prim::ListConstruct---1561";
"prim::ListConstruct---1561" -> "1440";
"1440" -> "aten::cat---1562";
"aten::cat---1562" -> "1442";
}

aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::max_pool2d   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::add   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::add   true
aten::relu_   true
prim::ListConstruct   true
aten::cat   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::max_pool2d   true
aten::max_pool2d   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::add   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::add   true
aten::relu_   true
prim::ListConstruct   true
aten::cat   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::add   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::add   true
aten::relu_   true
prim::ListConstruct   true
aten::cat   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::max_pool2d   true
aten::max_pool2d   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::add   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::add   true
aten::relu_   true
prim::ListConstruct   true
aten::cat   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::add   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::add   true
aten::relu_   true
prim::ListConstruct   true
aten::cat   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::max_pool2d   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::add   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::add   true
aten::relu_   true
prim::ListConstruct   true
aten::cat   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::_convolution   true
prim::ListConstruct   true
aten::cat   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::_convolution   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::_convolution   true
prim::ListConstruct   true
aten::cat   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
prim::ListConstruct   true
aten::cat   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::_convolution   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::_convolution   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::_convolution   true
prim::ListConstruct   true
aten::cat   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
prim::ListConstruct   true
aten::cat   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
prim::ListConstruct   true
aten::cat   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::sigmoid_   true
aten::max_pool2d   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
prim::ListConstruct   true
aten::cat   true
+++++++++++++++++++ after seg +++++++++++++++++++
++++++++++++++++++++++++ size:326 +++++++++++++++++++++++++
++++++++++++++++++++++++ target:1 +++++++++++++++++++++++++
+++++++++++++++++++ after resolve +++++++++++++++++++
++++++++++++++++++++++++ size:326 +++++++++++++++++++++++++
++++++++++++++++++++++++ target:1 +++++++++++++++++++++++++
+++++++++++++++++++ after register +++++++++++++++++++
++++++++++++++++++++++++ size:326 +++++++++++++++++++++++++
++++++++++++++++++++++++ target:1 +++++++++++++++++++++++++
+++++++++++++++++++ after erase +++++++++++++++++++
++++++++++++++++++++++++ size:326 +++++++++++++++++++++++++
++++++++++++++++++++++++ target:1 +++++++++++++++++++++++++
+++++++++++++++++++ after erase2 +++++++++++++++++++
++++++++++++++++++++++++ size:326 +++++++++++++++++++++++++
++++++++++++++++++++++++ target:1 +++++++++++++++++++++++++
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::max_pool2d   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::add   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::add   true
aten::relu_   true
prim::ListConstruct   true
aten::cat   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::max_pool2d   true
aten::max_pool2d   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::add   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::add   true
aten::relu_   true
prim::ListConstruct   true
aten::cat   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::add   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::add   true
aten::relu_   true
prim::ListConstruct   true
aten::cat   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::max_pool2d   true
aten::max_pool2d   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::add   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::add   true
aten::relu_   true
prim::ListConstruct   true
aten::cat   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::add   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::add   true
aten::relu_   true
prim::ListConstruct   true
aten::cat   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::max_pool2d   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::add   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::add   true
aten::relu_   true
prim::ListConstruct   true
aten::cat   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::_convolution   true
prim::ListConstruct   true
aten::cat   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::_convolution   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::_convolution   true
prim::ListConstruct   true
aten::cat   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
prim::ListConstruct   true
aten::cat   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::_convolution   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::_convolution   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::_convolution   true
prim::ListConstruct   true
aten::cat   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
prim::ListConstruct   true
aten::cat   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
prim::ListConstruct   true
aten::cat   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::batch_norm   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::sigmoid_   true
aten::max_pool2d   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
aten::relu_   true
aten::fake_quantize_per_tensor_affine   true
aten::fake_quantize_per_channel_affine   true
aten::_convolution   true
prim::ListConstruct   true
aten::cat   true
+++++++++++++++++++ after seg +++++++++++++++++++
++++++++++++++++++++++++ size:326 +++++++++++++++++++++++++
++++++++++++++++++++++++ target:1 +++++++++++++++++++++++++
+++++++++++++++++++ after resolve +++++++++++++++++++
++++++++++++++++++++++++ size:326 +++++++++++++++++++++++++
++++++++++++++++++++++++ target:1 +++++++++++++++++++++++++
+++++++++++++++++++ after register +++++++++++++++++++
++++++++++++++++++++++++ size:326 +++++++++++++++++++++++++
++++++++++++++++++++++++ target:1 +++++++++++++++++++++++++
+++++++++++++++++++ after erase +++++++++++++++++++
++++++++++++++++++++++++ size:326 +++++++++++++++++++++++++
++++++++++++++++++++++++ target:1 +++++++++++++++++++++++++
+++++++++++++++++++ after erase2 +++++++++++++++++++
++++++++++++++++++++++++ size:326 +++++++++++++++++++++++++
++++++++++++++++++++++++ target:1 +++++++++++++++++++++++++
1
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:64 DataCount:16
Per Channel scale_buf size:64
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:64 DataCount:16
Per Channel scale_buf size:64
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:128 DataCount:32
Per Channel scale_buf size:128
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:256 DataCount:64
Per Channel scale_buf size:256
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:256 DataCount:64
Per Channel scale_buf size:256
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:256 DataCount:64
Per Channel scale_buf size:256
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:256 DataCount:64
Per Channel scale_buf size:256
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:256 DataCount:64
Per Channel scale_buf size:256
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:256 DataCount:64
Per Channel scale_buf size:256
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:512 DataCount:128
Per Channel scale_buf size:512
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:512 DataCount:128
Per Channel scale_buf size:512
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:512 DataCount:128
Per Channel scale_buf size:512
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:512 DataCount:128
Per Channel scale_buf size:512
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:512 DataCount:128
Per Channel scale_buf size:512
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:512 DataCount:128
Per Channel scale_buf size:512
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:512 DataCount:128
Per Channel scale_buf size:512
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:512 DataCount:128
Per Channel scale_buf size:512
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:512 DataCount:128
Per Channel scale_buf size:512
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:512 DataCount:128
Per Channel scale_buf size:512
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:512 DataCount:128
Per Channel scale_buf size:512
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:1024 DataCount:256
Per Channel scale_buf size:1024
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:1024 DataCount:256
Per Channel scale_buf size:1024
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:1024 DataCount:256
Per Channel scale_buf size:1024
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:1024 DataCount:256
Per Channel scale_buf size:1024
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:1024 DataCount:256
Per Channel scale_buf size:1024
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:1024 DataCount:256
Per Channel scale_buf size:1024
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:1024 DataCount:256
Per Channel scale_buf size:1024
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:1024 DataCount:256
Per Channel scale_buf size:1024
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:1024 DataCount:256
Per Channel scale_buf size:1024
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:1024 DataCount:256
Per Channel scale_buf size:1024
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:1024 DataCount:256
Per Channel scale_buf size:1024
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:2048 DataCount:512
Per Channel scale_buf size:2048
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:2048 DataCount:512
Per Channel scale_buf size:2048
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:2048 DataCount:512
Per Channel scale_buf size:2048
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:2048 DataCount:512
Per Channel scale_buf size:2048
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:2048 DataCount:512
Per Channel scale_buf size:2048
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:2048 DataCount:512
Per Channel scale_buf size:2048
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:1024 DataCount:256
Per Channel scale_buf size:1024
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:1024 DataCount:256
Per Channel scale_buf size:1024
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:512 DataCount:128
Per Channel scale_buf size:512
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:512 DataCount:128
Per Channel scale_buf size:512
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:512 DataCount:128
Per Channel scale_buf size:512
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:512 DataCount:128
Per Channel scale_buf size:512
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:256 DataCount:64
Per Channel scale_buf size:256
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:256 DataCount:64
Per Channel scale_buf size:256
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:256 DataCount:64
Per Channel scale_buf size:256
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:256 DataCount:64
Per Channel scale_buf size:256
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:256 DataCount:64
Per Channel scale_buf size:256
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:256 DataCount:64
Per Channel scale_buf size:256
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:1024 DataCount:256
Per Channel scale_buf size:1024
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:512 DataCount:128
Per Channel scale_buf size:512
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:1024 DataCount:256
Per Channel scale_buf size:1024
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:8 DataCount:2
Per Channel scale_buf size:8
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:1024 DataCount:256
Per Channel scale_buf size:1024
In Per Tensor, call getValue
scale_buf size:4 DataCount:1
scale_buf size:4
In Per Channel, call getValue
Per Channel scale_buf size:8 DataCount:2
Per Channel scale_buf size:8
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.2TensorQ data type: 0 shape: [ 1 3 768 768 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.2 data type: 0 shape: [ 1 3 768 768 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.2ChannelQ data type: 0 shape: [ 16 3 7 7 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.base.base_layer.0.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.2 data type: 0 shape: [ 16 3 7 7 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.5 data type: 0 shape: [ 1 16 768 768 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.7 data type: 0 shape: [ 1 16 768 768 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 27 data type: 0 shape: [ 1 16 768 768 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.4TensorQ data type: 0 shape: [ 1 16 768 768 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.4 data type: 0 shape: [ 1 16 768 768 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.4ChannelQ data type: 0 shape: [ 16 16 3 3 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.base.level0.0.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.4 data type: 0 shape: [ 16 16 3 3 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.9 data type: 0 shape: [ 1 16 768 768 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.11 data type: 0 shape: [ 1 16 768 768 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 53 data type: 0 shape: [ 1 16 768 768 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.6TensorQ data type: 0 shape: [ 1 16 768 768 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.6 data type: 0 shape: [ 1 16 768 768 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.6ChannelQ data type: 0 shape: [ 32 16 3 3 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.base.level1.0.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.6 data type: 0 shape: [ 32 16 3 3 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.13 data type: 0 shape: [ 1 32 384 384 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.15 data type: 0 shape: [ 1 32 384 384 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 80 data type: 0 shape: [ 1 32 384 384 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: inputs.5 data type: 0 shape: [ 1 32 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.8TensorQ data type: 0 shape: [ 1 32 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.8 data type: 0 shape: [ 1 32 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.8ChannelQ data type: 0 shape: [ 64 32 1 1 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.base.level2.project.0.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.8 data type: 0 shape: [ 64 32 1 1 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.17 data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: inputs.7 data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: base_level2_project_1_output_quant.1TensorQ data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: base_level2_project_1_output_quant.1 data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.10TensorQ data type: 0 shape: [ 1 32 384 384 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.10 data type: 0 shape: [ 1 32 384 384 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.10ChannelQ data type: 0 shape: [ 64 32 3 3 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.base.level2.tree1.conv1.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.10 data type: 0 shape: [ 64 32 3 3 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.19 data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.21 data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 142 data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.12TensorQ data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.12 data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.12ChannelQ data type: 0 shape: [ 64 64 3 3 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.base.level2.tree1.conv2.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.12 data type: 0 shape: [ 64 64 3 3 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.23 data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: base_level2_tree1_bn2.1 data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.3 data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 170 data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: base_level2_tree1_relu_output_quant.1TensorQ data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: base_level2_tree1_relu_output_quant.1 data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.14TensorQ data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.14 data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.14ChannelQ data type: 0 shape: [ 64 64 3 3 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.base.level2.tree2.conv1.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.14 data type: 0 shape: [ 64 64 3 3 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.25 data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.27 data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 201 data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.16TensorQ data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.16 data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.16ChannelQ data type: 0 shape: [ 64 64 3 3 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.base.level2.tree2.conv2.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.16 data type: 0 shape: [ 64 64 3 3 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.29 data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: base_level2_tree2_bn2.1 data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input0.1 data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 228 data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: inputs.3 data type: 0 shape: [ 1 128 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.18TensorQ data type: 0 shape: [ 1 128 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.18 data type: 0 shape: [ 1 128 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.18ChannelQ data type: 0 shape: [ 64 128 1 1 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.base.level2.root.conv.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.18 data type: 0 shape: [ 64 128 1 1 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.31 data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.33 data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 256 data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: inputs.9 data type: 0 shape: [ 1 64 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: inputs.11 data type: 0 shape: [ 1 64 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.22TensorQ data type: 0 shape: [ 1 64 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.22 data type: 0 shape: [ 1 64 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.22ChannelQ data type: 0 shape: [ 128 64 1 1 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.base.level3.tree1.project.0.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.22 data type: 0 shape: [ 128 64 1 1 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.37 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: inputs.13 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: base_level3_tree1_project_1_output_quant.1TensorQ data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: base_level3_tree1_project_1_output_quant.1 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.24TensorQ data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.24 data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.24ChannelQ data type: 0 shape: [ 128 64 3 3 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.base.level3.tree1.tree1.conv1.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.24 data type: 0 shape: [ 128 64 3 3 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.39 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.41 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 323 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.26TensorQ data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.26 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.26ChannelQ data type: 0 shape: [ 128 128 3 3 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.base.level3.tree1.tree1.conv2.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.26 data type: 0 shape: [ 128 128 3 3 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.43 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: base_level3_tree1_tree1_bn2.1 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input1.1 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 350 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: base_level3_tree1_tree1_relu_output_quant.1TensorQ data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: base_level3_tree1_tree1_relu_output_quant.1 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.28TensorQ data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.28 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.28ChannelQ data type: 0 shape: [ 128 128 3 3 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.base.level3.tree1.tree2.conv1.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.28 data type: 0 shape: [ 128 128 3 3 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.45 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.47 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 381 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.30TensorQ data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.30 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.30ChannelQ data type: 0 shape: [ 128 128 3 3 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.base.level3.tree1.tree2.conv2.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.30 data type: 0 shape: [ 128 128 3 3 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.49 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: base_level3_tree1_tree2_bn2.1 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input2.1 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 408 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: inputs0.1 data type: 0 shape: [ 1 256 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.32TensorQ data type: 0 shape: [ 1 256 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.32 data type: 0 shape: [ 1 256 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.32ChannelQ data type: 0 shape: [ 128 256 1 1 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.base.level3.tree1.root.conv.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.32 data type: 0 shape: [ 128 256 1 1 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.51 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.53 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 436 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: base_level3_tree1_root_relu_output_quant.1TensorQ data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: base_level3_tree1_root_relu_output_quant.1 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.34TensorQ data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.34 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.34ChannelQ data type: 0 shape: [ 128 128 3 3 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.base.level3.tree2.tree1.conv1.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.34 data type: 0 shape: [ 128 128 3 3 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.55 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.57 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 467 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.36TensorQ data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.36 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.36ChannelQ data type: 0 shape: [ 128 128 3 3 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.base.level3.tree2.tree1.conv2.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.36 data type: 0 shape: [ 128 128 3 3 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.59 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: base_level3_tree2_tree1_bn2.1 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input3.1 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 494 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: base_level3_tree2_tree1_relu_output_quant.1TensorQ data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: base_level3_tree2_tree1_relu_output_quant.1 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.38TensorQ data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.38 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.38ChannelQ data type: 0 shape: [ 128 128 3 3 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.base.level3.tree2.tree2.conv1.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.38 data type: 0 shape: [ 128 128 3 3 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.61 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.63 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 525 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.40TensorQ data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.40 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.40ChannelQ data type: 0 shape: [ 128 128 3 3 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.base.level3.tree2.tree2.conv2.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.40 data type: 0 shape: [ 128 128 3 3 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.65 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: base_level3_tree2_tree2_bn2.1 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input4.1 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 552 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: inputs1.1 data type: 0 shape: [ 1 448 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.42TensorQ data type: 0 shape: [ 1 448 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.42 data type: 0 shape: [ 1 448 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.42ChannelQ data type: 0 shape: [ 128 448 1 1 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.base.level3.tree2.root.conv.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.42 data type: 0 shape: [ 128 448 1 1 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.67 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.69 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 580 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: inputs.15 data type: 0 shape: [ 1 128 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: inputs.17 data type: 0 shape: [ 1 128 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.46TensorQ data type: 0 shape: [ 1 128 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.46 data type: 0 shape: [ 1 128 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.46ChannelQ data type: 0 shape: [ 256 128 1 1 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.base.level4.tree1.project.0.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.46 data type: 0 shape: [ 256 128 1 1 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.73 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: inputs.19 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: base_level4_tree1_project_1_output_quant.1TensorQ data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: base_level4_tree1_project_1_output_quant.1 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.48TensorQ data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.48 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.48ChannelQ data type: 0 shape: [ 256 128 3 3 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.base.level4.tree1.tree1.conv1.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.48 data type: 0 shape: [ 256 128 3 3 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.75 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.77 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 647 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.50TensorQ data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.50 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.50ChannelQ data type: 0 shape: [ 256 256 3 3 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.base.level4.tree1.tree1.conv2.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.50 data type: 0 shape: [ 256 256 3 3 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.79 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: base_level4_tree1_tree1_bn2.1 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input5.1 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 674 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: base_level4_tree1_tree1_relu_output_quant.1TensorQ data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: base_level4_tree1_tree1_relu_output_quant.1 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.52TensorQ data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.52 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.52ChannelQ data type: 0 shape: [ 256 256 3 3 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.base.level4.tree1.tree2.conv1.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.52 data type: 0 shape: [ 256 256 3 3 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.81 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.83 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 705 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.54TensorQ data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.54 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.54ChannelQ data type: 0 shape: [ 256 256 3 3 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.base.level4.tree1.tree2.conv2.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.54 data type: 0 shape: [ 256 256 3 3 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.85 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: base_level4_tree1_tree2_bn2.1 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input6.1 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 732 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: inputs2.1 data type: 0 shape: [ 1 512 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.56TensorQ data type: 0 shape: [ 1 512 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.56 data type: 0 shape: [ 1 512 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.56ChannelQ data type: 0 shape: [ 256 512 1 1 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.base.level4.tree1.root.conv.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.56 data type: 0 shape: [ 256 512 1 1 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.87 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.89 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 760 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: base_level4_tree1_root_relu_output_quant.1TensorQ data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: base_level4_tree1_root_relu_output_quant.1 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.58TensorQ data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.58 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.58ChannelQ data type: 0 shape: [ 256 256 3 3 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.base.level4.tree2.tree1.conv1.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.58 data type: 0 shape: [ 256 256 3 3 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.91 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.93 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 791 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.60TensorQ data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.60 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.60ChannelQ data type: 0 shape: [ 256 256 3 3 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.base.level4.tree2.tree1.conv2.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.60 data type: 0 shape: [ 256 256 3 3 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.95 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: base_level4_tree2_tree1_bn2.1 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input7.1 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 818 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: base_level4_tree2_tree1_relu_output_quant.1TensorQ data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: base_level4_tree2_tree1_relu_output_quant.1 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.62TensorQ data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.62 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.62ChannelQ data type: 0 shape: [ 256 256 3 3 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.base.level4.tree2.tree2.conv1.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.62 data type: 0 shape: [ 256 256 3 3 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.97 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.99 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 849 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.64TensorQ data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.64 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.64ChannelQ data type: 0 shape: [ 256 256 3 3 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.base.level4.tree2.tree2.conv2.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.64 data type: 0 shape: [ 256 256 3 3 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.101 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: base_level4_tree2_tree2_bn2.1 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input8.1 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 876 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: inputs3.1 data type: 0 shape: [ 1 896 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.66TensorQ data type: 0 shape: [ 1 896 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.66 data type: 0 shape: [ 1 896 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.66ChannelQ data type: 0 shape: [ 256 896 1 1 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.base.level4.tree2.root.conv.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.66 data type: 0 shape: [ 256 896 1 1 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.103 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.105 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 904 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: inputs.21 data type: 0 shape: [ 1 256 24 24 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.68TensorQ data type: 0 shape: [ 1 256 24 24 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.68 data type: 0 shape: [ 1 256 24 24 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.68ChannelQ data type: 0 shape: [ 512 256 1 1 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.base.level5.project.0.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.68 data type: 0 shape: [ 512 256 1 1 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.107 data type: 0 shape: [ 1 512 24 24 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: inputs.1 data type: 0 shape: [ 1 512 24 24 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: base_level5_project_1_output_quant.1TensorQ data type: 0 shape: [ 1 512 24 24 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: base_level5_project_1_output_quant.1 data type: 0 shape: [ 1 512 24 24 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.70TensorQ data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.70 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.70ChannelQ data type: 0 shape: [ 512 256 3 3 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.base.level5.tree1.conv1.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.70 data type: 0 shape: [ 512 256 3 3 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.109 data type: 0 shape: [ 1 512 24 24 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.111 data type: 0 shape: [ 1 512 24 24 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 966 data type: 0 shape: [ 1 512 24 24 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.72TensorQ data type: 0 shape: [ 1 512 24 24 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.72 data type: 0 shape: [ 1 512 24 24 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.72ChannelQ data type: 0 shape: [ 512 512 3 3 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.base.level5.tree1.conv2.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.72 data type: 0 shape: [ 512 512 3 3 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.113 data type: 0 shape: [ 1 512 24 24 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: base_level5_tree1_bn2.1 data type: 0 shape: [ 1 512 24 24 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input9.1 data type: 0 shape: [ 1 512 24 24 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 993 data type: 0 shape: [ 1 512 24 24 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: base_level5_tree1_relu_output_quant.1TensorQ data type: 0 shape: [ 1 512 24 24 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: base_level5_tree1_relu_output_quant.1 data type: 0 shape: [ 1 512 24 24 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.74TensorQ data type: 0 shape: [ 1 512 24 24 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.74 data type: 0 shape: [ 1 512 24 24 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.74ChannelQ data type: 0 shape: [ 512 512 3 3 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.base.level5.tree2.conv1.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.74 data type: 0 shape: [ 512 512 3 3 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.115 data type: 0 shape: [ 1 512 24 24 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.117 data type: 0 shape: [ 1 512 24 24 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 1024 data type: 0 shape: [ 1 512 24 24 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.76TensorQ data type: 0 shape: [ 1 512 24 24 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.76 data type: 0 shape: [ 1 512 24 24 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.76ChannelQ data type: 0 shape: [ 512 512 3 3 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.base.level5.tree2.conv2.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.76 data type: 0 shape: [ 512 512 3 3 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.119 data type: 0 shape: [ 1 512 24 24 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: base_level5_tree2_bn2.1 data type: 0 shape: [ 1 512 24 24 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input10.1 data type: 0 shape: [ 1 512 24 24 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 1051 data type: 0 shape: [ 1 512 24 24 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: inputs4.1 data type: 0 shape: [ 1 1280 24 24 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.78TensorQ data type: 0 shape: [ 1 1280 24 24 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.78 data type: 0 shape: [ 1 1280 24 24 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.78ChannelQ data type: 0 shape: [ 512 1280 1 1 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.base.level5.root.conv.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.78 data type: 0 shape: [ 512 1280 1 1 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.121 data type: 0 shape: [ 1 512 24 24 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.123 data type: 0 shape: [ 1 512 24 24 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 1079 data type: 0 shape: [ 1 512 24 24 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.80TensorQ data type: 0 shape: [ 1 512 24 24 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.80 data type: 0 shape: [ 1 512 24 24 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.80ChannelQ data type: 0 shape: [ 256 512 1 1 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.dla_up.ida_0.proj_1.0.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.80 data type: 0 shape: [ 256 512 1 1 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.125 data type: 0 shape: [ 1 256 24 24 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.127 data type: 0 shape: [ 1 256 24 24 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 1105 data type: 0 shape: [ 1 256 24 24 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: dla_up_ida_0_up_1.1 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: inputs5.1 data type: 0 shape: [ 1 512 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.82TensorQ data type: 0 shape: [ 1 512 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.82 data type: 0 shape: [ 1 512 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.82ChannelQ data type: 0 shape: [ 256 512 3 3 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.dla_up.ida_0.node_1.0.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.82 data type: 0 shape: [ 256 512 3 3 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.129 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.131 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 1142 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.84TensorQ data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.84 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.84ChannelQ data type: 0 shape: [ 128 256 1 1 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.dla_up.ida_1.proj_1.0.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.84 data type: 0 shape: [ 128 256 1 1 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.133 data type: 0 shape: [ 1 128 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.135 data type: 0 shape: [ 1 128 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 1168 data type: 0 shape: [ 1 128 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: dla_up_ida_1_up_1.1 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.86TensorQ data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.86 data type: 0 shape: [ 1 256 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.86ChannelQ data type: 0 shape: [ 128 256 1 1 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.dla_up.ida_1.proj_2.0.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.86 data type: 0 shape: [ 128 256 1 1 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.137 data type: 0 shape: [ 1 128 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.139 data type: 0 shape: [ 1 128 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 1203 data type: 0 shape: [ 1 128 48 48 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: dla_up_ida_1_up_2.1 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: inputs6.1 data type: 0 shape: [ 1 256 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.88TensorQ data type: 0 shape: [ 1 256 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.88 data type: 0 shape: [ 1 256 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.88ChannelQ data type: 0 shape: [ 128 256 3 3 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.dla_up.ida_1.node_1.0.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.88 data type: 0 shape: [ 128 256 3 3 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.141 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.143 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 1240 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: inputs7.1 data type: 0 shape: [ 1 256 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.90TensorQ data type: 0 shape: [ 1 256 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.90 data type: 0 shape: [ 1 256 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.90ChannelQ data type: 0 shape: [ 128 256 3 3 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.dla_up.ida_1.node_2.0.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.90 data type: 0 shape: [ 128 256 3 3 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.145 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.147 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 1268 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.92TensorQ data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.92 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.92ChannelQ data type: 0 shape: [ 64 128 1 1 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.dla_up.ida_2.proj_1.0.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.92 data type: 0 shape: [ 64 128 1 1 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.149 data type: 0 shape: [ 1 64 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.151 data type: 0 shape: [ 1 64 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 1294 data type: 0 shape: [ 1 64 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: dla_up_ida_2_up_1.1 data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.94TensorQ data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.94 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.94ChannelQ data type: 0 shape: [ 64 128 1 1 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.dla_up.ida_2.proj_2.0.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.94 data type: 0 shape: [ 64 128 1 1 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.153 data type: 0 shape: [ 1 64 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.155 data type: 0 shape: [ 1 64 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 1329 data type: 0 shape: [ 1 64 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: dla_up_ida_2_up_2.1 data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.96TensorQ data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.96 data type: 0 shape: [ 1 128 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.96ChannelQ data type: 0 shape: [ 64 128 1 1 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.dla_up.ida_2.proj_3.0.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.96 data type: 0 shape: [ 64 128 1 1 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.157 data type: 0 shape: [ 1 64 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.159 data type: 0 shape: [ 1 64 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 1364 data type: 0 shape: [ 1 64 96 96 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: dla_up_ida_2_up_3.1 data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: inputs8.1 data type: 0 shape: [ 1 128 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.98TensorQ data type: 0 shape: [ 1 128 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.98 data type: 0 shape: [ 1 128 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.98ChannelQ data type: 0 shape: [ 64 128 3 3 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.dla_up.ida_2.node_1.0.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.98 data type: 0 shape: [ 64 128 3 3 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.161 data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.163 data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 1401 data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: inputs9.1 data type: 0 shape: [ 1 128 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.100TensorQ data type: 0 shape: [ 1 128 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.100 data type: 0 shape: [ 1 128 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.100ChannelQ data type: 0 shape: [ 64 128 3 3 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.dla_up.ida_2.node_2.0.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.100 data type: 0 shape: [ 64 128 3 3 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.165 data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.167 data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 1429 data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: inputs10.1 data type: 0 shape: [ 1 128 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.102TensorQ data type: 0 shape: [ 1 128 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.102 data type: 0 shape: [ 1 128 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.102ChannelQ data type: 0 shape: [ 64 128 3 3 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.dla_up.ida_2.node_3.0.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.102 data type: 0 shape: [ 64 128 3 3 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.169 data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.171 data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 1457 data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.104TensorQ data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.104 data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.104ChannelQ data type: 0 shape: [ 256 64 3 3 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.hm.0.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.104 data type: 0 shape: [ 256 64 3 3 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.173 data type: 0 shape: [ 1 256 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 1474 data type: 0 shape: [ 1 256 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.106TensorQ data type: 0 shape: [ 1 256 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.106 data type: 0 shape: [ 1 256 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.106ChannelQ data type: 0 shape: [ 128 256 1 1 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.hm.2.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.106 data type: 0 shape: [ 128 256 1 1 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: hm_2.1 data type: 0 shape: [ 1 128 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input11.1 data type: 0 shape: [ 1 128 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: max_pool2d.1 data type: 0 shape: [ 1 128 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.108TensorQ data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.108 data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.108ChannelQ data type: 0 shape: [ 256 64 3 3 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.wh.0.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.108 data type: 0 shape: [ 256 64 3 3 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.175 data type: 0 shape: [ 1 256 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 1512 data type: 0 shape: [ 1 256 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.110TensorQ data type: 0 shape: [ 1 256 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.110 data type: 0 shape: [ 1 256 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.110ChannelQ data type: 0 shape: [ 2 256 1 1 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.wh.2.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.110 data type: 0 shape: [ 2 256 1 1 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: wh_2.1 data type: 0 shape: [ 1 2 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.112TensorQ data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.112 data type: 0 shape: [ 1 64 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.112ChannelQ data type: 0 shape: [ 256 64 3 3 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.reg.0.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.112 data type: 0 shape: [ 256 64 3 3 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: input.1 data type: 0 shape: [ 1 256 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 1545 data type: 0 shape: [ 1 256 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.1TensorQ data type: 0 shape: [ 1 256 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_input.1 data type: 0 shape: [ 1 256 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.1ChannelQ data type: 0 shape: [ 2 256 1 1 ]
D/tnn: ReloadConstantBlobs [File /data/TNN-host/source/tnn/device/cuda/acc/cuda_layer_acc.cc][Line 81] CUDA Reload constant blob: self.reg.2.weight
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: quant_weight.1 data type: 0 shape: [ 2 256 1 1 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: reg_2.1 data type: 0 shape: [ 1 2 192 192 ]
D/tnn: Init [File /data/TNN-host/source/tnn/layer/base_layer.cc][Line 64] InferOutputShape: name: 1563 data type: 0 shape: [ 1 260 192 192 ]
[MemUsageChange] Init CUDA: CPU -12, GPU +0, now: CPU 320, GPU 1866 (MiB)
[MemUsageSnapshot] Begin constructing builder kernel library: CPU 338 MiB, GPU 1866 MiB
[MemUsageSnapshot] End constructing builder kernel library: CPU 309 MiB, GPU 1940 MiB
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.base.base_layer.0.weight> count:2352 DataType:0 shape:[16,3,7,7,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.base.level0.0.weight> count:2304 DataType:0 shape:[16,16,3,3,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.base.level1.0.weight> count:4608 DataType:0 shape:[32,16,3,3,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.base.level2.project.0.weight> count:2048 DataType:0 shape:[64,32,1,1,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.base.level2.root.conv.weight> count:8192 DataType:0 shape:[64,128,1,1,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.base.level2.tree1.conv1.weight> count:18432 DataType:0 shape:[64,32,3,3,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.base.level2.tree1.conv2.weight> count:36864 DataType:0 shape:[64,64,3,3,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.base.level2.tree2.conv1.weight> count:36864 DataType:0 shape:[64,64,3,3,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.base.level2.tree2.conv2.weight> count:36864 DataType:0 shape:[64,64,3,3,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.base.level3.tree1.project.0.weight> count:8192 DataType:0 shape:[128,64,1,1,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.base.level3.tree1.root.conv.weight> count:32768 DataType:0 shape:[128,256,1,1,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.base.level3.tree1.tree1.conv1.weight> count:73728 DataType:0 shape:[128,64,3,3,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.base.level3.tree1.tree1.conv2.weight> count:147456 DataType:0 shape:[128,128,3,3,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.base.level3.tree1.tree2.conv1.weight> count:147456 DataType:0 shape:[128,128,3,3,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.base.level3.tree1.tree2.conv2.weight> count:147456 DataType:0 shape:[128,128,3,3,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.base.level3.tree2.root.conv.weight> count:57344 DataType:0 shape:[128,448,1,1,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.base.level3.tree2.tree1.conv1.weight> count:147456 DataType:0 shape:[128,128,3,3,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.base.level3.tree2.tree1.conv2.weight> count:147456 DataType:0 shape:[128,128,3,3,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.base.level3.tree2.tree2.conv1.weight> count:147456 DataType:0 shape:[128,128,3,3,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.base.level3.tree2.tree2.conv2.weight> count:147456 DataType:0 shape:[128,128,3,3,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.base.level4.tree1.project.0.weight> count:32768 DataType:0 shape:[256,128,1,1,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.base.level4.tree1.root.conv.weight> count:131072 DataType:0 shape:[256,512,1,1,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.base.level4.tree1.tree1.conv1.weight> count:294912 DataType:0 shape:[256,128,3,3,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.base.level4.tree1.tree1.conv2.weight> count:589824 DataType:0 shape:[256,256,3,3,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.base.level4.tree1.tree2.conv1.weight> count:589824 DataType:0 shape:[256,256,3,3,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.base.level4.tree1.tree2.conv2.weight> count:589824 DataType:0 shape:[256,256,3,3,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.base.level4.tree2.root.conv.weight> count:229376 DataType:0 shape:[256,896,1,1,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.base.level4.tree2.tree1.conv1.weight> count:589824 DataType:0 shape:[256,256,3,3,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.base.level4.tree2.tree1.conv2.weight> count:589824 DataType:0 shape:[256,256,3,3,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.base.level4.tree2.tree2.conv1.weight> count:589824 DataType:0 shape:[256,256,3,3,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.base.level4.tree2.tree2.conv2.weight> count:589824 DataType:0 shape:[256,256,3,3,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.base.level5.project.0.weight> count:131072 DataType:0 shape:[512,256,1,1,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.base.level5.root.conv.weight> count:655360 DataType:0 shape:[512,1280,1,1,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.base.level5.tree1.conv1.weight> count:1179648 DataType:0 shape:[512,256,3,3,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.base.level5.tree1.conv2.weight> count:2359296 DataType:0 shape:[512,512,3,3,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.base.level5.tree2.conv1.weight> count:2359296 DataType:0 shape:[512,512,3,3,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.base.level5.tree2.conv2.weight> count:2359296 DataType:0 shape:[512,512,3,3,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.dla_up.ida_0.node_1.0.weight> count:1179648 DataType:0 shape:[256,512,3,3,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.dla_up.ida_0.proj_1.0.weight> count:131072 DataType:0 shape:[256,512,1,1,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.dla_up.ida_1.node_1.0.weight> count:294912 DataType:0 shape:[128,256,3,3,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.dla_up.ida_1.node_2.0.weight> count:294912 DataType:0 shape:[128,256,3,3,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.dla_up.ida_1.proj_1.0.weight> count:32768 DataType:0 shape:[128,256,1,1,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.dla_up.ida_1.proj_2.0.weight> count:32768 DataType:0 shape:[128,256,1,1,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.dla_up.ida_2.node_1.0.weight> count:73728 DataType:0 shape:[64,128,3,3,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.dla_up.ida_2.node_2.0.weight> count:73728 DataType:0 shape:[64,128,3,3,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.dla_up.ida_2.node_3.0.weight> count:73728 DataType:0 shape:[64,128,3,3,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.dla_up.ida_2.proj_1.0.weight> count:8192 DataType:0 shape:[64,128,1,1,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.dla_up.ida_2.proj_2.0.weight> count:8192 DataType:0 shape:[64,128,1,1,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.dla_up.ida_2.proj_3.0.weight> count:8192 DataType:0 shape:[64,128,1,1,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.hm.0.weight> count:147456 DataType:0 shape:[256,64,3,3,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.hm.2.weight> count:32768 DataType:0 shape:[128,256,1,1,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.reg.0.weight> count:147456 DataType:0 shape:[256,64,3,3,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.reg.2.weight> count:512 DataType:0 shape:[2,256,1,1,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.wh.0.weight> count:147456 DataType:0 shape:[256,64,3,3,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 688] Adding <self.wh.2.weight> count:512 DataType:0 shape:[2,256,1,1,] as weights from constant_map to trt network
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.2TensorQ", tensor shape -1,3,-1,-1, blob shape:1,3,768,768,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.2", tensor shape -1,3,-1,-1, blob shape:1,3,768,768,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.2ChannelQ", tensor shape 16,3,7,7, blob shape:16,3,7,7,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.2", tensor shape 16,3,7,7, blob shape:16,3,7,7,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.5", tensor shape -1,16,-1,-1, blob shape:1,16,768,768,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.7", tensor shape -1,16,-1,-1, blob shape:1,16,768,768,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "27", tensor shape -1,16,-1,-1, blob shape:1,16,768,768,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.4TensorQ", tensor shape -1,16,-1,-1, blob shape:1,16,768,768,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.4", tensor shape -1,16,-1,-1, blob shape:1,16,768,768,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.4ChannelQ", tensor shape 16,16,3,3, blob shape:16,16,3,3,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.4", tensor shape 16,16,3,3, blob shape:16,16,3,3,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.9", tensor shape -1,16,-1,-1, blob shape:1,16,768,768,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.11", tensor shape -1,16,-1,-1, blob shape:1,16,768,768,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "53", tensor shape -1,16,-1,-1, blob shape:1,16,768,768,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.6TensorQ", tensor shape -1,16,-1,-1, blob shape:1,16,768,768,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.6", tensor shape -1,16,-1,-1, blob shape:1,16,768,768,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.6ChannelQ", tensor shape 32,16,3,3, blob shape:32,16,3,3,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.6", tensor shape 32,16,3,3, blob shape:32,16,3,3,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.13", tensor shape -1,32,-1,-1, blob shape:1,32,384,384,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.15", tensor shape -1,32,-1,-1, blob shape:1,32,384,384,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "80", tensor shape -1,32,-1,-1, blob shape:1,32,384,384,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "inputs.5", tensor shape -1,32,-1,-1, blob shape:1,32,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.8TensorQ", tensor shape -1,32,-1,-1, blob shape:1,32,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.8", tensor shape -1,32,-1,-1, blob shape:1,32,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.8ChannelQ", tensor shape 64,32,1,1, blob shape:64,32,1,1,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.8", tensor shape 64,32,1,1, blob shape:64,32,1,1,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.17", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "inputs.7", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "base_level2_project_1_output_quant.1TensorQ", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "base_level2_project_1_output_quant.1", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.10TensorQ", tensor shape -1,32,-1,-1, blob shape:1,32,384,384,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.10", tensor shape -1,32,-1,-1, blob shape:1,32,384,384,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.10ChannelQ", tensor shape 64,32,3,3, blob shape:64,32,3,3,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.10", tensor shape 64,32,3,3, blob shape:64,32,3,3,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.19", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.21", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "142", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.12TensorQ", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.12", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.12ChannelQ", tensor shape 64,64,3,3, blob shape:64,64,3,3,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.12", tensor shape 64,64,3,3, blob shape:64,64,3,3,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.23", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "base_level2_tree1_bn2.1", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.3", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "170", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "base_level2_tree1_relu_output_quant.1TensorQ", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "base_level2_tree1_relu_output_quant.1", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.14TensorQ", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.14", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.14ChannelQ", tensor shape 64,64,3,3, blob shape:64,64,3,3,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.14", tensor shape 64,64,3,3, blob shape:64,64,3,3,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.25", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.27", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "201", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.16TensorQ", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.16", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.16ChannelQ", tensor shape 64,64,3,3, blob shape:64,64,3,3,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.16", tensor shape 64,64,3,3, blob shape:64,64,3,3,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.29", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "base_level2_tree2_bn2.1", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input0.1", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "228", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "inputs.3", tensor shape -1,128,-1,-1, blob shape:1,128,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.18TensorQ", tensor shape -1,128,-1,-1, blob shape:1,128,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.18", tensor shape -1,128,-1,-1, blob shape:1,128,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.18ChannelQ", tensor shape 64,128,1,1, blob shape:64,128,1,1,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.18", tensor shape 64,128,1,1, blob shape:64,128,1,1,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.31", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.33", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "256", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "inputs.9", tensor shape -1,64,-1,-1, blob shape:1,64,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "inputs.11", tensor shape -1,64,-1,-1, blob shape:1,64,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.22TensorQ", tensor shape -1,64,-1,-1, blob shape:1,64,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.22", tensor shape -1,64,-1,-1, blob shape:1,64,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.22ChannelQ", tensor shape 128,64,1,1, blob shape:128,64,1,1,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.22", tensor shape 128,64,1,1, blob shape:128,64,1,1,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.37", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "inputs.13", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "base_level3_tree1_project_1_output_quant.1TensorQ", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "base_level3_tree1_project_1_output_quant.1", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.24TensorQ", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.24", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.24ChannelQ", tensor shape 128,64,3,3, blob shape:128,64,3,3,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.24", tensor shape 128,64,3,3, blob shape:128,64,3,3,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.39", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.41", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "323", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.26TensorQ", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.26", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.26ChannelQ", tensor shape 128,128,3,3, blob shape:128,128,3,3,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.26", tensor shape 128,128,3,3, blob shape:128,128,3,3,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.43", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "base_level3_tree1_tree1_bn2.1", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input1.1", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "350", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "base_level3_tree1_tree1_relu_output_quant.1TensorQ", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "base_level3_tree1_tree1_relu_output_quant.1", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.28TensorQ", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.28", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.28ChannelQ", tensor shape 128,128,3,3, blob shape:128,128,3,3,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.28", tensor shape 128,128,3,3, blob shape:128,128,3,3,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.45", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.47", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "381", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.30TensorQ", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.30", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.30ChannelQ", tensor shape 128,128,3,3, blob shape:128,128,3,3,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.30", tensor shape 128,128,3,3, blob shape:128,128,3,3,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.49", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "base_level3_tree1_tree2_bn2.1", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input2.1", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "408", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "inputs0.1", tensor shape -1,256,-1,-1, blob shape:1,256,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.32TensorQ", tensor shape -1,256,-1,-1, blob shape:1,256,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.32", tensor shape -1,256,-1,-1, blob shape:1,256,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.32ChannelQ", tensor shape 128,256,1,1, blob shape:128,256,1,1,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.32", tensor shape 128,256,1,1, blob shape:128,256,1,1,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.51", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.53", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "436", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "base_level3_tree1_root_relu_output_quant.1TensorQ", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "base_level3_tree1_root_relu_output_quant.1", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.34TensorQ", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.34", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.34ChannelQ", tensor shape 128,128,3,3, blob shape:128,128,3,3,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.34", tensor shape 128,128,3,3, blob shape:128,128,3,3,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.55", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.57", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "467", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.36TensorQ", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.36", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.36ChannelQ", tensor shape 128,128,3,3, blob shape:128,128,3,3,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.36", tensor shape 128,128,3,3, blob shape:128,128,3,3,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.59", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "base_level3_tree2_tree1_bn2.1", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input3.1", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "494", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "base_level3_tree2_tree1_relu_output_quant.1TensorQ", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "base_level3_tree2_tree1_relu_output_quant.1", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.38TensorQ", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.38", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.38ChannelQ", tensor shape 128,128,3,3, blob shape:128,128,3,3,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.38", tensor shape 128,128,3,3, blob shape:128,128,3,3,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.61", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.63", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "525", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.40TensorQ", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.40", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.40ChannelQ", tensor shape 128,128,3,3, blob shape:128,128,3,3,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.40", tensor shape 128,128,3,3, blob shape:128,128,3,3,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.65", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "base_level3_tree2_tree2_bn2.1", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input4.1", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "552", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "inputs1.1", tensor shape -1,448,-1,-1, blob shape:1,448,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.42TensorQ", tensor shape -1,448,-1,-1, blob shape:1,448,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.42", tensor shape -1,448,-1,-1, blob shape:1,448,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.42ChannelQ", tensor shape 128,448,1,1, blob shape:128,448,1,1,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.42", tensor shape 128,448,1,1, blob shape:128,448,1,1,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.67", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.69", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "580", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "inputs.15", tensor shape -1,128,-1,-1, blob shape:1,128,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "inputs.17", tensor shape -1,128,-1,-1, blob shape:1,128,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.46TensorQ", tensor shape -1,128,-1,-1, blob shape:1,128,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.46", tensor shape -1,128,-1,-1, blob shape:1,128,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.46ChannelQ", tensor shape 256,128,1,1, blob shape:256,128,1,1,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.46", tensor shape 256,128,1,1, blob shape:256,128,1,1,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.73", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "inputs.19", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "base_level4_tree1_project_1_output_quant.1TensorQ", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "base_level4_tree1_project_1_output_quant.1", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.48TensorQ", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.48", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.48ChannelQ", tensor shape 256,128,3,3, blob shape:256,128,3,3,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.48", tensor shape 256,128,3,3, blob shape:256,128,3,3,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.75", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.77", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "647", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.50TensorQ", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.50", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.50ChannelQ", tensor shape 256,256,3,3, blob shape:256,256,3,3,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.50", tensor shape 256,256,3,3, blob shape:256,256,3,3,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.79", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "base_level4_tree1_tree1_bn2.1", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input5.1", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "674", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "base_level4_tree1_tree1_relu_output_quant.1TensorQ", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "base_level4_tree1_tree1_relu_output_quant.1", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.52TensorQ", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.52", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.52ChannelQ", tensor shape 256,256,3,3, blob shape:256,256,3,3,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.52", tensor shape 256,256,3,3, blob shape:256,256,3,3,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.81", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.83", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "705", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.54TensorQ", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.54", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.54ChannelQ", tensor shape 256,256,3,3, blob shape:256,256,3,3,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.54", tensor shape 256,256,3,3, blob shape:256,256,3,3,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.85", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "base_level4_tree1_tree2_bn2.1", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input6.1", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "732", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "inputs2.1", tensor shape -1,512,-1,-1, blob shape:1,512,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.56TensorQ", tensor shape -1,512,-1,-1, blob shape:1,512,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.56", tensor shape -1,512,-1,-1, blob shape:1,512,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.56ChannelQ", tensor shape 256,512,1,1, blob shape:256,512,1,1,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.56", tensor shape 256,512,1,1, blob shape:256,512,1,1,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.87", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.89", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "760", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "base_level4_tree1_root_relu_output_quant.1TensorQ", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "base_level4_tree1_root_relu_output_quant.1", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.58TensorQ", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.58", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.58ChannelQ", tensor shape 256,256,3,3, blob shape:256,256,3,3,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.58", tensor shape 256,256,3,3, blob shape:256,256,3,3,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.91", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.93", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "791", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.60TensorQ", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.60", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.60ChannelQ", tensor shape 256,256,3,3, blob shape:256,256,3,3,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.60", tensor shape 256,256,3,3, blob shape:256,256,3,3,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.95", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "base_level4_tree2_tree1_bn2.1", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input7.1", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "818", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "base_level4_tree2_tree1_relu_output_quant.1TensorQ", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "base_level4_tree2_tree1_relu_output_quant.1", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.62TensorQ", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.62", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.62ChannelQ", tensor shape 256,256,3,3, blob shape:256,256,3,3,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.62", tensor shape 256,256,3,3, blob shape:256,256,3,3,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.97", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.99", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "849", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.64TensorQ", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.64", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.64ChannelQ", tensor shape 256,256,3,3, blob shape:256,256,3,3,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.64", tensor shape 256,256,3,3, blob shape:256,256,3,3,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.101", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "base_level4_tree2_tree2_bn2.1", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input8.1", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "876", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "inputs3.1", tensor shape -1,896,-1,-1, blob shape:1,896,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.66TensorQ", tensor shape -1,896,-1,-1, blob shape:1,896,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.66", tensor shape -1,896,-1,-1, blob shape:1,896,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.66ChannelQ", tensor shape 256,896,1,1, blob shape:256,896,1,1,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.66", tensor shape 256,896,1,1, blob shape:256,896,1,1,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.103", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.105", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "904", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "inputs.21", tensor shape -1,256,-1,-1, blob shape:1,256,24,24,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.68TensorQ", tensor shape -1,256,-1,-1, blob shape:1,256,24,24,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.68", tensor shape -1,256,-1,-1, blob shape:1,256,24,24,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.68ChannelQ", tensor shape 512,256,1,1, blob shape:512,256,1,1,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.68", tensor shape 512,256,1,1, blob shape:512,256,1,1,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.107", tensor shape -1,512,-1,-1, blob shape:1,512,24,24,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "inputs.1", tensor shape -1,512,-1,-1, blob shape:1,512,24,24,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "base_level5_project_1_output_quant.1TensorQ", tensor shape -1,512,-1,-1, blob shape:1,512,24,24,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "base_level5_project_1_output_quant.1", tensor shape -1,512,-1,-1, blob shape:1,512,24,24,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.70TensorQ", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.70", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.70ChannelQ", tensor shape 512,256,3,3, blob shape:512,256,3,3,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.70", tensor shape 512,256,3,3, blob shape:512,256,3,3,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.109", tensor shape -1,512,-1,-1, blob shape:1,512,24,24,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.111", tensor shape -1,512,-1,-1, blob shape:1,512,24,24,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "966", tensor shape -1,512,-1,-1, blob shape:1,512,24,24,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.72TensorQ", tensor shape -1,512,-1,-1, blob shape:1,512,24,24,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.72", tensor shape -1,512,-1,-1, blob shape:1,512,24,24,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.72ChannelQ", tensor shape 512,512,3,3, blob shape:512,512,3,3,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.72", tensor shape 512,512,3,3, blob shape:512,512,3,3,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.113", tensor shape -1,512,-1,-1, blob shape:1,512,24,24,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "base_level5_tree1_bn2.1", tensor shape -1,512,-1,-1, blob shape:1,512,24,24,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input9.1", tensor shape -1,512,-1,-1, blob shape:1,512,24,24,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "993", tensor shape -1,512,-1,-1, blob shape:1,512,24,24,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "base_level5_tree1_relu_output_quant.1TensorQ", tensor shape -1,512,-1,-1, blob shape:1,512,24,24,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "base_level5_tree1_relu_output_quant.1", tensor shape -1,512,-1,-1, blob shape:1,512,24,24,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.74TensorQ", tensor shape -1,512,-1,-1, blob shape:1,512,24,24,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.74", tensor shape -1,512,-1,-1, blob shape:1,512,24,24,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.74ChannelQ", tensor shape 512,512,3,3, blob shape:512,512,3,3,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.74", tensor shape 512,512,3,3, blob shape:512,512,3,3,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.115", tensor shape -1,512,-1,-1, blob shape:1,512,24,24,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.117", tensor shape -1,512,-1,-1, blob shape:1,512,24,24,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "1024", tensor shape -1,512,-1,-1, blob shape:1,512,24,24,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.76TensorQ", tensor shape -1,512,-1,-1, blob shape:1,512,24,24,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.76", tensor shape -1,512,-1,-1, blob shape:1,512,24,24,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.76ChannelQ", tensor shape 512,512,3,3, blob shape:512,512,3,3,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.76", tensor shape 512,512,3,3, blob shape:512,512,3,3,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.119", tensor shape -1,512,-1,-1, blob shape:1,512,24,24,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "base_level5_tree2_bn2.1", tensor shape -1,512,-1,-1, blob shape:1,512,24,24,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input10.1", tensor shape -1,512,-1,-1, blob shape:1,512,24,24,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "1051", tensor shape -1,512,-1,-1, blob shape:1,512,24,24,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "inputs4.1", tensor shape -1,1280,-1,-1, blob shape:1,1280,24,24,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.78TensorQ", tensor shape -1,1280,-1,-1, blob shape:1,1280,24,24,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.78", tensor shape -1,1280,-1,-1, blob shape:1,1280,24,24,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.78ChannelQ", tensor shape 512,1280,1,1, blob shape:512,1280,1,1,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.78", tensor shape 512,1280,1,1, blob shape:512,1280,1,1,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.121", tensor shape -1,512,-1,-1, blob shape:1,512,24,24,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.123", tensor shape -1,512,-1,-1, blob shape:1,512,24,24,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "1079", tensor shape -1,512,-1,-1, blob shape:1,512,24,24,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.80TensorQ", tensor shape -1,512,-1,-1, blob shape:1,512,24,24,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.80", tensor shape -1,512,-1,-1, blob shape:1,512,24,24,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.80ChannelQ", tensor shape 256,512,1,1, blob shape:256,512,1,1,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.80", tensor shape 256,512,1,1, blob shape:256,512,1,1,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.125", tensor shape -1,256,-1,-1, blob shape:1,256,24,24,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.127", tensor shape -1,256,-1,-1, blob shape:1,256,24,24,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "1105", tensor shape -1,256,-1,-1, blob shape:1,256,24,24,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "dla_up_ida_0_up_1.1", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "inputs5.1", tensor shape -1,512,-1,-1, blob shape:1,512,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.82TensorQ", tensor shape -1,512,-1,-1, blob shape:1,512,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.82", tensor shape -1,512,-1,-1, blob shape:1,512,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.82ChannelQ", tensor shape 256,512,3,3, blob shape:256,512,3,3,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.82", tensor shape 256,512,3,3, blob shape:256,512,3,3,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.129", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.131", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "1142", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.84TensorQ", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.84", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.84ChannelQ", tensor shape 128,256,1,1, blob shape:128,256,1,1,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.84", tensor shape 128,256,1,1, blob shape:128,256,1,1,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.133", tensor shape -1,128,-1,-1, blob shape:1,128,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.135", tensor shape -1,128,-1,-1, blob shape:1,128,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "1168", tensor shape -1,128,-1,-1, blob shape:1,128,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "dla_up_ida_1_up_1.1", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.86TensorQ", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.86", tensor shape -1,256,-1,-1, blob shape:1,256,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.86ChannelQ", tensor shape 128,256,1,1, blob shape:128,256,1,1,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.86", tensor shape 128,256,1,1, blob shape:128,256,1,1,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.137", tensor shape -1,128,-1,-1, blob shape:1,128,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.139", tensor shape -1,128,-1,-1, blob shape:1,128,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "1203", tensor shape -1,128,-1,-1, blob shape:1,128,48,48,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "dla_up_ida_1_up_2.1", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "inputs6.1", tensor shape -1,256,-1,-1, blob shape:1,256,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.88TensorQ", tensor shape -1,256,-1,-1, blob shape:1,256,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.88", tensor shape -1,256,-1,-1, blob shape:1,256,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.88ChannelQ", tensor shape 128,256,3,3, blob shape:128,256,3,3,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.88", tensor shape 128,256,3,3, blob shape:128,256,3,3,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.141", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.143", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "1240", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "inputs7.1", tensor shape -1,256,-1,-1, blob shape:1,256,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.90TensorQ", tensor shape -1,256,-1,-1, blob shape:1,256,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.90", tensor shape -1,256,-1,-1, blob shape:1,256,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.90ChannelQ", tensor shape 128,256,3,3, blob shape:128,256,3,3,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.90", tensor shape 128,256,3,3, blob shape:128,256,3,3,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.145", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.147", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "1268", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.92TensorQ", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.92", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.92ChannelQ", tensor shape 64,128,1,1, blob shape:64,128,1,1,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.92", tensor shape 64,128,1,1, blob shape:64,128,1,1,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.149", tensor shape -1,64,-1,-1, blob shape:1,64,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.151", tensor shape -1,64,-1,-1, blob shape:1,64,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "1294", tensor shape -1,64,-1,-1, blob shape:1,64,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "dla_up_ida_2_up_1.1", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.94TensorQ", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.94", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.94ChannelQ", tensor shape 64,128,1,1, blob shape:64,128,1,1,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.94", tensor shape 64,128,1,1, blob shape:64,128,1,1,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.153", tensor shape -1,64,-1,-1, blob shape:1,64,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.155", tensor shape -1,64,-1,-1, blob shape:1,64,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "1329", tensor shape -1,64,-1,-1, blob shape:1,64,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "dla_up_ida_2_up_2.1", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.96TensorQ", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.96", tensor shape -1,128,-1,-1, blob shape:1,128,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.96ChannelQ", tensor shape 64,128,1,1, blob shape:64,128,1,1,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.96", tensor shape 64,128,1,1, blob shape:64,128,1,1,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.157", tensor shape -1,64,-1,-1, blob shape:1,64,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.159", tensor shape -1,64,-1,-1, blob shape:1,64,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "1364", tensor shape -1,64,-1,-1, blob shape:1,64,96,96,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "dla_up_ida_2_up_3.1", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "inputs8.1", tensor shape -1,128,-1,-1, blob shape:1,128,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.98TensorQ", tensor shape -1,128,-1,-1, blob shape:1,128,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.98", tensor shape -1,128,-1,-1, blob shape:1,128,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.98ChannelQ", tensor shape 64,128,3,3, blob shape:64,128,3,3,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.98", tensor shape 64,128,3,3, blob shape:64,128,3,3,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.161", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.163", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "1401", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "inputs9.1", tensor shape -1,128,-1,-1, blob shape:1,128,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.100TensorQ", tensor shape -1,128,-1,-1, blob shape:1,128,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.100", tensor shape -1,128,-1,-1, blob shape:1,128,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.100ChannelQ", tensor shape 64,128,3,3, blob shape:64,128,3,3,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.100", tensor shape 64,128,3,3, blob shape:64,128,3,3,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.165", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.167", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "1429", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "inputs10.1", tensor shape -1,128,-1,-1, blob shape:1,128,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.102TensorQ", tensor shape -1,128,-1,-1, blob shape:1,128,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.102", tensor shape -1,128,-1,-1, blob shape:1,128,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.102ChannelQ", tensor shape 64,128,3,3, blob shape:64,128,3,3,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.102", tensor shape 64,128,3,3, blob shape:64,128,3,3,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.169", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.171", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "1457", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.104TensorQ", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.104", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.104ChannelQ", tensor shape 256,64,3,3, blob shape:256,64,3,3,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.104", tensor shape 256,64,3,3, blob shape:256,64,3,3,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.173", tensor shape -1,256,-1,-1, blob shape:1,256,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "1474", tensor shape -1,256,-1,-1, blob shape:1,256,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.106TensorQ", tensor shape -1,256,-1,-1, blob shape:1,256,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.106", tensor shape -1,256,-1,-1, blob shape:1,256,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.106ChannelQ", tensor shape 128,256,1,1, blob shape:128,256,1,1,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.106", tensor shape 128,256,1,1, blob shape:128,256,1,1,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "hm_2.1", tensor shape -1,128,-1,-1, blob shape:1,128,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input11.1", tensor shape -1,128,-1,-1, blob shape:1,128,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "max_pool2d.1", tensor shape -1,128,-1,-1, blob shape:1,128,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.108TensorQ", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.108", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.108ChannelQ", tensor shape 256,64,3,3, blob shape:256,64,3,3,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.108", tensor shape 256,64,3,3, blob shape:256,64,3,3,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.175", tensor shape -1,256,-1,-1, blob shape:1,256,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "1512", tensor shape -1,256,-1,-1, blob shape:1,256,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.110TensorQ", tensor shape -1,256,-1,-1, blob shape:1,256,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.110", tensor shape -1,256,-1,-1, blob shape:1,256,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.110ChannelQ", tensor shape 2,256,1,1, blob shape:2,256,1,1,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.110", tensor shape 2,256,1,1, blob shape:2,256,1,1,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "wh_2.1", tensor shape -1,2,-1,-1, blob shape:1,2,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.112TensorQ", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.112", tensor shape -1,64,-1,-1, blob shape:1,64,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.112ChannelQ", tensor shape 256,64,3,3, blob shape:256,64,3,3,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.112", tensor shape 256,64,3,3, blob shape:256,64,3,3,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "input.1", tensor shape -1,256,-1,-1, blob shape:1,256,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "1545", tensor shape -1,256,-1,-1, blob shape:1,256,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.1TensorQ", tensor shape -1,256,-1,-1, blob shape:1,256,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_input.1", tensor shape -1,256,-1,-1, blob shape:1,256,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.1ChannelQ", tensor shape 2,256,1,1, blob shape:2,256,1,1,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "quant_weight.1", tensor shape 2,256,1,1, blob shape:2,256,1,1,
VERBOSE: Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "reg_2.1", tensor shape -1,2,-1,-1, blob shape:1,2,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 725] build trt layer for "1563", tensor shape -1,260,-1,-1, blob shape:1,260,192,192,
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 735] shape: -1
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 735] shape: 260
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 735] shape: -1
D/tnn: InitWithoutCache [File /data/TNN-host/source/tnn/network/tensorrt/tensorrt_network.cc][Line 735] shape: -1
PengTRTMode:1
VERBOSE: Applying generic optimizations to the graph for inference.
VERBOSE: Original: 734 layers
VERBOSE: After dead-layer removal: 734 layers
VERBOSE: QDQ graph optimizer - constant folding of Q/DQ initializers
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 55) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 59) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 70) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 81) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 93) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 107) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 118) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 134) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 145) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 158) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 171) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 185) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 196) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 212) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 223) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 236) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 251) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 262) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 278) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 289) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 302) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 315) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 329) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 340) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 356) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 367) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 380) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 395) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 406) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 422) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 433) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 446) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 458) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 472) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 483) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 499) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 510) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 523) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 534) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 547) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 558) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 570) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 583) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 595) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 606) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 618) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 630) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 643) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 655) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 667) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 678) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 688) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 699) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 709) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 718) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 728) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 57) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 61) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 72) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 83) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 95) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 109) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 120) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 136) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 147) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 160) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 173) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 187) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 198) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 214) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 225) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 238) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 253) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 264) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 280) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 291) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 304) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 317) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 331) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 342) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 358) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 369) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 382) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 397) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 408) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 424) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 435) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 448) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 460) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 474) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 485) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 501) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 512) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 525) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 536) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 549) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 560) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 572) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 585) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 597) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 608) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 620) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 632) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 645) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 657) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 669) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 680) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 690) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 701) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 711) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 720) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 730) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 66) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 68) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 77) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 79) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 103) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 89) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 105) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 91) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 99) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 114) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 101) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 116) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 126) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 130) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 128) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 132) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 141) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 143) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 154) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 156) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 181) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 167) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 183) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 169) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 177) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 192) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 179) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 194) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 204) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 208) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 206) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 210) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 219) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 221) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 232) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 234) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 243) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 247) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 245) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 249) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 258) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 260) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 270) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 274) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 272) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 276) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 285) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 287) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 298) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 300) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 325) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 602) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 311) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 327) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 604) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 313) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 321) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 336) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 323) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 338) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 639) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 641) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 348) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 352) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 350) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 354) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 363) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 365) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 376) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 378) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 387) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 391) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 389) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 393) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 402) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 404) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 414) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 418) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 416) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 420) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 429) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 431) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 442) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 444) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 468) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 554) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 454) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 470) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 556) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 456) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 464) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 479) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 466) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 481) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 579) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 581) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 491) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 495) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 493) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 497) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 614) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 616) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 506) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 508) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 651) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 653) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 519) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 521) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 530) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 532) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 543) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 545) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 566) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 568) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 591) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 593) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 626) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 628) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 663) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 665) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 674) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 695) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 714) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 676) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 697) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 716) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 684) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 705) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 724) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 686) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 707) [Constant]
VERBOSE: Running: ConstQDQInitializersFusion
VERBOSE: Removing (Unnamed Layer* 726) [Constant]
VERBOSE: After Myelin optimization: 490 layers
VERBOSE: QDQ graph optimizer - constant folding of Q/DQ initializers
VERBOSE: QDQ graph optimizer forward pass - DQ motions and fusions
VERBOSE: QDQ graph optimizer backward pass
VERBOSE: QDQ graph optimizer quantization pass - Generate quantized ops
VERBOSE: Running: EltReluFusion
VERBOSE: EltReluFusion: Fusing input.3 with 170
VERBOSE: Running: EltReluFusion
VERBOSE: EltReluFusion: Fusing input0.1 with 228
VERBOSE: Running: EltReluFusion
VERBOSE: EltReluFusion: Fusing input1.1 with 350
VERBOSE: Running: EltReluFusion
VERBOSE: EltReluFusion: Fusing input2.1 with 408
VERBOSE: Running: EltReluFusion
VERBOSE: EltReluFusion: Fusing input3.1 with 494
VERBOSE: Running: EltReluFusion
VERBOSE: EltReluFusion: Fusing input4.1 with 552
VERBOSE: Running: EltReluFusion
VERBOSE: EltReluFusion: Fusing input5.1 with 674
VERBOSE: Running: EltReluFusion
VERBOSE: EltReluFusion: Fusing input6.1 with 732
VERBOSE: Running: EltReluFusion
VERBOSE: EltReluFusion: Fusing input7.1 with 818
VERBOSE: Running: EltReluFusion
VERBOSE: EltReluFusion: Fusing input8.1 with 876
VERBOSE: Running: EltReluFusion
VERBOSE: EltReluFusion: Fusing input9.1 with 993
VERBOSE: Running: EltReluFusion
VERBOSE: EltReluFusion: Fusing input10.1 with 1051
VERBOSE: Running: ScaleActivationFusion
VERBOSE: ScaleActivationFusion: Fusing input.7 with 27
VERBOSE: Running: ScaleActivationFusion
VERBOSE: ScaleActivationFusion: Fusing input.11 with 53
VERBOSE: Running: ScaleActivationFusion
VERBOSE: ScaleActivationFusion: Fusing input.15 with 80
VERBOSE: Running: ScaleActivationFusion
VERBOSE: ScaleActivationFusion: Fusing input.21 with 142
VERBOSE: Running: ScaleActivationFusion
VERBOSE: ScaleActivationFusion: Fusing input.27 with 201
VERBOSE: Running: ScaleActivationFusion
VERBOSE: ScaleActivationFusion: Fusing input.33 with 256
VERBOSE: Running: ScaleActivationFusion
VERBOSE: ScaleActivationFusion: Fusing input.41 with 323
VERBOSE: Running: ScaleActivationFusion
VERBOSE: ScaleActivationFusion: Fusing input.47 with 381
VERBOSE: Running: ScaleActivationFusion
VERBOSE: ScaleActivationFusion: Fusing input.53 with 436
VERBOSE: Running: ScaleActivationFusion
VERBOSE: ScaleActivationFusion: Fusing input.57 with 467
VERBOSE: Running: ScaleActivationFusion
VERBOSE: ScaleActivationFusion: Fusing input.63 with 525
VERBOSE: Running: ScaleActivationFusion
VERBOSE: ScaleActivationFusion: Fusing input.69 with 580
VERBOSE: Running: ScaleActivationFusion
VERBOSE: ScaleActivationFusion: Fusing input.77 with 647
VERBOSE: Running: ScaleActivationFusion
VERBOSE: ScaleActivationFusion: Fusing input.151 with 1294
VERBOSE: Running: ScaleActivationFusion
VERBOSE: ScaleActivationFusion: Fusing input.163 with 1401
VERBOSE: Running: ScaleActivationFusion
VERBOSE: ScaleActivationFusion: Fusing input.83 with 705
VERBOSE: Running: ScaleActivationFusion
VERBOSE: ScaleActivationFusion: Fusing input.89 with 760
VERBOSE: Running: ScaleActivationFusion
VERBOSE: ScaleActivationFusion: Fusing input.93 with 791
VERBOSE: Running: ScaleActivationFusion
VERBOSE: ScaleActivationFusion: Fusing input.99 with 849
VERBOSE: Running: ScaleActivationFusion
VERBOSE: ScaleActivationFusion: Fusing input.105 with 904
VERBOSE: Running: ScaleActivationFusion
VERBOSE: ScaleActivationFusion: Fusing input.111 with 966
VERBOSE: Running: ScaleActivationFusion
VERBOSE: ScaleActivationFusion: Fusing input.135 with 1168
VERBOSE: Running: ScaleActivationFusion
VERBOSE: ScaleActivationFusion: Fusing input.143 with 1240
VERBOSE: Running: ScaleActivationFusion
VERBOSE: ScaleActivationFusion: Fusing input.117 with 1024
VERBOSE: Running: ScaleActivationFusion
VERBOSE: ScaleActivationFusion: Fusing input.155 with 1329
VERBOSE: Running: ScaleActivationFusion
VERBOSE: ScaleActivationFusion: Fusing input.167 with 1429
VERBOSE: Running: ScaleActivationFusion
VERBOSE: ScaleActivationFusion: Fusing input.123 with 1079
VERBOSE: Running: ScaleActivationFusion
VERBOSE: ScaleActivationFusion: Fusing input.127 with 1105
VERBOSE: Running: ScaleActivationFusion
VERBOSE: ScaleActivationFusion: Fusing input.131 with 1142
VERBOSE: Running: ScaleActivationFusion
VERBOSE: ScaleActivationFusion: Fusing input.139 with 1203
VERBOSE: Running: ScaleActivationFusion
VERBOSE: ScaleActivationFusion: Fusing input.147 with 1268
VERBOSE: Running: ScaleActivationFusion
VERBOSE: ScaleActivationFusion: Fusing input.159 with 1364
VERBOSE: Running: ScaleActivationFusion
VERBOSE: ScaleActivationFusion: Fusing input.171 with 1457
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.base.base_layer.0.weight with quant_weight.2ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.base.level0.0.weight with quant_weight.4ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.base.level1.0.weight with quant_weight.6ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.base.level2.project.0.weight with quant_weight.8ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.base.level2.root.conv.weight with quant_weight.18ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.base.level2.tree1.conv1.weight with quant_weight.10ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.base.level2.tree1.conv2.weight with quant_weight.12ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.base.level2.tree2.conv1.weight with quant_weight.14ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.base.level2.tree2.conv2.weight with quant_weight.16ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.base.level3.tree1.project.0.weight with quant_weight.22ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.base.level3.tree1.root.conv.weight with quant_weight.32ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.base.level3.tree1.tree1.conv1.weight with quant_weight.24ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.base.level3.tree1.tree1.conv2.weight with quant_weight.26ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.base.level3.tree1.tree2.conv1.weight with quant_weight.28ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.base.level3.tree1.tree2.conv2.weight with quant_weight.30ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.base.level3.tree2.root.conv.weight with quant_weight.42ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.base.level3.tree2.tree1.conv1.weight with quant_weight.34ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.base.level3.tree2.tree1.conv2.weight with quant_weight.36ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.base.level3.tree2.tree2.conv1.weight with quant_weight.38ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.base.level3.tree2.tree2.conv2.weight with quant_weight.40ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.base.level4.tree1.project.0.weight with quant_weight.46ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.base.level4.tree1.root.conv.weight with quant_weight.56ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.base.level4.tree1.tree1.conv1.weight with quant_weight.48ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.base.level4.tree1.tree1.conv2.weight with quant_weight.50ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.base.level4.tree1.tree2.conv1.weight with quant_weight.52ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.base.level4.tree1.tree2.conv2.weight with quant_weight.54ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.base.level4.tree2.root.conv.weight with quant_weight.66ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.base.level4.tree2.tree1.conv1.weight with quant_weight.58ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.base.level4.tree2.tree1.conv2.weight with quant_weight.60ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.base.level4.tree2.tree2.conv1.weight with quant_weight.62ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.base.level4.tree2.tree2.conv2.weight with quant_weight.64ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.base.level5.project.0.weight with quant_weight.68ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.base.level5.root.conv.weight with quant_weight.78ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.base.level5.tree1.conv1.weight with quant_weight.70ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.base.level5.tree1.conv2.weight with quant_weight.72ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.base.level5.tree2.conv1.weight with quant_weight.74ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.base.level5.tree2.conv2.weight with quant_weight.76ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.dla_up.ida_0.node_1.0.weight with quant_weight.82ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.dla_up.ida_0.proj_1.0.weight with quant_weight.80ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.dla_up.ida_1.node_1.0.weight with quant_weight.88ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.dla_up.ida_1.node_2.0.weight with quant_weight.90ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.dla_up.ida_1.proj_1.0.weight with quant_weight.84ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.dla_up.ida_1.proj_2.0.weight with quant_weight.86ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.dla_up.ida_2.node_1.0.weight with quant_weight.98ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.dla_up.ida_2.node_2.0.weight with quant_weight.100ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.dla_up.ida_2.node_3.0.weight with quant_weight.102ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.dla_up.ida_2.proj_1.0.weight with quant_weight.92ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.dla_up.ida_2.proj_2.0.weight with quant_weight.94ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.dla_up.ida_2.proj_3.0.weight with quant_weight.96ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.hm.0.weight with quant_weight.104ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.hm.2.weight with quant_weight.106ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.reg.0.weight with quant_weight.112ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.reg.2.weight with quant_weight.1ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.wh.0.weight with quant_weight.108ChannelQ
VERBOSE: Running: ConstWeightsQuantizeFusion
VERBOSE: ConstWeightsQuantizeFusion: Fusing self.wh.2.weight with quant_weight.110ChannelQ
VERBOSE: Running: ConvReluFusion
VERBOSE: ConvReluFusion: Fusing input.173 with 1474
VERBOSE: Running: ConvReluFusion
VERBOSE: ConvReluFusion: Fusing input.175 with 1512
VERBOSE: Running: ConvReluFusion
VERBOSE: ConvReluFusion: Fusing input.1 with 1545
VERBOSE: Running: VanillaSwapWithFollowingQ
VERBOSE: Swapping inputs.5 with quant_input.8TensorQ
VERBOSE: Running: VanillaSwapWithFollowingQ
VERBOSE: Swapping inputs.11 with quant_input.22TensorQ
VERBOSE: Running: VanillaSwapWithFollowingQ
VERBOSE: Swapping inputs.17 with quant_input.46TensorQ
VERBOSE: Running: HorizontalMergeQNodes
VERBOSE: Eliminating quant_input.10TensorQ which duplicates (Q) quant_input.8TensorQ
VERBOSE: Removing quant_input.10TensorQ
VERBOSE: Running: HorizontalMergeQNodes
VERBOSE: Eliminating quant_input.14TensorQ which duplicates (Q) base_level2_tree1_relu_output_quant.1TensorQ
VERBOSE: Removing quant_input.14TensorQ
VERBOSE: Running: HorizontalMergeQNodes
VERBOSE: Eliminating quant_input.24TensorQ which duplicates (Q) quant_input.22TensorQ
VERBOSE: Removing quant_input.24TensorQ
VERBOSE: Running: HorizontalMergeQNodes
VERBOSE: Eliminating quant_input.28TensorQ which duplicates (Q) base_level3_tree1_tree1_relu_output_quant.1TensorQ
VERBOSE: Removing quant_input.28TensorQ
VERBOSE: Running: HorizontalMergeQNodes
VERBOSE: Eliminating quant_input.34TensorQ which duplicates (Q) base_level3_tree1_root_relu_output_quant.1TensorQ
VERBOSE: Removing quant_input.34TensorQ
VERBOSE: Running: HorizontalMergeQNodes
VERBOSE: Eliminating quant_input.38TensorQ which duplicates (Q) base_level3_tree2_tree1_relu_output_quant.1TensorQ
VERBOSE: Removing quant_input.38TensorQ
VERBOSE: Running: HorizontalMergeQNodes
VERBOSE: Eliminating quant_input.48TensorQ which duplicates (Q) quant_input.46TensorQ
VERBOSE: Eliminating quant_input.92TensorQ which duplicates (Q) quant_input.46TensorQ
VERBOSE: Removing quant_input.48TensorQ
VERBOSE: Removing quant_input.92TensorQ
VERBOSE: Running: HorizontalMergeQNodes
VERBOSE: Eliminating quant_input.52TensorQ which duplicates (Q) base_level4_tree1_tree1_relu_output_quant.1TensorQ
VERBOSE: Removing quant_input.52TensorQ
VERBOSE: Running: HorizontalMergeQNodes
VERBOSE: Eliminating quant_input.58TensorQ which duplicates (Q) base_level4_tree1_root_relu_output_quant.1TensorQ
VERBOSE: Removing quant_input.58TensorQ
VERBOSE: Running: HorizontalMergeQNodes
VERBOSE: Eliminating quant_input.62TensorQ which duplicates (Q) base_level4_tree2_tree1_relu_output_quant.1TensorQ
VERBOSE: Removing quant_input.62TensorQ
VERBOSE: Running: HorizontalMergeQNodes
VERBOSE: Eliminating quant_input.84TensorQ which duplicates (Q) quant_input.70TensorQ
VERBOSE: Removing quant_input.84TensorQ
VERBOSE: Running: HorizontalMergeQNodes
VERBOSE: Eliminating quant_input.74TensorQ which duplicates (Q) base_level5_tree1_relu_output_quant.1TensorQ
VERBOSE: Removing quant_input.74TensorQ
VERBOSE: Running: HorizontalMergeQNodes
VERBOSE: Eliminating quant_input.108TensorQ which duplicates (Q) quant_input.104TensorQ
VERBOSE: Eliminating quant_input.112TensorQ which duplicates (Q) quant_input.104TensorQ
VERBOSE: Removing quant_input.108TensorQ
VERBOSE: Removing quant_input.112TensorQ
VERBOSE: Running: SplitQAcrossPrecedingFanIn
VERBOSE: Running: SplitQAcrossPrecedingFanIn
VERBOSE: Running: SplitQAcrossPrecedingFanIn
VERBOSE: Running: SplitQAcrossPrecedingFanIn
VERBOSE: Running: SplitQAcrossPrecedingFanIn
VERBOSE: Running: SplitQAcrossPrecedingFanIn
VERBOSE: Running: SplitQAcrossPrecedingFanIn
VERBOSE: Running: SplitQAcrossPrecedingFanIn
VERBOSE: Running: SplitQAcrossPrecedingFanIn
VERBOSE: Running: SplitQAcrossPrecedingFanIn
VERBOSE: Running: SplitQAcrossPrecedingFanIn
VERBOSE: Running: SplitQAcrossPrecedingFanIn
VERBOSE: Running: VanillaSwapWithFollowingQ
VERBOSE: Swapping inputs.9 with quant_input.42TensorQ_clone_2
VERBOSE: Running: VanillaSwapWithFollowingQ
VERBOSE: Swapping inputs.15 with quant_input.66TensorQ_clone_2
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing input.7 + 27
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing input.11 + 53
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing input.15 + 80
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing input.21 + 142
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing inputs.7
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing base_level2_tree1_bn2.1
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing input.27 + 201
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing base_level2_tree2_bn2.1
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing input.33 + 256
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing input.41 + 323
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing inputs.13
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing base_level3_tree1_tree1_bn2.1
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing input.47 + 381
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing base_level3_tree1_tree2_bn2.1
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing input.53 + 436
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing input.57 + 467
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing base_level3_tree2_tree1_bn2.1
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing input.63 + 525
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing base_level3_tree2_tree2_bn2.1
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing input.69 + 580
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing input.77 + 647
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing input.151 + 1294
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing inputs.19
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing base_level4_tree1_tree1_bn2.1
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing input.163 + 1401
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing input.83 + 705
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing base_level4_tree1_tree2_bn2.1
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing input.89 + 760
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing input.93 + 791
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing base_level4_tree2_tree1_bn2.1
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing input.99 + 849
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing base_level4_tree2_tree2_bn2.1
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing input.105 + 904
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing input.111 + 966
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing input.135 + 1168
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing inputs.1
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing base_level5_tree1_bn2.1
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing input.143 + 1240
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing input.117 + 1024
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing input.155 + 1329
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing base_level5_tree2_bn2.1
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing input.167 + 1429
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing input.123 + 1079
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing input.127 + 1105
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing input.131 + 1142
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing input.139 + 1203
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing input.147 + 1268
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing input.159 + 1364
VERBOSE: Running: QConvScaleFusion
VERBOSE: Removing input.171 + 1457
VERBOSE: Running: QuantizeConvWithResidualAdd
VERBOSE: Swapping input0.1 + 228 with quant_input.18TensorQ_clone_0
VERBOSE: QuantizeDoubleInputNodes: fusing quant_input.18TensorQ_clone_0 into input.29
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.16 and quant_weight.16) into input.29
VERBOSE: Removing quant_input.18TensorQ_clone_0
VERBOSE: Removing quant_input.16
VERBOSE: Removing quant_weight.16
VERBOSE: ConstWeightsFusion: Fusing self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ with input.29
VERBOSE: ConvEltwiseSumFusion: Fusing self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 with input0.1 + 228
VERBOSE: Removing base_level2_tree1_relu_output_quant.1
VERBOSE: Running: QuantizeConvWithResidualAdd
VERBOSE: Swapping input2.1 + 408 with quant_input.32TensorQ_clone_0
VERBOSE: QuantizeDoubleInputNodes: fusing quant_input.32TensorQ_clone_0 into input.49
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.30 and quant_weight.30) into input.49
VERBOSE: Removing quant_input.32TensorQ_clone_0
VERBOSE: Removing quant_input.30
VERBOSE: Removing quant_weight.30
VERBOSE: ConstWeightsFusion: Fusing self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ with input.49
VERBOSE: ConvEltwiseSumFusion: Fusing self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 with input2.1 + 408
VERBOSE: Removing base_level3_tree1_tree1_relu_output_quant.1
VERBOSE: Running: QuantizeConvWithResidualAdd
VERBOSE: Swapping input4.1 + 552 with quant_input.42TensorQ_clone_0
VERBOSE: QuantizeDoubleInputNodes: fusing quant_input.42TensorQ_clone_0 into input.65
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.40 and quant_weight.40) into input.65
VERBOSE: Removing quant_input.42TensorQ_clone_0
VERBOSE: Removing quant_input.40
VERBOSE: Removing quant_weight.40
VERBOSE: ConstWeightsFusion: Fusing self.base.level3.tree2.tree2.conv2.weight + quant_weight.40ChannelQ with input.65
VERBOSE: ConvEltwiseSumFusion: Fusing self.base.level3.tree2.tree2.conv2.weight + quant_weight.40ChannelQ + input.65 with input4.1 + 552
VERBOSE: Removing base_level3_tree2_tree1_relu_output_quant.1
VERBOSE: Running: QuantizeConvWithResidualAdd
VERBOSE: Swapping input6.1 + 732 with quant_input.56TensorQ_clone_0
VERBOSE: QuantizeDoubleInputNodes: fusing quant_input.56TensorQ_clone_0 into input.85
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.54 and quant_weight.54) into input.85
VERBOSE: Removing quant_input.56TensorQ_clone_0
VERBOSE: Removing quant_input.54
VERBOSE: Removing quant_weight.54
VERBOSE: ConstWeightsFusion: Fusing self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ with input.85
VERBOSE: ConvEltwiseSumFusion: Fusing self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 with input6.1 + 732
VERBOSE: Removing base_level4_tree1_tree1_relu_output_quant.1
VERBOSE: Running: QuantizeConvWithResidualAdd
VERBOSE: Swapping input8.1 + 876 with quant_input.66TensorQ_clone_0
VERBOSE: QuantizeDoubleInputNodes: fusing quant_input.66TensorQ_clone_0 into input.101
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.64 and quant_weight.64) into input.101
VERBOSE: Removing quant_input.66TensorQ_clone_0
VERBOSE: Removing quant_input.64
VERBOSE: Removing quant_weight.64
VERBOSE: ConstWeightsFusion: Fusing self.base.level4.tree2.tree2.conv2.weight + quant_weight.64ChannelQ with input.101
VERBOSE: ConvEltwiseSumFusion: Fusing self.base.level4.tree2.tree2.conv2.weight + quant_weight.64ChannelQ + input.101 with input8.1 + 876
VERBOSE: Removing base_level4_tree2_tree1_relu_output_quant.1
VERBOSE: Running: QuantizeConvWithResidualAdd
VERBOSE: Swapping input10.1 + 1051 with quant_input.78TensorQ_clone_0
VERBOSE: QuantizeDoubleInputNodes: fusing quant_input.78TensorQ_clone_0 into input.119
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.76 and quant_weight.76) into input.119
VERBOSE: Removing quant_input.78TensorQ_clone_0
VERBOSE: Removing quant_input.76
VERBOSE: Removing quant_weight.76
VERBOSE: ConstWeightsFusion: Fusing self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ with input.119
VERBOSE: ConvEltwiseSumFusion: Fusing self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 with input10.1 + 1051
VERBOSE: Removing base_level5_tree1_relu_output_quant.1
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing quant_input.4TensorQ into input.5
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.2 and quant_weight.2) into input.5
VERBOSE: Removing quant_input.4TensorQ
VERBOSE: Removing quant_input.2
VERBOSE: Removing quant_weight.2
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing quant_input.6TensorQ into input.9
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.4 and quant_weight.4) into input.9
VERBOSE: Removing quant_input.6TensorQ
VERBOSE: Removing quant_input.4
VERBOSE: Removing quant_weight.4
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing quant_input.8TensorQ into input.13
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.6 and quant_weight.6) into input.13
VERBOSE: Removing quant_input.8TensorQ
VERBOSE: Removing quant_input.6
VERBOSE: Removing quant_weight.6
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing quant_input.12TensorQ into input.19
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.10 and quant_weight.10) into input.19
VERBOSE: Removing quant_input.12TensorQ
VERBOSE: Removing quant_input.10
VERBOSE: Removing quant_weight.10
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.12 and quant_weight.12) into input.23
VERBOSE: Removing quant_input.12
VERBOSE: Removing quant_weight.12
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing quant_input.16TensorQ into input.25
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.14 and quant_weight.14) into input.25
VERBOSE: Removing quant_input.16TensorQ
VERBOSE: Removing quant_input.14
VERBOSE: Removing quant_weight.14
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.18 and quant_weight.18) into input.31
VERBOSE: Removing quant_input.18
VERBOSE: Removing quant_weight.18
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing quant_input.26TensorQ into input.39
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.24 and quant_weight.24) into input.39
VERBOSE: Removing quant_input.26TensorQ
VERBOSE: Removing quant_input.24
VERBOSE: Removing quant_weight.24
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.26 and quant_weight.26) into input.43
VERBOSE: Removing quant_input.26
VERBOSE: Removing quant_weight.26
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing quant_input.30TensorQ into input.45
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.28 and quant_weight.28) into input.45
VERBOSE: Removing quant_input.30TensorQ
VERBOSE: Removing quant_input.28
VERBOSE: Removing quant_weight.28
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.32 and quant_weight.32) into input.51
VERBOSE: Removing quant_input.32
VERBOSE: Removing quant_weight.32
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing quant_input.36TensorQ into input.55
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.34 and quant_weight.34) into input.55
VERBOSE: Removing quant_input.36TensorQ
VERBOSE: Removing quant_input.34
VERBOSE: Removing quant_weight.34
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.36 and quant_weight.36) into input.59
VERBOSE: Removing quant_input.36
VERBOSE: Removing quant_weight.36
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing quant_input.40TensorQ into input.61
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.38 and quant_weight.38) into input.61
VERBOSE: Removing quant_input.40TensorQ
VERBOSE: Removing quant_input.38
VERBOSE: Removing quant_weight.38
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.42 and quant_weight.42) into input.67
VERBOSE: Removing quant_input.42
VERBOSE: Removing quant_weight.42
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing quant_input.50TensorQ into input.75
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.48 and quant_weight.48) into input.75
VERBOSE: Removing quant_input.50TensorQ
VERBOSE: Removing quant_input.48
VERBOSE: Removing quant_weight.48
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing base_level4_tree1_project_1_output_quant.1TensorQ into input.73
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.46 and quant_weight.46) into input.73
VERBOSE: Removing base_level4_tree1_project_1_output_quant.1TensorQ
VERBOSE: Removing quant_input.46
VERBOSE: Removing quant_weight.46
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.50 and quant_weight.50) into input.79
VERBOSE: Removing quant_input.50
VERBOSE: Removing quant_weight.50
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing quant_input.100TensorQ_clone_0 into input.161
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.98 and quant_weight.98) into input.161
VERBOSE: Removing quant_input.100TensorQ_clone_0
VERBOSE: Removing quant_input.98
VERBOSE: Removing quant_weight.98
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing quant_input.54TensorQ into input.81
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.52 and quant_weight.52) into input.81
VERBOSE: Removing quant_input.54TensorQ
VERBOSE: Removing quant_input.52
VERBOSE: Removing quant_weight.52
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.56 and quant_weight.56) into input.87
VERBOSE: Removing quant_input.56
VERBOSE: Removing quant_weight.56
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing quant_input.60TensorQ into input.91
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.58 and quant_weight.58) into input.91
VERBOSE: Removing quant_input.60TensorQ
VERBOSE: Removing quant_input.58
VERBOSE: Removing quant_weight.58
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.60 and quant_weight.60) into input.95
VERBOSE: Removing quant_input.60
VERBOSE: Removing quant_weight.60
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing quant_input.64TensorQ into input.97
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.62 and quant_weight.62) into input.97
VERBOSE: Removing quant_input.64TensorQ
VERBOSE: Removing quant_input.62
VERBOSE: Removing quant_weight.62
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.66 and quant_weight.66) into input.103
VERBOSE: Removing quant_input.66
VERBOSE: Removing quant_weight.66
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing quant_input.72TensorQ into input.109
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.70 and quant_weight.70) into input.109
VERBOSE: Removing quant_input.72TensorQ
VERBOSE: Removing quant_input.70
VERBOSE: Removing quant_weight.70
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing base_level5_project_1_output_quant.1TensorQ into input.107
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.68 and quant_weight.68) into input.107
VERBOSE: Removing base_level5_project_1_output_quant.1TensorQ
VERBOSE: Removing quant_input.68
VERBOSE: Removing quant_weight.68
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.72 and quant_weight.72) into input.113
VERBOSE: Removing quant_input.72
VERBOSE: Removing quant_weight.72
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.88 and quant_weight.88) into input.141
VERBOSE: Removing quant_input.88
VERBOSE: Removing quant_weight.88
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing quant_input.76TensorQ into input.115
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.74 and quant_weight.74) into input.115
VERBOSE: Removing quant_input.76TensorQ
VERBOSE: Removing quant_input.74
VERBOSE: Removing quant_weight.74
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.94 and quant_weight.94) into input.153
VERBOSE: Removing quant_input.94
VERBOSE: Removing quant_weight.94
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing quant_input.102TensorQ_clone_0 into input.165
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.100 and quant_weight.100) into input.165
VERBOSE: Removing quant_input.102TensorQ_clone_0
VERBOSE: Removing quant_input.100
VERBOSE: Removing quant_weight.100
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing quant_input.80TensorQ into input.121
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.78 and quant_weight.78) into input.121
VERBOSE: Removing quant_input.80TensorQ
VERBOSE: Removing quant_input.78
VERBOSE: Removing quant_weight.78
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.80 and quant_weight.80) into input.125
VERBOSE: Removing quant_input.80
VERBOSE: Removing quant_weight.80
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing quant_input.86TensorQ into input.129
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.82 and quant_weight.82) into input.129
VERBOSE: Removing quant_input.86TensorQ
VERBOSE: Removing quant_input.82
VERBOSE: Removing quant_weight.82
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.86 and quant_weight.86) into input.137
VERBOSE: Removing quant_input.86
VERBOSE: Removing quant_weight.86
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing quant_input.96TensorQ into input.145
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.90 and quant_weight.90) into input.145
VERBOSE: Removing quant_input.96TensorQ
VERBOSE: Removing quant_input.90
VERBOSE: Removing quant_weight.90
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.96 and quant_weight.96) into input.157
VERBOSE: Removing quant_input.96
VERBOSE: Removing quant_weight.96
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing quant_input.104TensorQ into input.169
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.102 and quant_weight.102) into input.169
VERBOSE: Removing quant_input.104TensorQ
VERBOSE: Removing quant_input.102
VERBOSE: Removing quant_weight.102
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing quant_input.106TensorQ into input.173 + 1474
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.104 and quant_weight.104) into input.173 + 1474
VERBOSE: Removing quant_input.106TensorQ
VERBOSE: Removing quant_input.104
VERBOSE: Removing quant_weight.104
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing quant_input.1TensorQ into input.1 + 1545
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.112 and quant_weight.112) into input.1 + 1545
VERBOSE: Removing quant_input.1TensorQ
VERBOSE: Removing quant_input.112
VERBOSE: Removing quant_weight.112
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.106 and quant_weight.106) into hm_2.1
VERBOSE: Removing quant_input.106
VERBOSE: Removing quant_weight.106
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.1 and quant_weight.1) into reg_2.1
VERBOSE: Removing quant_input.1
VERBOSE: Removing quant_weight.1
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing base_level2_project_1_output_quant.1TensorQ into input.17
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.8 and quant_weight.8) into input.17
VERBOSE: Removing base_level2_project_1_output_quant.1TensorQ
VERBOSE: Removing quant_input.8
VERBOSE: Removing quant_weight.8
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing base_level3_tree1_project_1_output_quant.1TensorQ into input.37
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.22 and quant_weight.22) into input.37
VERBOSE: Removing base_level3_tree1_project_1_output_quant.1TensorQ
VERBOSE: Removing quant_input.22
VERBOSE: Removing quant_weight.22
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.92 and quant_weight.92) into input.149
VERBOSE: Removing quant_input.92
VERBOSE: Removing quant_weight.92
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.84 and quant_weight.84) into input.133
VERBOSE: Removing quant_input.84
VERBOSE: Removing quant_weight.84
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing quant_input.110TensorQ into input.175 + 1512
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.108 and quant_weight.108) into input.175 + 1512
VERBOSE: Removing quant_input.110TensorQ
VERBOSE: Removing quant_input.108
VERBOSE: Removing quant_weight.108
VERBOSE: Running: QuantizeDoubleInputNodes
VERBOSE: QuantizeDoubleInputNodes: fusing (quant_input.110 and quant_weight.110) into wh_2.1
VERBOSE: Removing quant_input.110
VERBOSE: Removing quant_weight.110
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.base.base_layer.0.weight + quant_weight.2ChannelQ with input.5
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.base.level0.0.weight + quant_weight.4ChannelQ with input.9
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.base.level1.0.weight + quant_weight.6ChannelQ with input.13
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.base.level2.project.0.weight + quant_weight.8ChannelQ with input.17
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.base.level2.root.conv.weight + quant_weight.18ChannelQ with input.31
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ with input.19
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ with input.23
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ with input.25
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ with input.37
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ with input.51
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ with input.39
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ with input.43
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ with input.45
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ with input.67
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.base.level3.tree2.tree1.conv1.weight + quant_weight.34ChannelQ with input.55
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.base.level3.tree2.tree1.conv2.weight + quant_weight.36ChannelQ with input.59
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.base.level3.tree2.tree2.conv1.weight + quant_weight.38ChannelQ with input.61
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ with input.73
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ with input.87
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ with input.75
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ with input.79
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ with input.81
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ with input.103
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.base.level4.tree2.tree1.conv1.weight + quant_weight.58ChannelQ with input.91
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.base.level4.tree2.tree1.conv2.weight + quant_weight.60ChannelQ with input.95
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.base.level4.tree2.tree2.conv1.weight + quant_weight.62ChannelQ with input.97
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.base.level5.project.0.weight + quant_weight.68ChannelQ with input.107
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.base.level5.root.conv.weight + quant_weight.78ChannelQ with input.121
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ with input.109
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ with input.113
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ with input.115
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ with input.129
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ with input.125
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ with input.141
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ with input.145
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ with input.133
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.dla_up.ida_1.proj_2.0.weight + quant_weight.86ChannelQ with input.137
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ with input.161
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.dla_up.ida_2.node_2.0.weight + quant_weight.100ChannelQ with input.165
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ with input.169
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ with input.149
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.dla_up.ida_2.proj_2.0.weight + quant_weight.94ChannelQ with input.153
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.dla_up.ida_2.proj_3.0.weight + quant_weight.96ChannelQ with input.157
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.hm.0.weight + quant_weight.104ChannelQ with input.173 + 1474
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.hm.2.weight + quant_weight.106ChannelQ with hm_2.1
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.reg.0.weight + quant_weight.112ChannelQ with input.1 + 1545
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.reg.2.weight + quant_weight.1ChannelQ with reg_2.1
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.wh.0.weight + quant_weight.108ChannelQ with input.175 + 1512
VERBOSE: Running: ConstWeightsFusion
VERBOSE: ConstWeightsFusion: Fusing self.wh.2.weight + quant_weight.110ChannelQ with wh_2.1
VERBOSE: Running: ActivationToPointwiseConversion
VERBOSE: Swap the layer type of input11.1 from ACTIVATION to POINTWISE
VERBOSE: Running: ConvEltwiseSumFusion
VERBOSE: ConvEltwiseSumFusion: Fusing self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 with input.3 + 170
VERBOSE: Running: ConvEltwiseSumFusion
VERBOSE: ConvEltwiseSumFusion: Fusing self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 with input1.1 + 350
VERBOSE: Running: ConvEltwiseSumFusion
VERBOSE: ConvEltwiseSumFusion: Fusing self.base.level3.tree2.tree1.conv2.weight + quant_weight.36ChannelQ + input.59 with input3.1 + 494
VERBOSE: Running: ConvEltwiseSumFusion
VERBOSE: ConvEltwiseSumFusion: Fusing self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 with input5.1 + 674
VERBOSE: Running: ConvEltwiseSumFusion
VERBOSE: ConvEltwiseSumFusion: Fusing self.base.level4.tree2.tree1.conv2.weight + quant_weight.60ChannelQ + input.95 with input7.1 + 818
VERBOSE: Running: ConvEltwiseSumFusion
VERBOSE: ConvEltwiseSumFusion: Fusing self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 with input9.1 + 993
VERBOSE: QDQ graph optimizer quantization epilogue pass
VERBOSE: QDQ graph optimizer constant fold dangling QDQ pass
VERBOSE: After vertical fusions: 123 layers
VERBOSE: After dupe layer removal: 123 layers
VERBOSE: After final dead-layer removal: 123 layers
VERBOSE: After tensor merging: 123 layers
VERBOSE: After slice removal: 123 layers
VERBOSE: Eliminating concatenation 1563
VERBOSE: Generating copy for input11.1 to 1563 because concat input is consumed by producer of another input.
VERBOSE: Retargeting max_pool2d.1 to 1563
VERBOSE: Retargeting wh_2.1 to 1563
VERBOSE: Retargeting reg_2.1 to 1563
VERBOSE: Eliminating concatenation inputs10.1
VERBOSE: Retargeting 1429_clone_0 to quant_input.102TensorQ
VERBOSE: Retargeting dla_up_ida_2_up_3.1_clone_1 to quant_input.102TensorQ
VERBOSE: Eliminating concatenation inputs7.1
VERBOSE: Retargeting 1240_clone_0 to quant_input.90TensorQ
VERBOSE: Retargeting dla_up_ida_1_up_2.1_clone_1 to quant_input.90TensorQ
VERBOSE: Eliminating concatenation inputs5.1
VERBOSE: Retargeting 904_clone_0 to quant_input.82TensorQ
VERBOSE: Retargeting dla_up_ida_0_up_1.1_clone_1 to quant_input.82TensorQ
VERBOSE: Eliminating concatenation inputs4.1
VERBOSE: Generating copy for 1051_clone_0 to quant_input.78TensorQ because input does not support striding.
VERBOSE: Retargeting 993_clone_1 to quant_input.78TensorQ
VERBOSE: Retargeting inputs.21_clone_2 to quant_input.78TensorQ
VERBOSE: Eliminating concatenation inputs9.1
VERBOSE: Retargeting 1401_clone_0 to quant_input.100TensorQ
VERBOSE: Retargeting dla_up_ida_2_up_2.1_clone_1 to quant_input.100TensorQ
VERBOSE: Eliminating concatenation inputs6.1
VERBOSE: Retargeting 580_clone_0 to quant_input.88TensorQ
VERBOSE: Retargeting dla_up_ida_1_up_1.1_clone_1 to quant_input.88TensorQ
VERBOSE: Eliminating concatenation inputs3.1
VERBOSE: Generating copy for 876_clone_0 to quant_input.66TensorQ because input does not support striding.
VERBOSE: Retargeting 818_clone_1 to quant_input.66TensorQ
VERBOSE: Retargeting inputs.15_clone_2 to quant_input.66TensorQ
VERBOSE: Retargeting 760_clone_3 to quant_input.66TensorQ
VERBOSE: Eliminating concatenation inputs2.1
VERBOSE: Generating copy for 732_clone_0 to quant_input.56TensorQ because input does not support striding.
VERBOSE: Retargeting 674_clone_1 to quant_input.56TensorQ
VERBOSE: Eliminating concatenation inputs8.1
VERBOSE: Retargeting 256_clone_0 to quant_input.98TensorQ
VERBOSE: Retargeting dla_up_ida_2_up_1.1_clone_1 to quant_input.98TensorQ
VERBOSE: Eliminating concatenation inputs1.1
VERBOSE: Generating copy for 552_clone_0 to quant_input.42TensorQ because input does not support striding.
VERBOSE: Retargeting 494_clone_1 to quant_input.42TensorQ
VERBOSE: Retargeting inputs.9_clone_2 to quant_input.42TensorQ
VERBOSE: Retargeting 436_clone_3 to quant_input.42TensorQ
VERBOSE: Eliminating concatenation inputs0.1
VERBOSE: Generating copy for 408_clone_0 to quant_input.32TensorQ because input does not support striding.
VERBOSE: Retargeting 350_clone_1 to quant_input.32TensorQ
VERBOSE: Eliminating concatenation inputs.3
VERBOSE: Generating copy for 228_clone_0 to quant_input.18TensorQ because input does not support striding.
VERBOSE: Retargeting 170_clone_1 to quant_input.18TensorQ
VERBOSE: After concat removal: 117 layers
VERBOSE: Graph construction and optimization completed in 1.10948 seconds.
VERBOSE: Using cublasLt as a tactic source
WARNING: TensorRT was linked against cuBLAS/cuBLAS LT 11.8.0 but loaded cuBLAS/cuBLAS LT 11.7.4
[MemUsageChange] Init cuBLAS/cuBLASLt: CPU +8, GPU +222, now: CPU 329, GPU 2162 (MiB)
VERBOSE: Using cuDNN as a tactic source
[MemUsageChange] Init cuDNN: CPU +1, GPU +8, now: CPU 330, GPU 2170 (MiB)
WARNING: TensorRT was linked against cuDNN 8.3.2 but loaded cuDNN 8.2.4
Local timing cache in use. Profiling results in this builder pass will not be stored.
VERBOSE: Constructing optimization profile number 0 [1/1].
VERBOSE: Reserving memory for activation tensors. Host: 0 bytes Device: 45416448 bytes
VERBOSE: =============== Computing reformatting costs
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(1769472,589824,768,1) -> Int8(589824,589824:4,768,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.2TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.080256
VERBOSE: Tactic: 0 Time: 0.040672
VERBOSE: Fastest Tactic: 0 Time: 0.040672
VERBOSE: *************** Autotuning Reformat: Int8(1769472,589824,768,1) -> Int8(589824,589824:32,768,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.2TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.435872
VERBOSE: Tactic: 0 Time: 0.494816
VERBOSE: Fastest Tactic: 1002 Time: 0.435872
VERBOSE: *************** Autotuning Reformat: Int8(589824,589824:4,768,1) -> Int8(1769472,589824,768,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.2TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.093888
VERBOSE: Tactic: 0 Time: 0.127904
VERBOSE: Fastest Tactic: 1002 Time: 0.093888
VERBOSE: *************** Autotuning Reformat: Int8(589824,589824:4,768,1) -> Int8(589824,589824:32,768,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.2TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.71168
VERBOSE: Tactic: 0 Time: 0.170688
VERBOSE: Fastest Tactic: 0 Time: 0.170688
VERBOSE: *************** Autotuning Reformat: Int8(589824,589824:32,768,1) -> Int8(1769472,589824,768,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.2TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.986368
VERBOSE: Tactic: 0 Time: 0.221728
VERBOSE: Fastest Tactic: 0 Time: 0.221728
VERBOSE: *************** Autotuning Reformat: Int8(589824,589824:32,768,1) -> Int8(589824,589824:4,768,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.2TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.758368
VERBOSE: Tactic: 0 Time: 0.085216
VERBOSE: Fastest Tactic: 0 Time: 0.085216
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(2359296,589824:4,768,1) -> Int8(589824,589824:32,768,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.4TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.688512
VERBOSE: Tactic: 0 Time: 0.189376
VERBOSE: Fastest Tactic: 0 Time: 0.189376
VERBOSE: *************** Autotuning Reformat: Int8(589824,589824:32,768,1) -> Int8(2359296,589824:4,768,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.4TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.790784
VERBOSE: Tactic: 0 Time: 0.32032
VERBOSE: Fastest Tactic: 0 Time: 0.32032
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(2359296,589824:4,768,1) -> Int8(589824,589824:32,768,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(589824,589824:32,768,1) -> Int8(2359296,589824:4,768,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(1179648,147456:4,384,1) -> Int8(147456,147456:32,384,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> inputs.5) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.176864
VERBOSE: Tactic: 0 Time: 0.067904
VERBOSE: Fastest Tactic: 0 Time: 0.067904
VERBOSE: *************** Autotuning Reformat: Int8(147456,147456:32,384,1) -> Int8(1179648,147456:4,384,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> inputs.5) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.1768
VERBOSE: Tactic: 0 Time: 0.054464
VERBOSE: Fastest Tactic: 0 Time: 0.054464
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(1179648,147456:4,384,1) -> Int8(147456,147456:32,384,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(inputs.5 -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.176576
VERBOSE: Tactic: 0 Time: 0.067936
VERBOSE: Fastest Tactic: 0 Time: 0.067936
VERBOSE: *************** Autotuning Reformat: Int8(147456,147456:32,384,1) -> Int8(1179648,147456:4,384,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(inputs.5 -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.176864
VERBOSE: Tactic: 0 Time: 0.054656
VERBOSE: Fastest Tactic: 0 Time: 0.054656
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(1179648,147456:4,384,1) -> Int8(147456,147456:32,384,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(147456,147456:32,384,1) -> Int8(1179648,147456:4,384,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(294912,36864:4,192,1) -> Int8(36864,36864:32,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.8TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.05008
VERBOSE: Tactic: 0 Time: 0.021472
VERBOSE: Fastest Tactic: 0 Time: 0.021472
VERBOSE: *************** Autotuning Reformat: Int8(36864,36864:32,192,1) -> Int8(294912,36864:4,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.8TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.05008
VERBOSE: Tactic: 0 Time: 0.01952
VERBOSE: Fastest Tactic: 0 Time: 0.01952
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(2359296,36864,192,1) -> Int8(589824,36864:4,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(base_level2_project_1_output_quant.1TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.086272
VERBOSE: Tactic: 0 Time: 0.060352
VERBOSE: Fastest Tactic: 0 Time: 0.060352
VERBOSE: *************** Autotuning Reformat: Int8(2359296,36864,192,1) -> Int8(73728,36864:32,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(base_level2_project_1_output_quant.1TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.094176
VERBOSE: Tactic: 0 Time: 0.271808
VERBOSE: Fastest Tactic: 1002 Time: 0.094176
VERBOSE: *************** Autotuning Reformat: Int8(589824,36864:4,192,1) -> Int8(2359296,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(base_level2_project_1_output_quant.1TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.092064
VERBOSE: Tactic: 0 Time: 0.170336
VERBOSE: Fastest Tactic: 1002 Time: 0.092064
VERBOSE: *************** Autotuning Reformat: Int8(589824,36864:4,192,1) -> Int8(73728,36864:32,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(base_level2_project_1_output_quant.1TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.093472
VERBOSE: Tactic: 0 Time: 0.051072
VERBOSE: Fastest Tactic: 0 Time: 0.051072
VERBOSE: *************** Autotuning Reformat: Int8(73728,36864:32,192,1) -> Int8(2359296,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(base_level2_project_1_output_quant.1TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.232448
VERBOSE: Tactic: 0 Time: 0.174624
VERBOSE: Fastest Tactic: 0 Time: 0.174624
VERBOSE: *************** Autotuning Reformat: Int8(73728,36864:32,192,1) -> Int8(589824,36864:4,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(base_level2_project_1_output_quant.1TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.093216
VERBOSE: Tactic: 0 Time: 0.03184
VERBOSE: Fastest Tactic: 0 Time: 0.03184
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(589824,36864:4,192,1) -> Int8(73728,36864:32,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(73728,36864:32,192,1) -> Int8(589824,36864:4,192,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(2359296,36864,192,1) -> Float(73728,36864:32,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(base_level2_project_1_output_quant.1 -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.150496
VERBOSE: Tactic: 0 Time: 1.65501
VERBOSE: Fastest Tactic: 1002 Time: 0.150496
VERBOSE: *************** Autotuning Reformat: Float(73728,36864:32,192,1) -> Float(2359296,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(base_level2_project_1_output_quant.1 -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.164544
VERBOSE: Tactic: 0 Time: 0.466016
VERBOSE: Fastest Tactic: 1002 Time: 0.164544
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(2359296,36864,192,1) -> Float(73728,36864:32,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 170) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.154368
VERBOSE: Tactic: 0 Time: 1.65414
VERBOSE: Fastest Tactic: 1002 Time: 0.154368
VERBOSE: *************** Autotuning Reformat: Float(73728,36864:32,192,1) -> Float(2359296,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 170) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.161952
VERBOSE: Tactic: 0 Time: 0.470464
VERBOSE: Fastest Tactic: 1002 Time: 0.161952
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(73728,36864:32,192,1) -> Float(2359296,36864,192,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(2359296,36864,192,1) -> Int8(589824,36864:4,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> base_level2_tree1_relu_output_quant.1TensorQ) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.086304
VERBOSE: Tactic: 0 Time: 0.060672
VERBOSE: Fastest Tactic: 0 Time: 0.060672
VERBOSE: *************** Autotuning Reformat: Int8(2359296,36864,192,1) -> Int8(73728,36864:32,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> base_level2_tree1_relu_output_quant.1TensorQ) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.094144
VERBOSE: Tactic: 0 Time: 0.272064
VERBOSE: Fastest Tactic: 1002 Time: 0.094144
VERBOSE: *************** Autotuning Reformat: Int8(589824,36864:4,192,1) -> Int8(2359296,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> base_level2_tree1_relu_output_quant.1TensorQ) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.091552
VERBOSE: Tactic: 0 Time: 0.170144
VERBOSE: Fastest Tactic: 1002 Time: 0.091552
VERBOSE: *************** Autotuning Reformat: Int8(589824,36864:4,192,1) -> Int8(73728,36864:32,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> base_level2_tree1_relu_output_quant.1TensorQ) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.093568
VERBOSE: Tactic: 0 Time: 0.051104
VERBOSE: Fastest Tactic: 0 Time: 0.051104
VERBOSE: *************** Autotuning Reformat: Int8(73728,36864:32,192,1) -> Int8(2359296,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> base_level2_tree1_relu_output_quant.1TensorQ) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.232128
VERBOSE: Tactic: 0 Time: 0.174688
VERBOSE: Fastest Tactic: 0 Time: 0.174688
VERBOSE: *************** Autotuning Reformat: Int8(73728,36864:32,192,1) -> Int8(589824,36864:4,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> base_level2_tree1_relu_output_quant.1TensorQ) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.093088
VERBOSE: Tactic: 0 Time: 0.032288
VERBOSE: Fastest Tactic: 0 Time: 0.032288
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(2359296,36864,192,1) -> Int8(589824,36864:4,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(2359296,36864,192,1) -> Int8(73728,36864:32,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(589824,36864:4,192,1) -> Int8(73728,36864:32,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(73728,36864:32,192,1) -> Int8(589824,36864:4,192,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(73728,36864:32,192,1) -> Float(2359296,36864,192,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(4718592,36864,192,1) -> Int8(1179648,36864:4,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 170) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.087008
VERBOSE: Tactic: 0 Time: 0.063968
VERBOSE: Fastest Tactic: 0 Time: 0.063968
VERBOSE: *************** Autotuning Reformat: Int8(4718592,36864,192,1) -> Int8(147456,36864:32,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 170) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.094208
VERBOSE: Tactic: 0 Time: 0.27184
VERBOSE: Fastest Tactic: 1002 Time: 0.094208
VERBOSE: *************** Autotuning Reformat: Int8(1179648,36864:4,192,1) -> Int8(4718592,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 170) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.09248
VERBOSE: Tactic: 0 Time: 0.1704
VERBOSE: Fastest Tactic: 1002 Time: 0.09248
VERBOSE: *************** Autotuning Reformat: Int8(1179648,36864:4,192,1) -> Int8(147456,36864:32,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 170) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.09344
VERBOSE: Tactic: 0 Time: 0.051072
VERBOSE: Fastest Tactic: 0 Time: 0.051072
VERBOSE: *************** Autotuning Reformat: Int8(147456,36864:32,192,1) -> Int8(4718592,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 170) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.231648
VERBOSE: Tactic: 0 Time: 0.174752
VERBOSE: Fastest Tactic: 0 Time: 0.174752
VERBOSE: *************** Autotuning Reformat: Int8(147456,36864:32,192,1) -> Int8(1179648,36864:4,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 170) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.09296
VERBOSE: Tactic: 0 Time: 0.032544
VERBOSE: Fastest Tactic: 0 Time: 0.032544
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(589824,36864:4,192,1) -> Int8(73728,36864:32,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(73728,36864:32,192,1) -> Int8(589824,36864:4,192,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(2359296,36864,192,1) -> Int8(589824,36864:4,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(2359296,36864,192,1) -> Int8(73728,36864:32,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(589824,36864:4,192,1) -> Int8(73728,36864:32,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(73728,36864:32,192,1) -> Int8(589824,36864:4,192,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(589824,36864:4,192,1) -> Int8(4718592,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: 228 copy (Reformat)
VERBOSE: Tactic: 1002 Time: 0.09216
VERBOSE: Tactic: 0 Time: 0.1704
VERBOSE: Fastest Tactic: 1002 Time: 0.09216
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 1002
VERBOSE: *************** Autotuning Reformat: Int8(589824,36864:4,192,1) -> Int8(1179648,36864:4,192,1) ***************
VERBOSE: --------------- Timing Runner: 228 copy (Reformat)
VERBOSE: Tactic: 1002 Time: 0.093344
VERBOSE: Tactic: 0 Time: 0.083584
VERBOSE: Fastest Tactic: 0 Time: 0.083584
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0
VERBOSE: *************** Autotuning Reformat: Int8(589824,36864:4,192,1) -> Int8(147456,36864:32,192,1) ***************
VERBOSE: --------------- Timing Runner: 228 copy (Reformat)
VERBOSE: Tactic: 1002 Time: 0.093248
VERBOSE: Tactic: 0 Time: 0.051072
VERBOSE: Fastest Tactic: 0 Time: 0.051072
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0
VERBOSE: *************** Autotuning Reformat: Int8(73728,36864:32,192,1) -> Int8(4718592,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: 228 copy (Reformat)
VERBOSE: Tactic: 1002 Time: 0.231872
VERBOSE: Tactic: 0 Time: 0.174752
VERBOSE: Fastest Tactic: 0 Time: 0.174752
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0
VERBOSE: *************** Autotuning Reformat: Int8(73728,36864:32,192,1) -> Int8(1179648,36864:4,192,1) ***************
VERBOSE: --------------- Timing Runner: 228 copy (Reformat)
VERBOSE: Tactic: 1002 Time: 0.093088
VERBOSE: Tactic: 0 Time: 0.032128
VERBOSE: Fastest Tactic: 0 Time: 0.032128
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0
VERBOSE: *************** Autotuning Reformat: Int8(73728,36864:32,192,1) -> Int8(147456,36864:32,192,1) ***************
VERBOSE: --------------- Timing Runner: 228 copy (Reformat)
VERBOSE: Tactic: 1002 Time: 0.057952
VERBOSE: Tactic: 0 Time: 0.033856
VERBOSE: Fastest Tactic: 0 Time: 0.033856
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(4718592,36864,192,1) -> Int8(1179648,36864:4,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.18TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.160224
VERBOSE: Tactic: 0 Time: 0.125216
VERBOSE: Fastest Tactic: 0 Time: 0.125216
VERBOSE: *************** Autotuning Reformat: Int8(4718592,36864,192,1) -> Int8(147456,36864:32,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.18TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.174784
VERBOSE: Tactic: 0 Time: 0.550048
VERBOSE: Fastest Tactic: 1002 Time: 0.174784
VERBOSE: *************** Autotuning Reformat: Int8(1179648,36864:4,192,1) -> Int8(147456,36864:32,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.18TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.1776
VERBOSE: Tactic: 0 Time: 0.152736
VERBOSE: Fastest Tactic: 0 Time: 0.152736
VERBOSE: *************** Autotuning Reformat: Int8(147456,36864:32,192,1) -> Int8(1179648,36864:4,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.18TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.176896
VERBOSE: Tactic: 0 Time: 0.054656
VERBOSE: Fastest Tactic: 0 Time: 0.054656
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(2359296,36864,192,1) -> Float(73728,36864:32,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Float(73728,36864:32,192,1) -> Float(2359296,36864,192,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(73728,36864:32,192,1) -> Float(2359296,36864,192,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(73728,36864:32,192,1) -> Float(2359296,36864,192,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(2359296,36864,192,1) -> Int8(589824,36864:4,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(2359296,36864,192,1) -> Int8(73728,36864:32,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(589824,36864:4,192,1) -> Int8(2359296,36864,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(589824,36864:4,192,1) -> Int8(73728,36864:32,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(73728,36864:32,192,1) -> Int8(2359296,36864,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(73728,36864:32,192,1) -> Int8(589824,36864:4,192,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(2359296,36864,192,1) -> Int8(589824,36864:4,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(2359296,36864,192,1) -> Int8(73728,36864:32,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(589824,36864:4,192,1) -> Int8(73728,36864:32,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(73728,36864:32,192,1) -> Int8(589824,36864:4,192,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(2359296,36864,192,1) -> Int8(589824,36864:4,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(2359296,36864,192,1) -> Int8(73728,36864:32,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(589824,36864:4,192,1) -> Int8(73728,36864:32,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(73728,36864:32,192,1) -> Int8(589824,36864:4,192,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(147456,9216:4,96,1) -> Int8(18432,9216:32,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.22TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.029984
VERBOSE: Tactic: 0 Time: 0.017728
VERBOSE: Fastest Tactic: 0 Time: 0.017728
VERBOSE: *************** Autotuning Reformat: Int8(18432,9216:32,96,1) -> Int8(147456,9216:4,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.22TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.02976
VERBOSE: Tactic: 0 Time: 0.0136
VERBOSE: Fastest Tactic: 0 Time: 0.0136
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(1179648,9216,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(base_level3_tree1_project_1_output_quant.1TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.048384
VERBOSE: Tactic: 0 Time: 0.027648
VERBOSE: Fastest Tactic: 0 Time: 0.027648
VERBOSE: *************** Autotuning Reformat: Int8(1179648,9216,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(base_level3_tree1_project_1_output_quant.1TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.053792
VERBOSE: Tactic: 0 Time: 0.144192
VERBOSE: Fastest Tactic: 1002 Time: 0.053792
VERBOSE: *************** Autotuning Reformat: Int8(294912,9216:4,96,1) -> Int8(1179648,9216,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(base_level3_tree1_project_1_output_quant.1TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.052096
VERBOSE: Tactic: 0 Time: 0.088
VERBOSE: Fastest Tactic: 1002 Time: 0.052096
VERBOSE: *************** Autotuning Reformat: Int8(294912,9216:4,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(base_level3_tree1_project_1_output_quant.1TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.050112
VERBOSE: Tactic: 0 Time: 0.043456
VERBOSE: Fastest Tactic: 0 Time: 0.043456
VERBOSE: *************** Autotuning Reformat: Int8(36864,9216:32,96,1) -> Int8(1179648,9216,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(base_level3_tree1_project_1_output_quant.1TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.125184
VERBOSE: Tactic: 0 Time: 0.092288
VERBOSE: Fastest Tactic: 0 Time: 0.092288
VERBOSE: *************** Autotuning Reformat: Int8(36864,9216:32,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(base_level3_tree1_project_1_output_quant.1TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.050048
VERBOSE: Tactic: 0 Time: 0.01984
VERBOSE: Fastest Tactic: 0 Time: 0.01984
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(294912,9216:4,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(36864,9216:32,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(1179648,9216,96,1) -> Float(36864,9216:32,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(base_level3_tree1_project_1_output_quant.1 -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.08992
VERBOSE: Tactic: 0 Time: 0.217824
VERBOSE: Fastest Tactic: 1002 Time: 0.08992
VERBOSE: *************** Autotuning Reformat: Float(36864,9216:32,96,1) -> Float(1179648,9216,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(base_level3_tree1_project_1_output_quant.1 -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.088544
VERBOSE: Tactic: 0 Time: 0.133312
VERBOSE: Fastest Tactic: 1002 Time: 0.088544
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(1179648,9216,96,1) -> Float(36864,9216:32,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 350) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.091392
VERBOSE: Tactic: 0 Time: 0.217568
VERBOSE: Fastest Tactic: 1002 Time: 0.091392
VERBOSE: *************** Autotuning Reformat: Float(36864,9216:32,96,1) -> Float(1179648,9216,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 350) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.087264
VERBOSE: Tactic: 0 Time: 0.133056
VERBOSE: Fastest Tactic: 1002 Time: 0.087264
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(36864,9216:32,96,1) -> Float(1179648,9216,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(1179648,9216,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> base_level3_tree1_tree1_relu_output_quant.1TensorQ) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.048352
VERBOSE: Tactic: 0 Time: 0.027648
VERBOSE: Fastest Tactic: 0 Time: 0.027648
VERBOSE: *************** Autotuning Reformat: Int8(1179648,9216,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> base_level3_tree1_tree1_relu_output_quant.1TensorQ) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.05376
VERBOSE: Tactic: 0 Time: 0.14384
VERBOSE: Fastest Tactic: 1002 Time: 0.05376
VERBOSE: *************** Autotuning Reformat: Int8(294912,9216:4,96,1) -> Int8(1179648,9216,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> base_level3_tree1_tree1_relu_output_quant.1TensorQ) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.05152
VERBOSE: Tactic: 0 Time: 0.087968
VERBOSE: Fastest Tactic: 1002 Time: 0.05152
VERBOSE: *************** Autotuning Reformat: Int8(294912,9216:4,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> base_level3_tree1_tree1_relu_output_quant.1TensorQ) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.050112
VERBOSE: Tactic: 0 Time: 0.043488
VERBOSE: Fastest Tactic: 0 Time: 0.043488
VERBOSE: *************** Autotuning Reformat: Int8(36864,9216:32,96,1) -> Int8(1179648,9216,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> base_level3_tree1_tree1_relu_output_quant.1TensorQ) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.125504
VERBOSE: Tactic: 0 Time: 0.09232
VERBOSE: Fastest Tactic: 0 Time: 0.09232
VERBOSE: *************** Autotuning Reformat: Int8(36864,9216:32,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> base_level3_tree1_tree1_relu_output_quant.1TensorQ) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.049856
VERBOSE: Tactic: 0 Time: 0.01984
VERBOSE: Fastest Tactic: 0 Time: 0.01984
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(1179648,9216,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(1179648,9216,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(294912,9216:4,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(36864,9216:32,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(36864,9216:32,96,1) -> Float(1179648,9216,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(2359296,9216,96,1) -> Int8(589824,9216:4,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 350) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.048096
VERBOSE: Tactic: 0 Time: 0.027648
VERBOSE: Fastest Tactic: 0 Time: 0.027648
VERBOSE: *************** Autotuning Reformat: Int8(2359296,9216,96,1) -> Int8(73728,9216:32,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 350) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.053216
VERBOSE: Tactic: 0 Time: 0.143776
VERBOSE: Fastest Tactic: 1002 Time: 0.053216
VERBOSE: *************** Autotuning Reformat: Int8(589824,9216:4,96,1) -> Int8(2359296,9216,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 350) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.051776
VERBOSE: Tactic: 0 Time: 0.088224
VERBOSE: Fastest Tactic: 1002 Time: 0.051776
VERBOSE: *************** Autotuning Reformat: Int8(589824,9216:4,96,1) -> Int8(73728,9216:32,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 350) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.05008
VERBOSE: Tactic: 0 Time: 0.043456
VERBOSE: Fastest Tactic: 0 Time: 0.043456
VERBOSE: *************** Autotuning Reformat: Int8(73728,9216:32,96,1) -> Int8(2359296,9216,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 350) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.125792
VERBOSE: Tactic: 0 Time: 0.092096
VERBOSE: Fastest Tactic: 0 Time: 0.092096
VERBOSE: *************** Autotuning Reformat: Int8(73728,9216:32,96,1) -> Int8(589824,9216:4,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 350) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.05008
VERBOSE: Tactic: 0 Time: 0.019776
VERBOSE: Fastest Tactic: 0 Time: 0.019776
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(294912,9216:4,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(36864,9216:32,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(1179648,9216,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(1179648,9216,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(294912,9216:4,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(36864,9216:32,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(294912,9216:4,96,1) -> Int8(2359296,9216,96,1) ***************
VERBOSE: --------------- Timing Runner: 408 copy (Reformat)
VERBOSE: Tactic: 1002 Time: 0.051776
VERBOSE: Tactic: 0 Time: 0.088256
VERBOSE: Fastest Tactic: 1002 Time: 0.051776
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 1002
VERBOSE: *************** Autotuning Reformat: Int8(294912,9216:4,96,1) -> Int8(589824,9216:4,96,1) ***************
VERBOSE: --------------- Timing Runner: 408 copy (Reformat)
VERBOSE: Tactic: 1002 Time: 0.05008
VERBOSE: Tactic: 0 Time: 0.043872
VERBOSE: Fastest Tactic: 0 Time: 0.043872
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0
VERBOSE: *************** Autotuning Reformat: Int8(294912,9216:4,96,1) -> Int8(73728,9216:32,96,1) ***************
VERBOSE: --------------- Timing Runner: 408 copy (Reformat)
VERBOSE: Tactic: 1002 Time: 0.05008
VERBOSE: Tactic: 0 Time: 0.043488
VERBOSE: Fastest Tactic: 0 Time: 0.043488
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0
VERBOSE: *************** Autotuning Reformat: Int8(36864,9216:32,96,1) -> Int8(2359296,9216,96,1) ***************
VERBOSE: --------------- Timing Runner: 408 copy (Reformat)
VERBOSE: Tactic: 1002 Time: 0.125472
VERBOSE: Tactic: 0 Time: 0.09232
VERBOSE: Fastest Tactic: 0 Time: 0.09232
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0
VERBOSE: *************** Autotuning Reformat: Int8(36864,9216:32,96,1) -> Int8(589824,9216:4,96,1) ***************
VERBOSE: --------------- Timing Runner: 408 copy (Reformat)
VERBOSE: Tactic: 1002 Time: 0.050048
VERBOSE: Tactic: 0 Time: 0.01984
VERBOSE: Fastest Tactic: 0 Time: 0.01984
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0
VERBOSE: *************** Autotuning Reformat: Int8(36864,9216:32,96,1) -> Int8(73728,9216:32,96,1) ***************
VERBOSE: --------------- Timing Runner: 408 copy (Reformat)
VERBOSE: Tactic: 1002 Time: 0.033952
VERBOSE: Tactic: 0 Time: 0.02112
VERBOSE: Fastest Tactic: 0 Time: 0.02112
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(2359296,9216,96,1) -> Int8(589824,9216:4,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.32TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.087264
VERBOSE: Tactic: 0 Time: 0.060608
VERBOSE: Fastest Tactic: 0 Time: 0.060608
VERBOSE: *************** Autotuning Reformat: Int8(2359296,9216,96,1) -> Int8(73728,9216:32,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.32TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.094144
VERBOSE: Tactic: 0 Time: 0.28368
VERBOSE: Fastest Tactic: 1002 Time: 0.094144
VERBOSE: *************** Autotuning Reformat: Int8(589824,9216:4,96,1) -> Int8(73728,9216:32,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.32TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.093536
VERBOSE: Tactic: 0 Time: 0.080736
VERBOSE: Fastest Tactic: 0 Time: 0.080736
VERBOSE: *************** Autotuning Reformat: Int8(73728,9216:32,96,1) -> Int8(589824,9216:4,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.32TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.092672
VERBOSE: Tactic: 0 Time: 0.032224
VERBOSE: Fastest Tactic: 0 Time: 0.032224
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(1179648,9216,96,1) -> Float(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Float(36864,9216:32,96,1) -> Float(1179648,9216,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(36864,9216:32,96,1) -> Float(1179648,9216,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(1179648,9216,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(1179648,9216,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(294912,9216:4,96,1) -> Int8(1179648,9216,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(294912,9216:4,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(36864,9216:32,96,1) -> Int8(1179648,9216,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(36864,9216:32,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(1179648,9216,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(1179648,9216,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(294912,9216:4,96,1) -> Int8(1179648,9216,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(294912,9216:4,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(36864,9216:32,96,1) -> Int8(1179648,9216,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(36864,9216:32,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(1179648,9216,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(1179648,9216,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(294912,9216:4,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(36864,9216:32,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(294912,9216:4,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(36864,9216:32,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(1179648,9216,96,1) -> Float(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Float(36864,9216:32,96,1) -> Float(1179648,9216,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(1179648,9216,96,1) -> Float(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Float(36864,9216:32,96,1) -> Float(1179648,9216,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(36864,9216:32,96,1) -> Float(1179648,9216,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(1179648,9216,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(1179648,9216,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(294912,9216:4,96,1) -> Int8(1179648,9216,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(294912,9216:4,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(36864,9216:32,96,1) -> Int8(1179648,9216,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(36864,9216:32,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(1179648,9216,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(1179648,9216,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(294912,9216:4,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(36864,9216:32,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(36864,9216:32,96,1) -> Float(1179648,9216,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(4128768,9216,96,1) -> Int8(1032192,9216:4,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 436) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.04864
VERBOSE: Tactic: 0 Time: 0.027712
VERBOSE: Fastest Tactic: 0 Time: 0.027712
VERBOSE: *************** Autotuning Reformat: Int8(4128768,9216,96,1) -> Int8(129024,9216:32,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 436) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.053984
VERBOSE: Tactic: 0 Time: 0.144192
VERBOSE: Fastest Tactic: 1002 Time: 0.053984
VERBOSE: *************** Autotuning Reformat: Int8(1032192,9216:4,96,1) -> Int8(4128768,9216,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 436) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.051808
VERBOSE: Tactic: 0 Time: 0.088224
VERBOSE: Fastest Tactic: 1002 Time: 0.051808
VERBOSE: *************** Autotuning Reformat: Int8(1032192,9216:4,96,1) -> Int8(129024,9216:32,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 436) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.050048
VERBOSE: Tactic: 0 Time: 0.043456
VERBOSE: Fastest Tactic: 0 Time: 0.043456
VERBOSE: *************** Autotuning Reformat: Int8(129024,9216:32,96,1) -> Int8(4128768,9216,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 436) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.125792
VERBOSE: Tactic: 0 Time: 0.092288
VERBOSE: Fastest Tactic: 0 Time: 0.092288
VERBOSE: *************** Autotuning Reformat: Int8(129024,9216:32,96,1) -> Int8(1032192,9216:4,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 436) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.05008
VERBOSE: Tactic: 0 Time: 0.020352
VERBOSE: Fastest Tactic: 0 Time: 0.020352
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(2359296,36864,192,1) -> Int8(589824,36864:4,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(2359296,36864,192,1) -> Int8(73728,36864:32,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(589824,36864:4,192,1) -> Int8(73728,36864:32,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(73728,36864:32,192,1) -> Int8(589824,36864:4,192,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(1032192,9216:4,96,1) -> Int8(4128768,9216,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> inputs.9) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.03424
VERBOSE: Tactic: 0 Time: 0.047776
VERBOSE: Fastest Tactic: 1002 Time: 0.03424
VERBOSE: *************** Autotuning Reformat: Int8(1032192,9216:4,96,1) -> Int8(129024,9216:32,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> inputs.9) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.029696
VERBOSE: Tactic: 0 Time: 0.018016
VERBOSE: Fastest Tactic: 0 Time: 0.018016
VERBOSE: *************** Autotuning Reformat: Int8(129024,9216:32,96,1) -> Int8(4128768,9216,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> inputs.9) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.072448
VERBOSE: Tactic: 0 Time: 0.050656
VERBOSE: Fastest Tactic: 0 Time: 0.050656
VERBOSE: *************** Autotuning Reformat: Int8(129024,9216:32,96,1) -> Int8(1032192,9216:4,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> inputs.9) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.029984
VERBOSE: Tactic: 0 Time: 0.0136
VERBOSE: Fastest Tactic: 0 Time: 0.0136
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(36864,9216:32,96,1) -> Float(1179648,9216,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(4128768,9216,96,1) -> Int8(1032192,9216:4,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(4128768,9216,96,1) -> Int8(129024,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(1032192,9216:4,96,1) -> Int8(4128768,9216,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(1032192,9216:4,96,1) -> Int8(129024,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(129024,9216:32,96,1) -> Int8(4128768,9216,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(129024,9216:32,96,1) -> Int8(1032192,9216:4,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(294912,9216:4,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(36864,9216:32,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(1179648,9216,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(1179648,9216,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(294912,9216:4,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(36864,9216:32,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(294912,9216:4,96,1) -> Int8(4128768,9216,96,1) ***************
VERBOSE: --------------- Timing Runner: 552 copy (Reformat)
VERBOSE: Tactic: 1002 Time: 0.051552
VERBOSE: Tactic: 0 Time: 0.088224
VERBOSE: Fastest Tactic: 1002 Time: 0.051552
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 1002
VERBOSE: *************** Autotuning Reformat: Int8(294912,9216:4,96,1) -> Int8(1032192,9216:4,96,1) ***************
VERBOSE: --------------- Timing Runner: 552 copy (Reformat)
VERBOSE: Tactic: 1002 Time: 0.05008
VERBOSE: Tactic: 0 Time: 0.043904
VERBOSE: Fastest Tactic: 0 Time: 0.043904
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0
VERBOSE: *************** Autotuning Reformat: Int8(294912,9216:4,96,1) -> Int8(129024,9216:32,96,1) ***************
VERBOSE: --------------- Timing Runner: 552 copy (Reformat)
VERBOSE: Tactic: 1002 Time: 0.050048
VERBOSE: Tactic: 0 Time: 0.043488
VERBOSE: Fastest Tactic: 0 Time: 0.043488
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0
VERBOSE: *************** Autotuning Reformat: Int8(36864,9216:32,96,1) -> Int8(4128768,9216,96,1) ***************
VERBOSE: --------------- Timing Runner: 552 copy (Reformat)
VERBOSE: Tactic: 1002 Time: 0.125472
VERBOSE: Tactic: 0 Time: 0.092288
VERBOSE: Fastest Tactic: 0 Time: 0.092288
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0
VERBOSE: *************** Autotuning Reformat: Int8(36864,9216:32,96,1) -> Int8(1032192,9216:4,96,1) ***************
VERBOSE: --------------- Timing Runner: 552 copy (Reformat)
VERBOSE: Tactic: 1002 Time: 0.050048
VERBOSE: Tactic: 0 Time: 0.019872
VERBOSE: Fastest Tactic: 0 Time: 0.019872
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0
VERBOSE: *************** Autotuning Reformat: Int8(36864,9216:32,96,1) -> Int8(129024,9216:32,96,1) ***************
VERBOSE: --------------- Timing Runner: 552 copy (Reformat)
VERBOSE: Tactic: 1002 Time: 0.033952
VERBOSE: Tactic: 0 Time: 0.02112
VERBOSE: Fastest Tactic: 0 Time: 0.02112
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(4128768,9216,96,1) -> Int8(1032192,9216:4,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.42TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.14256
VERBOSE: Tactic: 0 Time: 0.10976
VERBOSE: Fastest Tactic: 0 Time: 0.10976
VERBOSE: *************** Autotuning Reformat: Int8(4128768,9216,96,1) -> Int8(129024,9216:32,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.42TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.154496
VERBOSE: Tactic: 0 Time: 0.490368
VERBOSE: Fastest Tactic: 1002 Time: 0.154496
VERBOSE: *************** Autotuning Reformat: Int8(1032192,9216:4,96,1) -> Int8(129024,9216:32,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.42TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.156896
VERBOSE: Tactic: 0 Time: 0.136032
VERBOSE: Fastest Tactic: 0 Time: 0.136032
VERBOSE: *************** Autotuning Reformat: Int8(129024,9216:32,96,1) -> Int8(1032192,9216:4,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.42TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.155616
VERBOSE: Tactic: 0 Time: 0.049056
VERBOSE: Fastest Tactic: 0 Time: 0.049056
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(1179648,9216,96,1) -> Float(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Float(36864,9216:32,96,1) -> Float(1179648,9216,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(36864,9216:32,96,1) -> Float(1179648,9216,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(36864,9216:32,96,1) -> Float(1179648,9216,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(1179648,9216,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(1179648,9216,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(294912,9216:4,96,1) -> Int8(1179648,9216,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(294912,9216:4,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(36864,9216:32,96,1) -> Int8(1179648,9216,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(36864,9216:32,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(1179648,9216,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(1179648,9216,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(294912,9216:4,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(36864,9216:32,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(1179648,9216,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(1179648,9216,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(294912,9216:4,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(36864,9216:32,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(1179648,9216,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(1179648,9216,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(294912,9216:4,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(36864,9216:32,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(73728,2304:4,48,1) -> Int8(9216,2304:32,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.46TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.019776
VERBOSE: Tactic: 0 Time: 0.016064
VERBOSE: Fastest Tactic: 0 Time: 0.016064
VERBOSE: *************** Autotuning Reformat: Int8(9216,2304:32,48,1) -> Int8(73728,2304:4,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.46TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.019808
VERBOSE: Tactic: 0 Time: 0.01072
VERBOSE: Fastest Tactic: 0 Time: 0.01072
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(589824,9216,96,1) -> Float(589824,1,6144,64) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(1294 -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.056896
VERBOSE: Tactic: 0 Time: 0.063072
VERBOSE: Fastest Tactic: 1002 Time: 0.056896
VERBOSE: *************** Autotuning Reformat: Float(589824,1,6144,64) -> Float(589824,9216,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(1294 -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.05712
VERBOSE: Tactic: 0 Time: 0.065664
VERBOSE: Fastest Tactic: 1002 Time: 0.05712
VERBOSE: *************** Autotuning Reformat: Float(18432,9216:32,96,1) -> Float(589824,9216,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(1294 -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.061376
VERBOSE: Tactic: 0 Time: 0.06624
VERBOSE: Fastest Tactic: 1002 Time: 0.061376
VERBOSE: *************** Autotuning Reformat: Float(18432,9216:32,96,1) -> Float(589824,1,6144,64) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(1294 -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.053952
VERBOSE: Tactic: 0 Time: 0.04128
VERBOSE: Fastest Tactic: 0 Time: 0.04128
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(589824,2304,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(base_level4_tree1_project_1_output_quant.1TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.033376
VERBOSE: Tactic: 0 Time: 0.017152
VERBOSE: Fastest Tactic: 0 Time: 0.017152
VERBOSE: *************** Autotuning Reformat: Int8(589824,2304,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(base_level4_tree1_project_1_output_quant.1TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.03424
VERBOSE: Tactic: 0 Time: 0.076704
VERBOSE: Fastest Tactic: 1002 Time: 0.03424
VERBOSE: *************** Autotuning Reformat: Int8(147456,2304:4,48,1) -> Int8(589824,2304,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(base_level4_tree1_project_1_output_quant.1TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.034016
VERBOSE: Tactic: 0 Time: 0.049184
VERBOSE: Fastest Tactic: 1002 Time: 0.034016
VERBOSE: *************** Autotuning Reformat: Int8(147456,2304:4,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(base_level4_tree1_project_1_output_quant.1TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.029728
VERBOSE: Tactic: 0 Time: 0.025376
VERBOSE: Fastest Tactic: 0 Time: 0.025376
VERBOSE: *************** Autotuning Reformat: Int8(18432,2304:32,48,1) -> Int8(589824,2304,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(base_level4_tree1_project_1_output_quant.1TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.0736
VERBOSE: Tactic: 0 Time: 0.052864
VERBOSE: Fastest Tactic: 0 Time: 0.052864
VERBOSE: *************** Autotuning Reformat: Int8(18432,2304:32,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(base_level4_tree1_project_1_output_quant.1TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.029952
VERBOSE: Tactic: 0 Time: 0.013824
VERBOSE: Fastest Tactic: 0 Time: 0.013824
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(2359296,1,12288,64) -> Float(2359296,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(dla_up_ida_2_up_1.1 -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.158368
VERBOSE: Tactic: 0 Time: 0.488416
VERBOSE: Fastest Tactic: 1002 Time: 0.158368
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(4718592,36864,192,1) -> Int8(1179648,36864:4,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(4718592,36864,192,1) -> Int8(147456,36864:32,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(1179648,36864:4,192,1) -> Int8(4718592,36864,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(1179648,36864:4,192,1) -> Int8(147456,36864:32,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(147456,36864:32,192,1) -> Int8(4718592,36864,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(147456,36864:32,192,1) -> Int8(1179648,36864:4,192,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(73728,36864:32,192,1) -> Float(2359296,36864,192,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(4718592,36864,192,1) -> Int8(1179648,36864:4,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(4718592,36864,192,1) -> Int8(147456,36864:32,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(1179648,36864:4,192,1) -> Int8(4718592,36864,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(1179648,36864:4,192,1) -> Int8(147456,36864:32,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(147456,36864:32,192,1) -> Int8(4718592,36864,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(147456,36864:32,192,1) -> Int8(1179648,36864:4,192,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(147456,2304:4,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(18432,2304:32,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(589824,2304,48,1) -> Float(18432,2304:32,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(base_level4_tree1_project_1_output_quant.1 -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.058816
VERBOSE: Tactic: 0 Time: 0.11248
VERBOSE: Fastest Tactic: 1002 Time: 0.058816
VERBOSE: *************** Autotuning Reformat: Float(18432,2304:32,48,1) -> Float(589824,2304,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(base_level4_tree1_project_1_output_quant.1 -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.058848
VERBOSE: Tactic: 0 Time: 0.067552
VERBOSE: Fastest Tactic: 1002 Time: 0.058848
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(589824,2304,48,1) -> Float(18432,2304:32,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 674) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.058368
VERBOSE: Tactic: 0 Time: 0.112352
VERBOSE: Fastest Tactic: 1002 Time: 0.058368
VERBOSE: *************** Autotuning Reformat: Float(18432,2304:32,48,1) -> Float(589824,2304,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 674) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.059488
VERBOSE: Tactic: 0 Time: 0.067552
VERBOSE: Fastest Tactic: 1002 Time: 0.059488
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(4718592,36864,192,1) -> Int8(1179648,36864:4,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(4718592,36864,192,1) -> Int8(147456,36864:32,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(1179648,36864:4,192,1) -> Int8(147456,36864:32,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(147456,36864:32,192,1) -> Int8(1179648,36864:4,192,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(1179648,36864:4,192,1) -> Int8(4718592,36864,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(1179648,36864:4,192,1) -> Int8(147456,36864:32,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(147456,36864:32,192,1) -> Int8(4718592,36864,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(147456,36864:32,192,1) -> Int8(1179648,36864:4,192,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(18432,2304:32,48,1) -> Float(589824,2304,48,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(589824,2304,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> base_level4_tree1_tree1_relu_output_quant.1TensorQ) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.032256
VERBOSE: Tactic: 0 Time: 0.017184
VERBOSE: Fastest Tactic: 0 Time: 0.017184
VERBOSE: *************** Autotuning Reformat: Int8(589824,2304,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> base_level4_tree1_tree1_relu_output_quant.1TensorQ) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.033696
VERBOSE: Tactic: 0 Time: 0.076448
VERBOSE: Fastest Tactic: 1002 Time: 0.033696
VERBOSE: *************** Autotuning Reformat: Int8(147456,2304:4,48,1) -> Int8(589824,2304,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> base_level4_tree1_tree1_relu_output_quant.1TensorQ) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.033984
VERBOSE: Tactic: 0 Time: 0.049216
VERBOSE: Fastest Tactic: 1002 Time: 0.033984
VERBOSE: *************** Autotuning Reformat: Int8(147456,2304:4,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> base_level4_tree1_tree1_relu_output_quant.1TensorQ) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.030016
VERBOSE: Tactic: 0 Time: 0.025408
VERBOSE: Fastest Tactic: 0 Time: 0.025408
VERBOSE: *************** Autotuning Reformat: Int8(18432,2304:32,48,1) -> Int8(589824,2304,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> base_level4_tree1_tree1_relu_output_quant.1TensorQ) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.07168
VERBOSE: Tactic: 0 Time: 0.052928
VERBOSE: Fastest Tactic: 0 Time: 0.052928
VERBOSE: *************** Autotuning Reformat: Int8(18432,2304:32,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> base_level4_tree1_tree1_relu_output_quant.1TensorQ) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.029728
VERBOSE: Tactic: 0 Time: 0.013824
VERBOSE: Fastest Tactic: 0 Time: 0.013824
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(589824,2304,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(589824,2304,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(147456,2304:4,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(18432,2304:32,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(18432,2304:32,48,1) -> Float(589824,2304,48,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(1179648,2304,48,1) -> Int8(294912,2304:4,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 674) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.032544
VERBOSE: Tactic: 0 Time: 0.017184
VERBOSE: Fastest Tactic: 0 Time: 0.017184
VERBOSE: *************** Autotuning Reformat: Int8(1179648,2304,48,1) -> Int8(36864,2304:32,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 674) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.033376
VERBOSE: Tactic: 0 Time: 0.076416
VERBOSE: Fastest Tactic: 1002 Time: 0.033376
VERBOSE: *************** Autotuning Reformat: Int8(294912,2304:4,48,1) -> Int8(1179648,2304,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 674) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.033984
VERBOSE: Tactic: 0 Time: 0.049184
VERBOSE: Fastest Tactic: 1002 Time: 0.033984
VERBOSE: *************** Autotuning Reformat: Int8(294912,2304:4,48,1) -> Int8(36864,2304:32,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 674) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.029728
VERBOSE: Tactic: 0 Time: 0.025344
VERBOSE: Fastest Tactic: 0 Time: 0.025344
VERBOSE: *************** Autotuning Reformat: Int8(36864,2304:32,48,1) -> Int8(1179648,2304,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 674) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.07168
VERBOSE: Tactic: 0 Time: 0.052544
VERBOSE: Fastest Tactic: 0 Time: 0.052544
VERBOSE: *************** Autotuning Reformat: Int8(36864,2304:32,48,1) -> Int8(294912,2304:4,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 674) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.029696
VERBOSE: Tactic: 0 Time: 0.013856
VERBOSE: Fastest Tactic: 0 Time: 0.013856
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(147456,2304:4,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(18432,2304:32,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(589824,2304,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(589824,2304,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(147456,2304:4,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(18432,2304:32,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(147456,2304:4,48,1) -> Int8(1179648,2304,48,1) ***************
VERBOSE: --------------- Timing Runner: 732 copy (Reformat)
VERBOSE: Tactic: 1002 Time: 0.033952
VERBOSE: Tactic: 0 Time: 0.049216
VERBOSE: Fastest Tactic: 1002 Time: 0.033952
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 1002
VERBOSE: *************** Autotuning Reformat: Int8(147456,2304:4,48,1) -> Int8(294912,2304:4,48,1) ***************
VERBOSE: --------------- Timing Runner: 732 copy (Reformat)
VERBOSE: Tactic: 1002 Time: 0.030016
VERBOSE: Tactic: 0 Time: 0.025728
VERBOSE: Fastest Tactic: 0 Time: 0.025728
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0
VERBOSE: *************** Autotuning Reformat: Int8(147456,2304:4,48,1) -> Int8(36864,2304:32,48,1) ***************
VERBOSE: --------------- Timing Runner: 732 copy (Reformat)
VERBOSE: Tactic: 1002 Time: 0.029984
VERBOSE: Tactic: 0 Time: 0.025664
VERBOSE: Fastest Tactic: 0 Time: 0.025664
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0
VERBOSE: *************** Autotuning Reformat: Int8(18432,2304:32,48,1) -> Int8(1179648,2304,48,1) ***************
VERBOSE: --------------- Timing Runner: 732 copy (Reformat)
VERBOSE: Tactic: 1002 Time: 0.07168
VERBOSE: Tactic: 0 Time: 0.052736
VERBOSE: Fastest Tactic: 0 Time: 0.052736
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0
VERBOSE: *************** Autotuning Reformat: Int8(18432,2304:32,48,1) -> Int8(294912,2304:4,48,1) ***************
VERBOSE: --------------- Timing Runner: 732 copy (Reformat)
VERBOSE: Tactic: 1002 Time: 0.029696
VERBOSE: Tactic: 0 Time: 0.013824
VERBOSE: Fastest Tactic: 0 Time: 0.013824
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0
VERBOSE: *************** Autotuning Reformat: Int8(18432,2304:32,48,1) -> Int8(36864,2304:32,48,1) ***************
VERBOSE: --------------- Timing Runner: 732 copy (Reformat)
VERBOSE: Tactic: 1002 Time: 0.022912
VERBOSE: Tactic: 0 Time: 0.014048
VERBOSE: Fastest Tactic: 0 Time: 0.014048
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(1179648,2304,48,1) -> Int8(294912,2304:4,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.56TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.048928
VERBOSE: Tactic: 0 Time: 0.027648
VERBOSE: Fastest Tactic: 0 Time: 0.027648
VERBOSE: *************** Autotuning Reformat: Int8(1179648,2304,48,1) -> Int8(36864,2304:32,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.56TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.053504
VERBOSE: Tactic: 0 Time: 0.144256
VERBOSE: Fastest Tactic: 1002 Time: 0.053504
VERBOSE: *************** Autotuning Reformat: Int8(294912,2304:4,48,1) -> Int8(36864,2304:32,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.56TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.050368
VERBOSE: Tactic: 0 Time: 0.04432
VERBOSE: Fastest Tactic: 0 Time: 0.04432
VERBOSE: *************** Autotuning Reformat: Int8(36864,2304:32,48,1) -> Int8(294912,2304:4,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.56TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.050048
VERBOSE: Tactic: 0 Time: 0.019584
VERBOSE: Fastest Tactic: 0 Time: 0.019584
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(589824,2304,48,1) -> Float(18432,2304:32,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Float(18432,2304:32,48,1) -> Float(589824,2304,48,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(18432,2304:32,48,1) -> Float(589824,2304,48,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(589824,2304,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(589824,2304,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(147456,2304:4,48,1) -> Int8(589824,2304,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(147456,2304:4,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(18432,2304:32,48,1) -> Int8(589824,2304,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(18432,2304:32,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(589824,2304,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(589824,2304,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(147456,2304:4,48,1) -> Int8(589824,2304,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(147456,2304:4,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(18432,2304:32,48,1) -> Int8(589824,2304,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(18432,2304:32,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(589824,2304,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(589824,2304,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(147456,2304:4,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(18432,2304:32,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(147456,2304:4,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(18432,2304:32,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(589824,2304,48,1) -> Float(18432,2304:32,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Float(18432,2304:32,48,1) -> Float(589824,2304,48,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(589824,2304,48,1) -> Float(18432,2304:32,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Float(18432,2304:32,48,1) -> Float(589824,2304,48,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(18432,2304:32,48,1) -> Float(589824,2304,48,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(589824,2304,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(589824,2304,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(147456,2304:4,48,1) -> Int8(589824,2304,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(147456,2304:4,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(18432,2304:32,48,1) -> Int8(589824,2304,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(18432,2304:32,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(589824,2304,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(589824,2304,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(147456,2304:4,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(18432,2304:32,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(18432,2304:32,48,1) -> Float(589824,2304,48,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(2064384,2304,48,1) -> Int8(516096,2304:4,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 760) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.032256
VERBOSE: Tactic: 0 Time: 0.017184
VERBOSE: Fastest Tactic: 0 Time: 0.017184
VERBOSE: *************** Autotuning Reformat: Int8(2064384,2304,48,1) -> Int8(64512,2304:32,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 760) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.034208
VERBOSE: Tactic: 0 Time: 0.076704
VERBOSE: Fastest Tactic: 1002 Time: 0.034208
VERBOSE: *************** Autotuning Reformat: Int8(516096,2304:4,48,1) -> Int8(2064384,2304,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 760) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.033984
VERBOSE: Tactic: 0 Time: 0.049184
VERBOSE: Fastest Tactic: 1002 Time: 0.033984
VERBOSE: *************** Autotuning Reformat: Int8(516096,2304:4,48,1) -> Int8(64512,2304:32,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 760) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.029728
VERBOSE: Tactic: 0 Time: 0.025664
VERBOSE: Fastest Tactic: 0 Time: 0.025664
VERBOSE: *************** Autotuning Reformat: Int8(64512,2304:32,48,1) -> Int8(2064384,2304,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 760) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.07248
VERBOSE: Tactic: 0 Time: 0.052576
VERBOSE: Fastest Tactic: 0 Time: 0.052576
VERBOSE: *************** Autotuning Reformat: Int8(64512,2304:32,48,1) -> Int8(516096,2304:4,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 760) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.029728
VERBOSE: Tactic: 0 Time: 0.013824
VERBOSE: Fastest Tactic: 0 Time: 0.013824
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(1179648,9216,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(1179648,9216,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(294912,9216:4,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(36864,9216:32,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(516096,2304:4,48,1) -> Int8(2064384,2304,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> inputs.15) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.022944
VERBOSE: Tactic: 0 Time: 0.028512
VERBOSE: Fastest Tactic: 1002 Time: 0.022944
VERBOSE: *************** Autotuning Reformat: Int8(516096,2304:4,48,1) -> Int8(64512,2304:32,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> inputs.15) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.019808
VERBOSE: Tactic: 0 Time: 0.016064
VERBOSE: Fastest Tactic: 0 Time: 0.016064
VERBOSE: *************** Autotuning Reformat: Int8(64512,2304:32,48,1) -> Int8(2064384,2304,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> inputs.15) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.042496
VERBOSE: Tactic: 0 Time: 0.030976
VERBOSE: Fastest Tactic: 0 Time: 0.030976
VERBOSE: *************** Autotuning Reformat: Int8(64512,2304:32,48,1) -> Int8(516096,2304:4,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> inputs.15) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.019808
VERBOSE: Tactic: 0 Time: 0.010688
VERBOSE: Fastest Tactic: 0 Time: 0.010688
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(18432,2304:32,48,1) -> Float(589824,2304,48,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(2064384,2304,48,1) -> Int8(516096,2304:4,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(2064384,2304,48,1) -> Int8(64512,2304:32,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(516096,2304:4,48,1) -> Int8(2064384,2304,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(516096,2304:4,48,1) -> Int8(64512,2304:32,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(64512,2304:32,48,1) -> Int8(2064384,2304,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(64512,2304:32,48,1) -> Int8(516096,2304:4,48,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(147456,2304:4,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(18432,2304:32,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(589824,2304,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(589824,2304,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(147456,2304:4,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(18432,2304:32,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(147456,2304:4,48,1) -> Int8(2064384,2304,48,1) ***************
VERBOSE: --------------- Timing Runner: 876 copy (Reformat)
VERBOSE: Tactic: 1002 Time: 0.033696
VERBOSE: Tactic: 0 Time: 0.049216
VERBOSE: Fastest Tactic: 1002 Time: 0.033696
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 1002
VERBOSE: *************** Autotuning Reformat: Int8(147456,2304:4,48,1) -> Int8(516096,2304:4,48,1) ***************
VERBOSE: --------------- Timing Runner: 876 copy (Reformat)
VERBOSE: Tactic: 1002 Time: 0.030016
VERBOSE: Tactic: 0 Time: 0.025728
VERBOSE: Fastest Tactic: 0 Time: 0.025728
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0
VERBOSE: *************** Autotuning Reformat: Int8(147456,2304:4,48,1) -> Int8(64512,2304:32,48,1) ***************
VERBOSE: --------------- Timing Runner: 876 copy (Reformat)
VERBOSE: Tactic: 1002 Time: 0.029952
VERBOSE: Tactic: 0 Time: 0.025664
VERBOSE: Fastest Tactic: 0 Time: 0.025664
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0
VERBOSE: *************** Autotuning Reformat: Int8(18432,2304:32,48,1) -> Int8(2064384,2304,48,1) ***************
VERBOSE: --------------- Timing Runner: 876 copy (Reformat)
VERBOSE: Tactic: 1002 Time: 0.073088
VERBOSE: Tactic: 0 Time: 0.052832
VERBOSE: Fastest Tactic: 0 Time: 0.052832
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0
VERBOSE: *************** Autotuning Reformat: Int8(18432,2304:32,48,1) -> Int8(516096,2304:4,48,1) ***************
VERBOSE: --------------- Timing Runner: 876 copy (Reformat)
VERBOSE: Tactic: 1002 Time: 0.029696
VERBOSE: Tactic: 0 Time: 0.013824
VERBOSE: Fastest Tactic: 0 Time: 0.013824
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0
VERBOSE: *************** Autotuning Reformat: Int8(18432,2304:32,48,1) -> Int8(64512,2304:32,48,1) ***************
VERBOSE: --------------- Timing Runner: 876 copy (Reformat)
VERBOSE: Tactic: 1002 Time: 0.022912
VERBOSE: Tactic: 0 Time: 0.014048
VERBOSE: Fastest Tactic: 0 Time: 0.014048
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(2064384,2304,48,1) -> Int8(516096,2304:4,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.66TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.07376
VERBOSE: Tactic: 0 Time: 0.043232
VERBOSE: Fastest Tactic: 0 Time: 0.043232
VERBOSE: *************** Autotuning Reformat: Int8(2064384,2304,48,1) -> Int8(64512,2304:32,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.66TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.083936
VERBOSE: Tactic: 0 Time: 0.247008
VERBOSE: Fastest Tactic: 1002 Time: 0.083936
VERBOSE: *************** Autotuning Reformat: Int8(516096,2304:4,48,1) -> Int8(64512,2304:32,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.66TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.082016
VERBOSE: Tactic: 0 Time: 0.072416
VERBOSE: Fastest Tactic: 0 Time: 0.072416
VERBOSE: *************** Autotuning Reformat: Int8(64512,2304:32,48,1) -> Int8(516096,2304:4,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.66TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.081472
VERBOSE: Tactic: 0 Time: 0.028096
VERBOSE: Fastest Tactic: 0 Time: 0.028096
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(589824,2304,48,1) -> Float(18432,2304:32,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Float(18432,2304:32,48,1) -> Float(589824,2304,48,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(18432,2304:32,48,1) -> Float(589824,2304,48,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(18432,2304:32,48,1) -> Float(589824,2304,48,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(589824,2304,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(589824,2304,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(147456,2304:4,48,1) -> Int8(589824,2304,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(147456,2304:4,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(18432,2304:32,48,1) -> Int8(589824,2304,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(18432,2304:32,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(589824,2304,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(589824,2304,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(147456,2304:4,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(18432,2304:32,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(589824,2304,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(589824,2304,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(147456,2304:4,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(18432,2304:32,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(147456,576,24,1) -> Int8(36864,576:4,24,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.68TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.018688
VERBOSE: Tactic: 0 Time: 0.009152
VERBOSE: Fastest Tactic: 0 Time: 0.009152
VERBOSE: *************** Autotuning Reformat: Int8(147456,576,24,1) -> Int8(4608,576:32,24,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.68TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.016416
VERBOSE: Tactic: 0 Time: 0.02608
VERBOSE: Fastest Tactic: 1002 Time: 0.016416
VERBOSE: *************** Autotuning Reformat: Int8(36864,576:4,24,1) -> Int8(4608,576:32,24,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.68TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.015008
VERBOSE: Tactic: 0 Time: 0.011552
VERBOSE: Fastest Tactic: 0 Time: 0.011552
VERBOSE: *************** Autotuning Reformat: Int8(4608,576:32,24,1) -> Int8(36864,576:4,24,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.68TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.01472
VERBOSE: Tactic: 0 Time: 0.009248
VERBOSE: Fastest Tactic: 0 Time: 0.009248
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(294912,2304,48,1) -> Float(294912,1,6144,128) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(1168 -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.027168
VERBOSE: Tactic: 0 Time: 0.036192
VERBOSE: Fastest Tactic: 1002 Time: 0.027168
VERBOSE: *************** Autotuning Reformat: Float(294912,1,6144,128) -> Float(294912,2304,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(1168 -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.029184
VERBOSE: Tactic: 0 Time: 0.0368
VERBOSE: Fastest Tactic: 1002 Time: 0.029184
VERBOSE: *************** Autotuning Reformat: Float(9216,2304:32,48,1) -> Float(294912,2304,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(1168 -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.029472
VERBOSE: Tactic: 0 Time: 0.037344
VERBOSE: Fastest Tactic: 1002 Time: 0.029472
VERBOSE: *************** Autotuning Reformat: Float(9216,2304:32,48,1) -> Float(294912,1,6144,128) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(1168 -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.02496
VERBOSE: Tactic: 0 Time: 0.022656
VERBOSE: Fastest Tactic: 0 Time: 0.022656
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(294912,576,24,1) -> Int8(73728,576:4,24,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(base_level5_project_1_output_quant.1TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.019808
VERBOSE: Tactic: 0 Time: 0.011776
VERBOSE: Fastest Tactic: 0 Time: 0.011776
VERBOSE: *************** Autotuning Reformat: Int8(294912,576,24,1) -> Int8(9216,576:32,24,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(base_level5_project_1_output_quant.1TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.02064
VERBOSE: Tactic: 0 Time: 0.043424
VERBOSE: Fastest Tactic: 1002 Time: 0.02064
VERBOSE: *************** Autotuning Reformat: Int8(73728,576:4,24,1) -> Int8(294912,576,24,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(base_level5_project_1_output_quant.1TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.022912
VERBOSE: Tactic: 0 Time: 0.029408
VERBOSE: Fastest Tactic: 1002 Time: 0.022912
VERBOSE: *************** Autotuning Reformat: Int8(73728,576:4,24,1) -> Int8(9216,576:32,24,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(base_level5_project_1_output_quant.1TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.019776
VERBOSE: Tactic: 0 Time: 0.016352
VERBOSE: Fastest Tactic: 0 Time: 0.016352
VERBOSE: *************** Autotuning Reformat: Int8(9216,576:32,24,1) -> Int8(294912,576,24,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(base_level5_project_1_output_quant.1TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.04272
VERBOSE: Tactic: 0 Time: 0.032128
VERBOSE: Fastest Tactic: 0 Time: 0.032128
VERBOSE: *************** Autotuning Reformat: Int8(9216,576:32,24,1) -> Int8(73728,576:4,24,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(base_level5_project_1_output_quant.1TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.019552
VERBOSE: Tactic: 0 Time: 0.01072
VERBOSE: Fastest Tactic: 0 Time: 0.01072
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(1179648,1,12288,128) -> Float(1179648,9216,96,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(dla_up_ida_1_up_1.1 -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.092384
VERBOSE: Tactic: 0 Time: 0.136352
VERBOSE: Fastest Tactic: 1002 Time: 0.092384
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(2359296,9216,96,1) -> Int8(589824,9216:4,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(2359296,9216,96,1) -> Int8(73728,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(589824,9216:4,96,1) -> Int8(2359296,9216,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(589824,9216:4,96,1) -> Int8(73728,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(73728,9216:32,96,1) -> Int8(2359296,9216,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(73728,9216:32,96,1) -> Int8(589824,9216:4,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(36864,9216:32,96,1) -> Float(1179648,9216,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(2359296,9216,96,1) -> Int8(589824,9216:4,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(2359296,9216,96,1) -> Int8(73728,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(589824,9216:4,96,1) -> Int8(2359296,9216,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(589824,9216:4,96,1) -> Int8(73728,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(73728,9216:32,96,1) -> Int8(2359296,9216,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(73728,9216:32,96,1) -> Int8(589824,9216:4,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(73728,576:4,24,1) -> Int8(9216,576:32,24,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(9216,576:32,24,1) -> Int8(73728,576:4,24,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(294912,576,24,1) -> Float(9216,576:32,24,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(base_level5_project_1_output_quant.1 -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.027552
VERBOSE: Tactic: 0 Time: 0.059904
VERBOSE: Fastest Tactic: 1002 Time: 0.027552
VERBOSE: *************** Autotuning Reformat: Float(9216,576:32,24,1) -> Float(294912,576,24,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(base_level5_project_1_output_quant.1 -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.028928
VERBOSE: Tactic: 0 Time: 0.037632
VERBOSE: Fastest Tactic: 1002 Time: 0.028928
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(294912,576,24,1) -> Float(9216,576:32,24,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 993) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.027232
VERBOSE: Tactic: 0 Time: 0.059744
VERBOSE: Fastest Tactic: 1002 Time: 0.027232
VERBOSE: *************** Autotuning Reformat: Float(9216,576:32,24,1) -> Float(294912,576,24,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 993) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.029216
VERBOSE: Tactic: 0 Time: 0.037664
VERBOSE: Fastest Tactic: 1002 Time: 0.029216
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(2359296,9216,96,1) -> Int8(589824,9216:4,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(2359296,9216,96,1) -> Int8(73728,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(589824,9216:4,96,1) -> Int8(73728,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(73728,9216:32,96,1) -> Int8(589824,9216:4,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(1179648,9216,96,1) -> Float(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Float(36864,9216:32,96,1) -> Float(1179648,9216,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(9216,576:32,24,1) -> Float(294912,576,24,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(294912,576,24,1) -> Int8(73728,576:4,24,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> base_level5_tree1_relu_output_quant.1TensorQ) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.01984
VERBOSE: Tactic: 0 Time: 0.011776
VERBOSE: Fastest Tactic: 0 Time: 0.011776
VERBOSE: *************** Autotuning Reformat: Int8(294912,576,24,1) -> Int8(9216,576:32,24,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> base_level5_tree1_relu_output_quant.1TensorQ) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.02064
VERBOSE: Tactic: 0 Time: 0.042848
VERBOSE: Fastest Tactic: 1002 Time: 0.02064
VERBOSE: *************** Autotuning Reformat: Int8(73728,576:4,24,1) -> Int8(294912,576,24,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> base_level5_tree1_relu_output_quant.1TensorQ) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.022912
VERBOSE: Tactic: 0 Time: 0.02944
VERBOSE: Fastest Tactic: 1002 Time: 0.022912
VERBOSE: *************** Autotuning Reformat: Int8(73728,576:4,24,1) -> Int8(9216,576:32,24,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> base_level5_tree1_relu_output_quant.1TensorQ) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.01984
VERBOSE: Tactic: 0 Time: 0.01632
VERBOSE: Fastest Tactic: 0 Time: 0.01632
VERBOSE: *************** Autotuning Reformat: Int8(9216,576:32,24,1) -> Int8(294912,576,24,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> base_level5_tree1_relu_output_quant.1TensorQ) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.043008
VERBOSE: Tactic: 0 Time: 0.032384
VERBOSE: Fastest Tactic: 0 Time: 0.032384
VERBOSE: *************** Autotuning Reformat: Int8(9216,576:32,24,1) -> Int8(73728,576:4,24,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> base_level5_tree1_relu_output_quant.1TensorQ) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.019776
VERBOSE: Tactic: 0 Time: 0.010688
VERBOSE: Fastest Tactic: 0 Time: 0.010688
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(36864,9216:32,96,1) -> Float(1179648,9216,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(294912,576,24,1) -> Int8(73728,576:4,24,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(294912,576,24,1) -> Int8(9216,576:32,24,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(73728,576:4,24,1) -> Int8(9216,576:32,24,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(9216,576:32,24,1) -> Int8(73728,576:4,24,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(1179648,9216,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(1179648,9216,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(294912,9216:4,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(36864,9216:32,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(589824,9216,96,1) -> Float(589824,1,6144,64) ***************
VERBOSE: *************** Autotuning Reformat: Float(589824,1,6144,64) -> Float(589824,9216,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Float(18432,9216:32,96,1) -> Float(589824,9216,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Float(18432,9216:32,96,1) -> Float(589824,1,6144,64) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(2359296,1,12288,64) -> Float(2359296,36864,192,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(4718592,36864,192,1) -> Int8(1179648,36864:4,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(4718592,36864,192,1) -> Int8(147456,36864:32,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(1179648,36864:4,192,1) -> Int8(4718592,36864,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(1179648,36864:4,192,1) -> Int8(147456,36864:32,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(147456,36864:32,192,1) -> Int8(4718592,36864,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(147456,36864:32,192,1) -> Int8(1179648,36864:4,192,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(4718592,36864,192,1) -> Int8(1179648,36864:4,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(4718592,36864,192,1) -> Int8(147456,36864:32,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(1179648,36864:4,192,1) -> Int8(147456,36864:32,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(147456,36864:32,192,1) -> Int8(1179648,36864:4,192,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(1179648,36864:4,192,1) -> Int8(4718592,36864,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(1179648,36864:4,192,1) -> Int8(147456,36864:32,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(147456,36864:32,192,1) -> Int8(4718592,36864,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(147456,36864:32,192,1) -> Int8(1179648,36864:4,192,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(737280,576,24,1) -> Int8(184320,576:4,24,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> inputs.21) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.018688
VERBOSE: Tactic: 0 Time: 0.008992
VERBOSE: Fastest Tactic: 0 Time: 0.008992
VERBOSE: *************** Autotuning Reformat: Int8(737280,576,24,1) -> Int8(23040,576:32,24,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> inputs.21) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.016416
VERBOSE: Tactic: 0 Time: 0.026112
VERBOSE: Fastest Tactic: 1002 Time: 0.016416
VERBOSE: *************** Autotuning Reformat: Int8(184320,576:4,24,1) -> Int8(737280,576,24,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> inputs.21) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.019552
VERBOSE: Tactic: 0 Time: 0.018624
VERBOSE: Fastest Tactic: 0 Time: 0.018624
VERBOSE: *************** Autotuning Reformat: Int8(184320,576:4,24,1) -> Int8(23040,576:32,24,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> inputs.21) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.015008
VERBOSE: Tactic: 0 Time: 0.011808
VERBOSE: Fastest Tactic: 0 Time: 0.011808
VERBOSE: *************** Autotuning Reformat: Int8(23040,576:32,24,1) -> Int8(737280,576,24,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> inputs.21) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.027776
VERBOSE: Tactic: 0 Time: 0.020736
VERBOSE: Fastest Tactic: 0 Time: 0.020736
VERBOSE: *************** Autotuning Reformat: Int8(23040,576:32,24,1) -> Int8(184320,576:4,24,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> inputs.21) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.01472
VERBOSE: Tactic: 0 Time: 0.00928
VERBOSE: Fastest Tactic: 0 Time: 0.00928
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(9216,576:32,24,1) -> Float(294912,576,24,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(737280,576,24,1) -> Int8(184320,576:4,24,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 993) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.019808
VERBOSE: Tactic: 0 Time: 0.011808
VERBOSE: Fastest Tactic: 0 Time: 0.011808
VERBOSE: *************** Autotuning Reformat: Int8(737280,576,24,1) -> Int8(23040,576:32,24,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 993) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.02064
VERBOSE: Tactic: 0 Time: 0.043168
VERBOSE: Fastest Tactic: 1002 Time: 0.02064
VERBOSE: *************** Autotuning Reformat: Int8(184320,576:4,24,1) -> Int8(737280,576,24,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 993) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.022976
VERBOSE: Tactic: 0 Time: 0.029376
VERBOSE: Fastest Tactic: 1002 Time: 0.022976
VERBOSE: *************** Autotuning Reformat: Int8(184320,576:4,24,1) -> Int8(23040,576:32,24,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 993) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.019776
VERBOSE: Tactic: 0 Time: 0.016608
VERBOSE: Fastest Tactic: 0 Time: 0.016608
VERBOSE: *************** Autotuning Reformat: Int8(23040,576:32,24,1) -> Int8(737280,576,24,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 993) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.042784
VERBOSE: Tactic: 0 Time: 0.032128
VERBOSE: Fastest Tactic: 0 Time: 0.032128
VERBOSE: *************** Autotuning Reformat: Int8(23040,576:32,24,1) -> Int8(184320,576:4,24,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> 993) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.019808
VERBOSE: Tactic: 0 Time: 0.010656
VERBOSE: Fastest Tactic: 0 Time: 0.010656
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(73728,576:4,24,1) -> Int8(9216,576:32,24,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(9216,576:32,24,1) -> Int8(73728,576:4,24,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(294912,576,24,1) -> Int8(73728,576:4,24,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(294912,576,24,1) -> Int8(9216,576:32,24,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(73728,576:4,24,1) -> Int8(9216,576:32,24,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(9216,576:32,24,1) -> Int8(73728,576:4,24,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(73728,576:4,24,1) -> Int8(737280,576,24,1) ***************
VERBOSE: --------------- Timing Runner: 1051 copy (Reformat)
VERBOSE: Tactic: 1002 Time: 0.022944
VERBOSE: Tactic: 0 Time: 0.029376
VERBOSE: Fastest Tactic: 1002 Time: 0.022944
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 1002
VERBOSE: *************** Autotuning Reformat: Int8(73728,576:4,24,1) -> Int8(184320,576:4,24,1) ***************
VERBOSE: --------------- Timing Runner: 1051 copy (Reformat)
VERBOSE: Tactic: 1002 Time: 0.019808
VERBOSE: Tactic: 0 Time: 0.016352
VERBOSE: Fastest Tactic: 0 Time: 0.016352
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0
VERBOSE: *************** Autotuning Reformat: Int8(73728,576:4,24,1) -> Int8(23040,576:32,24,1) ***************
VERBOSE: --------------- Timing Runner: 1051 copy (Reformat)
VERBOSE: Tactic: 1002 Time: 0.019808
VERBOSE: Tactic: 0 Time: 0.01632
VERBOSE: Fastest Tactic: 0 Time: 0.01632
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0
VERBOSE: *************** Autotuning Reformat: Int8(9216,576:32,24,1) -> Int8(737280,576,24,1) ***************
VERBOSE: --------------- Timing Runner: 1051 copy (Reformat)
VERBOSE: Tactic: 1002 Time: 0.042752
VERBOSE: Tactic: 0 Time: 0.032096
VERBOSE: Fastest Tactic: 0 Time: 0.032096
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0
VERBOSE: *************** Autotuning Reformat: Int8(9216,576:32,24,1) -> Int8(184320,576:4,24,1) ***************
VERBOSE: --------------- Timing Runner: 1051 copy (Reformat)
VERBOSE: Tactic: 1002 Time: 0.019776
VERBOSE: Tactic: 0 Time: 0.010688
VERBOSE: Fastest Tactic: 0 Time: 0.010688
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0
VERBOSE: *************** Autotuning Reformat: Int8(9216,576:32,24,1) -> Int8(23040,576:32,24,1) ***************
VERBOSE: --------------- Timing Runner: 1051 copy (Reformat)
VERBOSE: Tactic: 1002 Time: 0.015872
VERBOSE: Tactic: 0 Time: 0.010976
VERBOSE: Fastest Tactic: 0 Time: 0.010976
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(737280,576,24,1) -> Int8(184320,576:4,24,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.78TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.033696
VERBOSE: Tactic: 0 Time: 0.020288
VERBOSE: Fastest Tactic: 0 Time: 0.020288
VERBOSE: *************** Autotuning Reformat: Int8(737280,576,24,1) -> Int8(23040,576:32,24,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.78TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.032544
VERBOSE: Tactic: 0 Time: 0.094112
VERBOSE: Fastest Tactic: 1002 Time: 0.032544
VERBOSE: *************** Autotuning Reformat: Int8(184320,576:4,24,1) -> Int8(23040,576:32,24,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.78TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.034816
VERBOSE: Tactic: 0 Time: 0.03024
VERBOSE: Fastest Tactic: 0 Time: 0.03024
VERBOSE: *************** Autotuning Reformat: Int8(23040,576:32,24,1) -> Int8(184320,576:4,24,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.78TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.034528
VERBOSE: Tactic: 0 Time: 0.01552
VERBOSE: Fastest Tactic: 0 Time: 0.01552
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(73728,576:4,24,1) -> Int8(9216,576:32,24,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(9216,576:32,24,1) -> Int8(73728,576:4,24,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(147456,576,24,1) -> Float(147456,1,6144,256) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(1105 -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.026624
VERBOSE: Tactic: 0 Time: 0.022016
VERBOSE: Fastest Tactic: 0 Time: 0.022016
VERBOSE: *************** Autotuning Reformat: Float(147456,1,6144,256) -> Float(147456,576,24,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(1105 -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.027456
VERBOSE: Tactic: 0 Time: 0.02256
VERBOSE: Fastest Tactic: 0 Time: 0.02256
VERBOSE: *************** Autotuning Reformat: Float(4608,576:32,24,1) -> Float(147456,576,24,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(1105 -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.027744
VERBOSE: Tactic: 0 Time: 0.022944
VERBOSE: Fastest Tactic: 0 Time: 0.022944
VERBOSE: *************** Autotuning Reformat: Float(4608,576:32,24,1) -> Float(147456,1,6144,256) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(1105 -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.02464
VERBOSE: Tactic: 0 Time: 0.01472
VERBOSE: Fastest Tactic: 0 Time: 0.01472
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(589824,1,12288,256) -> Float(589824,2304,48,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(dla_up_ida_0_up_1.1 -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.060704
VERBOSE: Tactic: 0 Time: 0.066464
VERBOSE: Fastest Tactic: 1002 Time: 0.060704
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(1179648,2304,48,1) -> Int8(294912,2304:4,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(1179648,2304,48,1) -> Int8(36864,2304:32,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(294912,2304:4,48,1) -> Int8(1179648,2304,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(294912,2304:4,48,1) -> Int8(36864,2304:32,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(36864,2304:32,48,1) -> Int8(1179648,2304,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(36864,2304:32,48,1) -> Int8(294912,2304:4,48,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(18432,2304:32,48,1) -> Float(589824,2304,48,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(1179648,2304,48,1) -> Int8(294912,2304:4,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(1179648,2304,48,1) -> Int8(36864,2304:32,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(294912,2304:4,48,1) -> Int8(1179648,2304,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(294912,2304:4,48,1) -> Int8(36864,2304:32,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(36864,2304:32,48,1) -> Int8(1179648,2304,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(36864,2304:32,48,1) -> Int8(294912,2304:4,48,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(1179648,2304,48,1) -> Int8(294912,2304:4,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(1179648,2304,48,1) -> Int8(36864,2304:32,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(294912,2304:4,48,1) -> Int8(36864,2304:32,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(36864,2304:32,48,1) -> Int8(294912,2304:4,48,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(147456,2304:4,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(18432,2304:32,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(294912,2304,48,1) -> Float(294912,1,6144,128) ***************
VERBOSE: *************** Autotuning Reformat: Float(294912,1,6144,128) -> Float(294912,2304,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Float(9216,2304:32,48,1) -> Float(294912,2304,48,1) ***************
VERBOSE: *************** Autotuning Reformat: Float(9216,2304:32,48,1) -> Float(294912,1,6144,128) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(1179648,1,12288,128) -> Float(1179648,9216,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(2359296,9216,96,1) -> Int8(589824,9216:4,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(2359296,9216,96,1) -> Int8(73728,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(589824,9216:4,96,1) -> Int8(2359296,9216,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(589824,9216:4,96,1) -> Int8(73728,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(73728,9216:32,96,1) -> Int8(2359296,9216,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(73728,9216:32,96,1) -> Int8(589824,9216:4,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(36864,9216:32,96,1) -> Float(1179648,9216,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(2359296,9216,96,1) -> Int8(589824,9216:4,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(2359296,9216,96,1) -> Int8(73728,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(589824,9216:4,96,1) -> Int8(2359296,9216,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(589824,9216:4,96,1) -> Int8(73728,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(73728,9216:32,96,1) -> Int8(2359296,9216,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(73728,9216:32,96,1) -> Int8(589824,9216:4,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(2359296,9216,96,1) -> Int8(589824,9216:4,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(2359296,9216,96,1) -> Int8(73728,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(589824,9216:4,96,1) -> Int8(73728,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(73728,9216:32,96,1) -> Int8(589824,9216:4,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(294912,9216:4,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(36864,9216:32,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(589824,9216,96,1) -> Float(589824,1,6144,64) ***************
VERBOSE: *************** Autotuning Reformat: Float(589824,1,6144,64) -> Float(589824,9216,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Float(18432,9216:32,96,1) -> Float(589824,9216,96,1) ***************
VERBOSE: *************** Autotuning Reformat: Float(18432,9216:32,96,1) -> Float(589824,1,6144,64) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(2359296,1,12288,64) -> Float(2359296,36864,192,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(4718592,36864,192,1) -> Int8(1179648,36864:4,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(4718592,36864,192,1) -> Int8(147456,36864:32,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(1179648,36864:4,192,1) -> Int8(4718592,36864,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(1179648,36864:4,192,1) -> Int8(147456,36864:32,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(147456,36864:32,192,1) -> Int8(4718592,36864,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(147456,36864:32,192,1) -> Int8(1179648,36864:4,192,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(4718592,36864,192,1) -> Int8(1179648,36864:4,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(4718592,36864,192,1) -> Int8(147456,36864:32,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(1179648,36864:4,192,1) -> Int8(147456,36864:32,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(147456,36864:32,192,1) -> Int8(1179648,36864:4,192,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(589824,36864:4,192,1) -> Int8(73728,36864:32,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(73728,36864:32,192,1) -> Int8(589824,36864:4,192,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(589824,36864:4,192,1) -> Int8(73728,36864:32,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(73728,36864:32,192,1) -> Int8(589824,36864:4,192,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(589824,36864:4,192,1) -> Int8(73728,36864:32,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(73728,36864:32,192,1) -> Int8(589824,36864:4,192,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(589824,36864:4,192,1) -> Int8(73728,36864:32,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(73728,36864:32,192,1) -> Int8(589824,36864:4,192,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(2359296,36864:4,192,1) -> Int8(294912,36864:32,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.106TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.344992
VERBOSE: Tactic: 0 Time: 0.302432
VERBOSE: Fastest Tactic: 0 Time: 0.302432
VERBOSE: *************** Autotuning Reformat: Int8(294912,36864:32,192,1) -> Int8(2359296,36864:4,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(quant_input.106TensorQ -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.344032
VERBOSE: Tactic: 0 Time: 0.099008
VERBOSE: Fastest Tactic: 0 Time: 0.099008
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(2359296,36864:4,192,1) -> Int8(294912,36864:32,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(294912,36864:32,192,1) -> Int8(2359296,36864:4,192,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(331776,36864:32,192,1) -> Float(9584640,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> wh_2.1) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.123808
VERBOSE: Tactic: 0 Time: 0.02464
VERBOSE: Fastest Tactic: 0 Time: 0.02464
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Int8(2359296,36864:4,192,1) -> Int8(294912,36864:32,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Int8(294912,36864:32,192,1) -> Int8(2359296,36864:4,192,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(331776,36864:32,192,1) -> Float(9584640,36864,192,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(4718592,36864,192,1) -> Float(4718592,1,24576,128) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(hm_2.1 -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.310816
VERBOSE: Tactic: 0 Time: 0.466304
VERBOSE: Fastest Tactic: 1002 Time: 0.310816
VERBOSE: *************** Autotuning Reformat: Float(4718592,36864,192,1) -> Float(147456,36864:32,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(hm_2.1 -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.30736
VERBOSE: Tactic: 0 Time: 3.19238
VERBOSE: Fastest Tactic: 1002 Time: 0.30736
VERBOSE: *************** Autotuning Reformat: Float(4718592,36864,192,1) -> Float(1:4,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(hm_2.1 -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 1.0425
VERBOSE: Tactic: 0 Time: 1.35261
VERBOSE: Fastest Tactic: 1002 Time: 1.0425
VERBOSE: *************** Autotuning Reformat: Float(4718592,1,24576,128) -> Float(4718592,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(hm_2.1 -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.328544
VERBOSE: Tactic: 0 Time: 1.09014
VERBOSE: Fastest Tactic: 1002 Time: 0.328544
VERBOSE: *************** Autotuning Reformat: Float(4718592,1,24576,128) -> Float(147456,36864:32,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(hm_2.1 -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.289056
VERBOSE: Tactic: 0 Time: 4.83421
VERBOSE: Fastest Tactic: 1002 Time: 0.289056
VERBOSE: *************** Autotuning Reformat: Float(4718592,1,24576,128) -> Float(1:4,36864,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Float(147456,36864:32,192,1) -> Float(4718592,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(hm_2.1 -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.318048
VERBOSE: Tactic: 0 Time: 0.918272
VERBOSE: Fastest Tactic: 1002 Time: 0.318048
VERBOSE: *************** Autotuning Reformat: Float(147456,36864:32,192,1) -> Float(4718592,1,24576,128) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(hm_2.1 -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.284288
VERBOSE: Tactic: 0 Time: 0.279072
VERBOSE: Fastest Tactic: 0 Time: 0.279072
VERBOSE: *************** Autotuning Reformat: Float(147456,36864:32,192,1) -> Float(1:4,36864,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Float(1:4,36864,192,1) -> Float(4718592,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(hm_2.1 -> <out>) (Reformat)
VERBOSE: Tactic: 1002 Time: 14.3425
VERBOSE: Tactic: 0 Time: 0.355424
VERBOSE: Fastest Tactic: 0 Time: 0.355424
VERBOSE: *************** Autotuning Reformat: Float(1:4,36864,192,1) -> Float(4718592,1,24576,128) ***************
VERBOSE: *************** Autotuning Reformat: Float(1:4,36864,192,1) -> Float(147456,36864:32,192,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(4718592,36864,192,1) -> Float(4718592,1,24576,128) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> input11.1) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.300672
VERBOSE: Tactic: 0 Time: 0.46688
VERBOSE: Fastest Tactic: 1002 Time: 0.300672
VERBOSE: *************** Autotuning Reformat: Float(4718592,36864,192,1) -> Float(147456,36864:32,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> input11.1) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.30656
VERBOSE: Tactic: 0 Time: 3.19802
VERBOSE: Fastest Tactic: 1002 Time: 0.30656
VERBOSE: *************** Autotuning Reformat: Float(4718592,36864,192,1) -> Float(1:4,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> input11.1) (Reformat)
VERBOSE: Tactic: 1002 Time: 1.04291
VERBOSE: Tactic: 0 Time: 1.35238
VERBOSE: Fastest Tactic: 1002 Time: 1.04291
VERBOSE: *************** Autotuning Reformat: Float(4718592,1,24576,128) -> Float(4718592,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> input11.1) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.316608
VERBOSE: Tactic: 0 Time: 1.1039
VERBOSE: Fastest Tactic: 1002 Time: 0.316608
VERBOSE: *************** Autotuning Reformat: Float(4718592,1,24576,128) -> Float(147456,36864:32,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> input11.1) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.300992
VERBOSE: Tactic: 0 Time: 4.7496
VERBOSE: Fastest Tactic: 1002 Time: 0.300992
VERBOSE: *************** Autotuning Reformat: Float(4718592,1,24576,128) -> Float(1:4,36864,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Float(147456,36864:32,192,1) -> Float(4718592,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> input11.1) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.31088
VERBOSE: Tactic: 0 Time: 0.915616
VERBOSE: Fastest Tactic: 1002 Time: 0.31088
VERBOSE: *************** Autotuning Reformat: Float(147456,36864:32,192,1) -> Float(4718592,1,24576,128) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> input11.1) (Reformat)
VERBOSE: Tactic: 1002 Time: 0.280544
VERBOSE: Tactic: 0 Time: 0.278944
VERBOSE: Fastest Tactic: 0 Time: 0.278944
VERBOSE: *************** Autotuning Reformat: Float(147456,36864:32,192,1) -> Float(1:4,36864,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Float(1:4,36864,192,1) -> Float(4718592,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: Optimizer Reformat(<in> -> input11.1) (Reformat)
VERBOSE: Tactic: 1002 Time: 11.259
VERBOSE: Tactic: 0 Time: 0.353568
VERBOSE: Fastest Tactic: 0 Time: 0.353568
VERBOSE: *************** Autotuning Reformat: Float(1:4,36864,192,1) -> Float(4718592,1,24576,128) ***************
VERBOSE: *************** Autotuning Reformat: Float(1:4,36864,192,1) -> Float(147456,36864:32,192,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(4718592,1,24576,128) -> Float(4718592,36864,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Float(147456,36864:32,192,1) -> Float(4718592,36864,192,1) ***************
VERBOSE: *************** Autotuning Reformat: Float(1:4,36864,192,1) -> Float(4718592,36864,192,1) ***************
VERBOSE: =============== Computing reformatting costs
VERBOSE: =============== Computing reformatting costs
VERBOSE: *************** Autotuning Reformat: Float(4718592,36864,192,1) -> Float(9584640,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: input11.1 copy (Reformat)
VERBOSE: Tactic: 1002 Time: 0.201344
VERBOSE: Tactic: 0 Time: 0.157408
VERBOSE: Fastest Tactic: 0 Time: 0.157408
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0
VERBOSE: *************** Autotuning Reformat: Float(4718592,1,24576,128) -> Float(9584640,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: input11.1 copy (Reformat)
VERBOSE: Tactic: 1002 Time: 0.285536
VERBOSE: Tactic: 0 Time: 1.03312
VERBOSE: Fastest Tactic: 1002 Time: 0.285536
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 1002
VERBOSE: *************** Autotuning Reformat: Float(147456,36864:32,192,1) -> Float(9584640,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: input11.1 copy (Reformat)
VERBOSE: Tactic: 1002 Time: 0.273504
VERBOSE: Tactic: 0 Time: 0.827648
VERBOSE: Fastest Tactic: 1002 Time: 0.273504
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 1002
VERBOSE: *************** Autotuning Reformat: Float(1:4,36864,192,1) -> Float(9584640,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: input11.1 copy (Reformat)
VERBOSE: Tactic: 1002 Time: 7.57581
VERBOSE: Tactic: 0 Time: 0.353408
VERBOSE: Fastest Tactic: 0 Time: 0.353408
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(1769472,589824,768,1) -> Int8(1769472,589824,768,1) ***************
VERBOSE: --------------- Timing Runner: quant_input.2TensorQ (Scale)
VERBOSE: Tactic: 0 Time: 0.04032
VERBOSE: Fastest Tactic: 0 Time: 0.04032
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: *************** Autotuning format combination: Float(1769472,589824,768,1) -> Int8(589824,589824:4,768,1) ***************
VERBOSE: --------------- Timing Runner: quant_input.2TensorQ (Scale)
VERBOSE: Tactic: 0 Time: 0.044352
VERBOSE: Fastest Tactic: 0 Time: 0.044352
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: *************** Autotuning format combination: Float(1769472,589824,768,1) -> Int8(589824,589824:32,768,1) ***************
VERBOSE: --------------- Timing Runner: quant_input.2TensorQ (Scale)
VERBOSE: Tactic: 0 Time: 0.224
VERBOSE: Fastest Tactic: 0 Time: 0.224
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(1769472,589824,768,1) -> Int8(589824,589824:32,768,1) ***************
VERBOSE: --------------- Timing Runner: self.base.base_layer.0.weight + quant_weight.2ChannelQ + input.5 (CaskConvolution)
VERBOSE: CaskConvolution has no valid tactics for this config, skipping
VERBOSE: *************** Autotuning format combination: Int8(589824,589824:4,768,1) -> Int8(2359296,589824:4,768,1) ***************
VERBOSE: --------------- Timing Runner: self.base.base_layer.0.weight + quant_weight.2ChannelQ + input.5 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.base_layer.0.weight + quant_weight.2ChannelQ + input.5 (FusedConvActConvolution)
VERBOSE: FusedConvActConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.base_layer.0.weight + quant_weight.2ChannelQ + input.5 (CaskConvolution)
VERBOSE: self.base.base_layer.0.weight + quant_weight.2ChannelQ + input.5 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 4438325421691896755
VERBOSE: Tactic: 4438325421691896755 Time: 0.792448
VERBOSE: self.base.base_layer.0.weight + quant_weight.2ChannelQ + input.5 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 4934335053031119367
VERBOSE: Tactic: 4934335053031119367 Time: 0.449504
VERBOSE: self.base.base_layer.0.weight + quant_weight.2ChannelQ + input.5 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 6797040896965118050
VERBOSE: Tactic: 6797040896965118050 Time: 1.52512
VERBOSE: self.base.base_layer.0.weight + quant_weight.2ChannelQ + input.5 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 8006952294591770973
VERBOSE: Tactic: 8006952294591770973 Time: 0.414528
VERBOSE: self.base.base_layer.0.weight + quant_weight.2ChannelQ + input.5 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: -5026383765466876607
VERBOSE: Tactic: -5026383765466876607 Time: 2.00016
VERBOSE: self.base.base_layer.0.weight + quant_weight.2ChannelQ + input.5 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: -1370999262391786833
VERBOSE: Tactic: -1370999262391786833 Time: 0.788672
VERBOSE: Fastest Tactic: 8006952294591770973 Time: 0.414528
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 8006952294591770973
VERBOSE: *************** Autotuning format combination: Int8(589824,589824:4,768,1) -> Int8(589824,589824:32,768,1) ***************
VERBOSE: --------------- Timing Runner: self.base.base_layer.0.weight + quant_weight.2ChannelQ + input.5 (CaskConvolution)
VERBOSE: self.base.base_layer.0.weight + quant_weight.2ChannelQ + input.5 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: 1213457772632185722
VERBOSE: Tactic: 1213457772632185722 Time: 0.440576
VERBOSE: self.base.base_layer.0.weight + quant_weight.2ChannelQ + input.5 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 1713441381477652893
VERBOSE: Tactic: 1713441381477652893 Time: 0.4096
VERBOSE: self.base.base_layer.0.weight + quant_weight.2ChannelQ + input.5 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_xregs_large_c32_nn_v1 Tactic: 7125598890155666458
VERBOSE: Tactic: 7125598890155666458 Time: 0.764032
VERBOSE: self.base.base_layer.0.weight + quant_weight.2ChannelQ + input.5 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: -3566249366964946311
VERBOSE: Tactic: -3566249366964946311 Time: 0.76224
VERBOSE: self.base.base_layer.0.weight + quant_weight.2ChannelQ + input.5 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: -2002418013575043687
VERBOSE: Tactic: -2002418013575043687 Time: 1.47392
VERBOSE: self.base.base_layer.0.weight + quant_weight.2ChannelQ + input.5 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_xregs_large_c32_nn_v1 Tactic: -1659631603542281459
VERBOSE: Tactic: -1659631603542281459 Time: 1.94227
VERBOSE: Fastest Tactic: 1713441381477652893 Time: 0.4096
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 1713441381477652893
VERBOSE: *************** Autotuning format combination: Int8(589824,589824:32,768,1) -> Int8(589824,589824:32,768,1) ***************
VERBOSE: --------------- Timing Runner: self.base.base_layer.0.weight + quant_weight.2ChannelQ + input.5 (CudaGroupConvolution)
VERBOSE: CudaGroupConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.base_layer.0.weight + quant_weight.2ChannelQ + input.5 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.base_layer.0.weight + quant_weight.2ChannelQ + input.5 (FusedConvActConvolution)
VERBOSE: FusedConvActConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.base_layer.0.weight + quant_weight.2ChannelQ + input.5 (CaskConvolution)
VERBOSE: self.base.base_layer.0.weight + quant_weight.2ChannelQ + input.5 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 1230105269624924765
VERBOSE: Tactic: 1230105269624924765 Time: 1.5777
VERBOSE: self.base.base_layer.0.weight + quant_weight.2ChannelQ + input.5 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: 4099749238296892232
VERBOSE: Tactic: 4099749238296892232 Time: 1.57306
VERBOSE: self.base.base_layer.0.weight + quant_weight.2ChannelQ + input.5 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: -2965659229411050116
VERBOSE: Tactic: -2965659229411050116 Time: 2.06045
VERBOSE: self.base.base_layer.0.weight + quant_weight.2ChannelQ + input.5 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -1841683966837205309
VERBOSE: Tactic: -1841683966837205309 Time: 2.8799
VERBOSE: self.base.base_layer.0.weight + quant_weight.2ChannelQ + input.5 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -1405083410359886949
VERBOSE: Tactic: -1405083410359886949 Time: 2.03962
VERBOSE: self.base.base_layer.0.weight + quant_weight.2ChannelQ + input.5 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -1044618751912149244
VERBOSE: Tactic: -1044618751912149244 Time: 2.90947
VERBOSE: self.base.base_layer.0.weight + quant_weight.2ChannelQ + input.5 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -496455309852654971
VERBOSE: Tactic: -496455309852654971 Time: 2.79667
VERBOSE: self.base.base_layer.0.weight + quant_weight.2ChannelQ + input.5 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -409197672346571968
VERBOSE: Tactic: -409197672346571968 Time: 2.47907
VERBOSE: Fastest Tactic: 4099749238296892232 Time: 1.57306
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 4099749238296892232
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(2359296,589824:4,768,1) -> Int8(2359296,589824:4,768,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 (FusedConvActConvolution)
VERBOSE: Tactic: 524287 Time: 0.390432
VERBOSE: Tactic: 720895 Time: 0.55536
VERBOSE: Tactic: 983039 Time: 0.575872
VERBOSE: Tactic: 1048575 Time: 0.387136
VERBOSE: Tactic: 1703935 Time: 0.448448
VERBOSE: Tactic: 3014655 Time: 0.430144
VERBOSE: Tactic: 3604479 Time: 0.419488
VERBOSE: Tactic: 5046271 Time: 0.343072
VERBOSE: Tactic: 6488063 Time: 0.402112
VERBOSE: Tactic: 9043967 Time: 0.448256
VERBOSE: Tactic: 10027007 Time: 0.348576
VERBOSE: Tactic: 10485759 Time: 0.404256
VERBOSE: Tactic: 10813439 Time: 0.564576
VERBOSE: Fastest Tactic: 5046271 Time: 0.343072
VERBOSE: --------------- Timing Runner: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 (CaskConvolution)
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 4438325421691896755
VERBOSE: Tactic: 4438325421691896755 Time: 0.513248
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 4581732244273465060
VERBOSE: Tactic: 4581732244273465060 Time: 0.507424
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 4934335053031119367
VERBOSE: Tactic: 4934335053031119367 Time: 0.311008
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 6797040896965118050
VERBOSE: Tactic: 6797040896965118050 Time: 0.947808
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 8006952294591770973
VERBOSE: Tactic: 8006952294591770973 Time: 0.25984
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -7210942453088153035
VERBOSE: Tactic: -7210942453088153035 Time: 0.950464
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -6282183216199417697
VERBOSE: Tactic: -6282183216199417697 Time: 0.251648
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: -5026383765466876607
VERBOSE: Tactic: -5026383765466876607 Time: 1.34691
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: -5016725782072253841
VERBOSE: Tactic: -5016725782072253841 Time: 0.291552
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: -1370999262391786833
VERBOSE: Tactic: -1370999262391786833 Time: 0.491168
VERBOSE: Fastest Tactic: -6282183216199417697 Time: 0.251648
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -6282183216199417697
VERBOSE: *************** Autotuning format combination: Int8(2359296,589824:4,768,1) -> Int8(589824,589824:32,768,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 (CaskConvolution)
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: 1213457772632185722
VERBOSE: Tactic: 1213457772632185722 Time: 0.305664
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 1713441381477652893
VERBOSE: Tactic: 1713441381477652893 Time: 0.274048
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_xregs_large_c32_nn_v1 Tactic: 7125598890155666458
VERBOSE: Tactic: 7125598890155666458 Time: 0.496128
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 8047041638267142825
VERBOSE: Tactic: 8047041638267142825 Time: 0.272128
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: -7846982807478255793
VERBOSE: Tactic: -7846982807478255793 Time: 0.475584
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: -6459719113600909000
VERBOSE: Tactic: -6459719113600909000 Time: 0.293632
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: -4573925292554651334
VERBOSE: Tactic: -4573925292554651334 Time: 0.923776
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: -3566249366964946311
VERBOSE: Tactic: -3566249366964946311 Time: 0.48112
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: -2002418013575043687
VERBOSE: Tactic: -2002418013575043687 Time: 0.923712
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_xregs_large_c32_nn_v1 Tactic: -1659631603542281459
VERBOSE: Tactic: -1659631603542281459 Time: 1.3007
VERBOSE: Fastest Tactic: 8047041638267142825 Time: 0.272128
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 8047041638267142825
VERBOSE: *************** Autotuning format combination: Int8(589824,589824:32,768,1) -> Int8(589824,589824:32,768,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 (CudaGroupConvolution)
VERBOSE: Tactic: 0 Time: 0.584928
VERBOSE: Tactic: 1 Time: 0.594752
VERBOSE: Tactic: 2 Time: 0.612864
VERBOSE: Tactic: 3 Time: 0.637248
VERBOSE: Tactic: 4 Time: 0.633248
VERBOSE: Tactic: 5 Time: 0.826048
VERBOSE: Tactic: 6 Time: 0.64304
VERBOSE: Tactic: 7 Time: 0.635712
VERBOSE: Tactic: 8 Time: 0.640608
VERBOSE: Tactic: 9 Time: 0.642464
VERBOSE: Tactic: 10 Time: 0.802592
VERBOSE: Tactic: 11 Time: 1.03389
VERBOSE: Tactic: 12 Time: 0.565504
VERBOSE: Tactic: 13 Time: 0.605088
VERBOSE: Tactic: 14 Time: 0.622368
VERBOSE: Tactic: 15 Time: 0.6336
VERBOSE: Tactic: 16 Time: 0.621568
VERBOSE: Tactic: 17 Time: 0.940096
VERBOSE: Tactic: 18 Time: 0.638144
VERBOSE: Tactic: 19 Time: 0.573984
VERBOSE: Tactic: 20 Time: 0.568512
VERBOSE: Tactic: 21 Time: 0.595616
VERBOSE: Tactic: 22 Time: 0.611264
VERBOSE: Tactic: 23 Time: 0.612832
VERBOSE: Tactic: 24 Time: 0.774048
VERBOSE: Tactic: 25 Time: 0.644736
VERBOSE: Tactic: 26 Time: 0.601312
VERBOSE: Tactic: 27 Time: 0.58352
VERBOSE: Tactic: 28 Time: 0.591936
VERBOSE: Tactic: 29 Time: 0.594592
VERBOSE: Tactic: 30 Time: 0.78384
VERBOSE: Tactic: 31 Time: 0.921088
VERBOSE: Tactic: 32 Time: 0.60528
VERBOSE: Tactic: 33 Time: 0.54384
VERBOSE: Tactic: 34 Time: 0.567552
VERBOSE: Tactic: 35 Time: 0.581056
VERBOSE: Tactic: 36 Time: 0.592128
VERBOSE: Tactic: 37 Time: 0.599936
VERBOSE: Tactic: 38 Time: 0.835776
VERBOSE: Tactic: 39 Time: 0.77872
VERBOSE: Tactic: 40 Time: 0.620672
VERBOSE: Tactic: 41 Time: 0.559168
VERBOSE: Tactic: 42 Time: 0.549152
VERBOSE: Tactic: 43 Time: 0.56928
VERBOSE: Tactic: 44 Time: 0.56368
VERBOSE: Tactic: 45 Time: 0.555008
VERBOSE: Tactic: 46 Time: 0.560096
VERBOSE: Tactic: 47 Time: 0.704192
VERBOSE: Tactic: 48 Time: 0.606464
VERBOSE: Tactic: 49 Time: 0.563104
VERBOSE: Tactic: 50 Time: 0.541472
VERBOSE: Tactic: 51 Time: 0.562112
VERBOSE: Tactic: 52 Time: 0.556224
VERBOSE: Tactic: 53 Time: 0.690432
VERBOSE: Tactic: 54 Time: 0.861408
VERBOSE: Tactic: 55 Time: 0.67072
VERBOSE: Tactic: 56 Time: 0.583904
VERBOSE: Tactic: 57 Time: 0.539552
VERBOSE: Tactic: 58 Time: 0.53344
VERBOSE: Tactic: 59 Time: 0.552928
VERBOSE: Tactic: 60 Time: 0.548128
VERBOSE: Tactic: 61 Time: 0.548256
VERBOSE: Tactic: 62 Time: 0.551584
VERBOSE: Fastest Tactic: 58 Time: 0.53344
VERBOSE: --------------- Timing Runner: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 (FusedConvActConvolution)
VERBOSE: Tactic: 524287 Time: 0.384192
VERBOSE: Tactic: 720895 Time: 0.546496
VERBOSE: Tactic: 983039 Time: 0.565504
VERBOSE: Tactic: 1048575 Time: 0.367168
VERBOSE: Tactic: 1703935 Time: 0.438944
VERBOSE: Tactic: 3014655 Time: 0.471936
VERBOSE: Tactic: 3604479 Time: 0.464672
VERBOSE: Tactic: 5046271 Time: 0.354304
VERBOSE: Tactic: 6488063 Time: 0.391584
VERBOSE: Tactic: 9043967 Time: 0.412288
VERBOSE: Tactic: 10027007 Time: 0.360576
VERBOSE: Tactic: 10485759 Time: 0.400064
VERBOSE: Tactic: 10813439 Time: 0.594336
VERBOSE: Fastest Tactic: 5046271 Time: 0.354304
VERBOSE: --------------- Timing Runner: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 (CaskConvolution)
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 289888059097454627
VERBOSE: Tactic: 289888059097454627 Time: 0.807872
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 680740992583869928
VERBOSE: Tactic: 680740992583869928 Time: 0.714464
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 1230105269624924765
VERBOSE: Tactic: 1230105269624924765 Time: 0.358272
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 1388866374720163187
VERBOSE: Tactic: 1388866374720163187 Time: 0.34368
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 2133329569091732311
VERBOSE: Tactic: 2133329569091732311 Time: 0.65792
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 2579824863892891529
VERBOSE: Tactic: 2579824863892891529 Time: 0.697152
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: 4099749238296892232
VERBOSE: Tactic: 4099749238296892232 Time: 0.349664
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 4214794893922618058
VERBOSE: Tactic: 4214794893922618058 Time: 0.630208
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 4384868749799132354
VERBOSE: Tactic: 4384868749799132354 Time: 0.69584
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 4871133328510103657
VERBOSE: Tactic: 4871133328510103657 Time: 0.337952
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 4931167631624420067
VERBOSE: Tactic: 4931167631624420067 Time: 1.492
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 5641967928706599451
VERBOSE: Tactic: 5641967928706599451 Time: 0.44784
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 6394572396369862482
VERBOSE: Tactic: 6394572396369862482 Time: 0.447936
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 7048234086361926570
VERBOSE: Tactic: 7048234086361926570 Time: 0.458912
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -9138158634332993439
VERBOSE: Tactic: -9138158634332993439 Time: 0.761824
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -9108166971364503411
VERBOSE: Tactic: -9108166971364503411 Time: 0.460736
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: -8861822316054763526
VERBOSE: Tactic: -8861822316054763526 Time: 0.822048
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: -8475551154769412306
VERBOSE: Tactic: -8475551154769412306 Time: 0.37408
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -8431788508843860955
VERBOSE: Tactic: -8431788508843860955 Time: 0.593792
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: -6371781333659293809
VERBOSE: Tactic: -6371781333659293809 Time: 0.387072
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -5932545395595141539
VERBOSE: Tactic: -5932545395595141539 Time: 0.533248
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: -5180570335464125033
VERBOSE: Tactic: -5180570335464125033 Time: 0.382496
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: -2965659229411050116
VERBOSE: Tactic: -2965659229411050116 Time: 0.553856
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -2499089240293650188
VERBOSE: Tactic: -2499089240293650188 Time: 0.680256
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -2328318099174473157
VERBOSE: Tactic: -2328318099174473157 Time: 0.399264
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -1841683966837205309
VERBOSE: Tactic: -1841683966837205309 Time: 0.73888
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -1405083410359886949
VERBOSE: Tactic: -1405083410359886949 Time: 0.543008
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -1044618751912149244
VERBOSE: Tactic: -1044618751912149244 Time: 0.607584
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -496455309852654971
VERBOSE: Tactic: -496455309852654971 Time: 0.587872
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -409197672346571968
VERBOSE: Tactic: -409197672346571968 Time: 0.734336
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: -366411318217594794
VERBOSE: Tactic: -366411318217594794 Time: 0.321568
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: -351548418071036983
VERBOSE: Tactic: -351548418071036983 Time: 1.49424
VERBOSE: Fastest Tactic: -366411318217594794 Time: 0.321568
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -366411318217594794
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(2359296,589824:4,768,1) -> Int8(1179648,147456:4,384,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 (FusedConvActConvolution)
VERBOSE: FusedConvActConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 (CaskConvolution)
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 4438325421691896755
VERBOSE: Tactic: 4438325421691896755 Time: 0.129792
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 4581732244273465060
VERBOSE: Tactic: 4581732244273465060 Time: 0.128256
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 4934335053031119367
VERBOSE: Tactic: 4934335053031119367 Time: 0.091168
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 6797040896965118050
VERBOSE: Tactic: 6797040896965118050 Time: 0.23984
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 8006952294591770973
VERBOSE: Tactic: 8006952294591770973 Time: 0.079712
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -7210942453088153035
VERBOSE: Tactic: -7210942453088153035 Time: 0.240512
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -6282183216199417697
VERBOSE: Tactic: -6282183216199417697 Time: 0.078336
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: -5026383765466876607
VERBOSE: Tactic: -5026383765466876607 Time: 0.33376
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: -5016725782072253841
VERBOSE: Tactic: -5016725782072253841 Time: 0.087968
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: -1370999262391786833
VERBOSE: Tactic: -1370999262391786833 Time: 0.132992
VERBOSE: Fastest Tactic: -6282183216199417697 Time: 0.078336
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -6282183216199417697
VERBOSE: *************** Autotuning format combination: Int8(2359296,589824:4,768,1) -> Int8(147456,147456:32,384,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 (CaskConvolution)
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: 1213457772632185722
VERBOSE: Tactic: 1213457772632185722 Time: 0.097472
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 1713441381477652893
VERBOSE: Tactic: 1713441381477652893 Time: 0.090848
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_xregs_large_c32_nn_v1 Tactic: 7125598890155666458
VERBOSE: Tactic: 7125598890155666458 Time: 0.130656
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 8047041638267142825
VERBOSE: Tactic: 8047041638267142825 Time: 0.089888
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: -7846982807478255793
VERBOSE: Tactic: -7846982807478255793 Time: 0.125856
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: -6459719113600909000
VERBOSE: Tactic: -6459719113600909000 Time: 0.095424
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: -4573925292554651334
VERBOSE: Tactic: -4573925292554651334 Time: 0.235104
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: -3566249366964946311
VERBOSE: Tactic: -3566249366964946311 Time: 0.12768
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: -2002418013575043687
VERBOSE: Tactic: -2002418013575043687 Time: 0.235584
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_xregs_large_c32_nn_v1 Tactic: -1659631603542281459
VERBOSE: Tactic: -1659631603542281459 Time: 0.323936
VERBOSE: Fastest Tactic: 8047041638267142825 Time: 0.089888
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 8047041638267142825
VERBOSE: *************** Autotuning format combination: Int8(589824,589824:32,768,1) -> Int8(147456,147456:32,384,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 (CudaGroupConvolution)
VERBOSE: CudaGroupConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 (FusedConvActConvolution)
VERBOSE: FusedConvActConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 (CaskConvolution)
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 289888059097454627
VERBOSE: Tactic: 289888059097454627 Time: 0.212064
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 680740992583869928
VERBOSE: Tactic: 680740992583869928 Time: 0.182816
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 1230105269624924765
VERBOSE: Tactic: 1230105269624924765 Time: 0.133568
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 1388866374720163187
VERBOSE: Tactic: 1388866374720163187 Time: 0.112608
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 2133329569091732311
VERBOSE: Tactic: 2133329569091732311 Time: 0.179232
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 2579824863892891529
VERBOSE: Tactic: 2579824863892891529 Time: 0.183936
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: 4099749238296892232
VERBOSE: Tactic: 4099749238296892232 Time: 0.137152
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 4214794893922618058
VERBOSE: Tactic: 4214794893922618058 Time: 0.174048
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 4384868749799132354
VERBOSE: Tactic: 4384868749799132354 Time: 0.183168
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 4871133328510103657
VERBOSE: Tactic: 4871133328510103657 Time: 0.132512
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 4931167631624420067
VERBOSE: Tactic: 4931167631624420067 Time: 0.380192
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 5641967928706599451
VERBOSE: Tactic: 5641967928706599451 Time: 0.129344
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 6394572396369862482
VERBOSE: Tactic: 6394572396369862482 Time: 0.130624
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 7048234086361926570
VERBOSE: Tactic: 7048234086361926570 Time: 0.131136
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -9138158634332993439
VERBOSE: Tactic: -9138158634332993439 Time: 0.205376
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -9108166971364503411
VERBOSE: Tactic: -9108166971364503411 Time: 0.134528
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: -8861822316054763526
VERBOSE: Tactic: -8861822316054763526 Time: 0.221536
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: -8475551154769412306
VERBOSE: Tactic: -8475551154769412306 Time: 0.167776
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -8431788508843860955
VERBOSE: Tactic: -8431788508843860955 Time: 0.16464
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: -6371781333659293809
VERBOSE: Tactic: -6371781333659293809 Time: 0.140576
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -5932545395595141539
VERBOSE: Tactic: -5932545395595141539 Time: 0.165504
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: -5180570335464125033
VERBOSE: Tactic: -5180570335464125033 Time: 0.164672
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: -2965659229411050116
VERBOSE: Tactic: -2965659229411050116 Time: 0.171392
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -2499089240293650188
VERBOSE: Tactic: -2499089240293650188 Time: 0.187072
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -2328318099174473157
VERBOSE: Tactic: -2328318099174473157 Time: 0.151936
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -1841683966837205309
VERBOSE: Tactic: -1841683966837205309 Time: 0.204352
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -1405083410359886949
VERBOSE: Tactic: -1405083410359886949 Time: 0.169792
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -1044618751912149244
VERBOSE: Tactic: -1044618751912149244 Time: 0.171168
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -496455309852654971
VERBOSE: Tactic: -496455309852654971 Time: 0.165216
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -409197672346571968
VERBOSE: Tactic: -409197672346571968 Time: 0.205824
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: -366411318217594794
VERBOSE: Tactic: -366411318217594794 Time: 0.11456
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: -351548418071036983
VERBOSE: Tactic: -351548418071036983 Time: 0.39952
VERBOSE: Fastest Tactic: 1388866374720163187 Time: 0.112608
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 1388866374720163187
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(1179648,147456:4,384,1) -> Int8(294912,36864:4,192,1) ***************
VERBOSE: --------------- Timing Runner: inputs.5 (TiledPooling)
VERBOSE: Tactic: 5505281 Time: 0.036832
VERBOSE: Tactic: 5570817 Time: 0.028928
VERBOSE: Tactic: 5636353 Time: 0.027552
VERBOSE: Tactic: 5701889 Time: 0.027328
VERBOSE: Tactic: 5767425 Time: 0.027392
VERBOSE: Tactic: 5832961 Time: 0.027616
VERBOSE: Tactic: 5898497 Time: 0.028832
VERBOSE: Tactic: 5964033 Time: 0.02848
VERBOSE: Tactic: 6029569 Time: 0.03616
VERBOSE: Tactic: 6095105 Time: 0.029888
VERBOSE: Tactic: 6160641 Time: 0.02864
VERBOSE: Tactic: 6226177 Time: 0.029184
VERBOSE: Tactic: 6291713 Time: 0.02896
VERBOSE: Tactic: 6357249 Time: 0.030112
VERBOSE: Tactic: 6422785 Time: 0.030144
VERBOSE: Tactic: 6488321 Time: 0.030528
VERBOSE: Fastest Tactic: 5701889 Time: 0.027328
VERBOSE: --------------- Timing Runner: inputs.5 (CudaPooling)
VERBOSE: Tactic: -3 Time: 0.026336
VERBOSE: Fastest Tactic: -3 Time: 0.026336
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CudaPooling Tactic: -3
VERBOSE: *************** Autotuning format combination: Int8(147456,147456:32,384,1) -> Int8(36864,36864:32,192,1) ***************
VERBOSE: --------------- Timing Runner: inputs.5 (TiledPooling)
VERBOSE: TiledPooling has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: inputs.5 (CudaPooling)
VERBOSE: Tactic: -4 Time: 0.026944
VERBOSE: Fastest Tactic: -4 Time: 0.026944
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CudaPooling Tactic: -4
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(1179648,147456:4,384,1) -> Int8(589824,36864:4,192,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 (FusedConvActConvolution)
VERBOSE: Tactic: 458751 Time: 0.13712
VERBOSE: Fastest Tactic: 458751 Time: 0.13712
VERBOSE: --------------- Timing Runner: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 (CaskConvolution)
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 4438325421691896755
VERBOSE: Tactic: 4438325421691896755 Time: 0.063456
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 4581732244273465060
VERBOSE: Tactic: 4581732244273465060 Time: 0.062656
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 4934335053031119367
VERBOSE: Tactic: 4934335053031119367 Time: 0.07136
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 6797040896965118050
VERBOSE: Tactic: 6797040896965118050 Time: 0.111264
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 8006952294591770973
VERBOSE: Tactic: 8006952294591770973 Time: 0.075264
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -7210942453088153035
VERBOSE: Tactic: -7210942453088153035 Time: 0.11104
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -6282183216199417697
VERBOSE: Tactic: -6282183216199417697 Time: 0.072864
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: -5026383765466876607
VERBOSE: Tactic: -5026383765466876607 Time: 0.135552
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: -5016725782072253841
VERBOSE: Tactic: -5016725782072253841 Time: 0.069536
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: -1370999262391786833
VERBOSE: Tactic: -1370999262391786833 Time: 0.0632
VERBOSE: Fastest Tactic: 4581732244273465060 Time: 0.062656
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 4581732244273465060
VERBOSE: *************** Autotuning format combination: Int8(1179648,147456:4,384,1) -> Int8(73728,36864:32,192,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 (CaskConvolution)
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: 1213457772632185722
VERBOSE: Tactic: 1213457772632185722 Time: 0.07392
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 1713441381477652893
VERBOSE: Tactic: 1713441381477652893 Time: 0.07712
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_xregs_large_c32_nn_v1 Tactic: 7125598890155666458
VERBOSE: Tactic: 7125598890155666458 Time: 0.064736
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 8047041638267142825
VERBOSE: Tactic: 8047041638267142825 Time: 0.075424
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: -7846982807478255793
VERBOSE: Tactic: -7846982807478255793 Time: 0.064
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: -6459719113600909000
VERBOSE: Tactic: -6459719113600909000 Time: 0.073152
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: -4573925292554651334
VERBOSE: Tactic: -4573925292554651334 Time: 0.111104
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: -3566249366964946311
VERBOSE: Tactic: -3566249366964946311 Time: 0.064224
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: -2002418013575043687
VERBOSE: Tactic: -2002418013575043687 Time: 0.111296
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_xregs_large_c32_nn_v1 Tactic: -1659631603542281459
VERBOSE: Tactic: -1659631603542281459 Time: 0.135136
VERBOSE: Fastest Tactic: -7846982807478255793 Time: 0.064
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -7846982807478255793
VERBOSE: *************** Autotuning format combination: Int8(147456,147456:32,384,1) -> Int8(73728,36864:32,192,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 (CudaGroupConvolution)
VERBOSE: CudaGroupConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 (FusedConvActConvolution)
VERBOSE: Tactic: 458751 Time: 0.133088
VERBOSE: Fastest Tactic: 458751 Time: 0.133088
VERBOSE: --------------- Timing Runner: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 (CaskConvolution)
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 289888059097454627
VERBOSE: Tactic: 289888059097454627 Time: 0.069696
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 680740992583869928
VERBOSE: Tactic: 680740992583869928 Time: 0.056736
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 1230105269624924765
VERBOSE: Tactic: 1230105269624924765 Time: 0.045792
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 1388866374720163187
VERBOSE: Tactic: 1388866374720163187 Time: 0.053408
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 2133329569091732311
VERBOSE: Tactic: 2133329569091732311 Time: 0.062304
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 2579824863892891529
VERBOSE: Tactic: 2579824863892891529 Time: 0.056192
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: 4099749238296892232
VERBOSE: Tactic: 4099749238296892232 Time: 0.046624
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 4214794893922618058
VERBOSE: Tactic: 4214794893922618058 Time: 0.061152
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 4384868749799132354
VERBOSE: Tactic: 4384868749799132354 Time: 0.055648
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 4871133328510103657
VERBOSE: Tactic: 4871133328510103657 Time: 0.04528
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 4931167631624420067
VERBOSE: Tactic: 4931167631624420067 Time: 0.107936
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 5641967928706599451
VERBOSE: Tactic: 5641967928706599451 Time: 0.066976
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 6394572396369862482
VERBOSE: Tactic: 6394572396369862482 Time: 0.0672
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 7048234086361926570
VERBOSE: Tactic: 7048234086361926570 Time: 0.042752
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -9138158634332993439
VERBOSE: Tactic: -9138158634332993439 Time: 0.064608
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -9108166971364503411
VERBOSE: Tactic: -9108166971364503411 Time: 0.043328
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: -8861822316054763526
VERBOSE: Tactic: -8861822316054763526 Time: 0.069632
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: -8475551154769412306
VERBOSE: Tactic: -8475551154769412306 Time: 0.054176
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -8431788508843860955
VERBOSE: Tactic: -8431788508843860955 Time: 0.055168
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: -6371781333659293809
VERBOSE: Tactic: -6371781333659293809 Time: 0.04928
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -5932545395595141539
VERBOSE: Tactic: -5932545395595141539 Time: 0.052832
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: -5180570335464125033
VERBOSE: Tactic: -5180570335464125033 Time: 0.055072
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: -2965659229411050116
VERBOSE: Tactic: -2965659229411050116 Time: 0.053376
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -2499089240293650188
VERBOSE: Tactic: -2499089240293650188 Time: 0.058944
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -2328318099174473157
VERBOSE: Tactic: -2328318099174473157 Time: 0.051264
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -1841683966837205309
VERBOSE: Tactic: -1841683966837205309 Time: 0.06416
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -1405083410359886949
VERBOSE: Tactic: -1405083410359886949 Time: 0.053792
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -1044618751912149244
VERBOSE: Tactic: -1044618751912149244 Time: 0.056768
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -496455309852654971
VERBOSE: Tactic: -496455309852654971 Time: 0.056096
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -409197672346571968
VERBOSE: Tactic: -409197672346571968 Time: 0.065952
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: -366411318217594794
VERBOSE: Tactic: -366411318217594794 Time: 0.053952
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: -351548418071036983
VERBOSE: Tactic: -351548418071036983 Time: 0.111488
VERBOSE: Fastest Tactic: 7048234086361926570 Time: 0.042752
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 7048234086361926570
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(294912,36864:4,192,1) -> Int8(589824,36864:4,192,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 (FusedConvActConvolution)
VERBOSE: Tactic: 589823 Time: 0.061024
VERBOSE: Tactic: 655359 Time: 0.047488
VERBOSE: Tactic: 786431 Time: 0.058784
VERBOSE: Tactic: 851967 Time: 0.094944
VERBOSE: Tactic: 1638399 Time: 0.082496
VERBOSE: Tactic: 1835007 Time: 0.060384
VERBOSE: Tactic: 3080191 Time: 0.060192
VERBOSE: Tactic: 3538943 Time: 0.057728
VERBOSE: Tactic: 4063231 Time: 0.092832
VERBOSE: Tactic: 4325375 Time: 0.062496
VERBOSE: Tactic: 5636095 Time: 0.093184
VERBOSE: Tactic: 6225919 Time: 0.05968
VERBOSE: Tactic: 6422527 Time: 0.0648
VERBOSE: Tactic: 7077887 Time: 0.05584
VERBOSE: Tactic: 7405567 Time: 0.079456
VERBOSE: Tactic: 7602175 Time: 0.057344
VERBOSE: Tactic: 7733247 Time: 0.055712
VERBOSE: Tactic: 8191999 Time: 0.064736
VERBOSE: Tactic: 8323071 Time: 0.061056
VERBOSE: Tactic: 8650751 Time: 0.062304
VERBOSE: Tactic: 10747903 Time: 0.055008
VERBOSE: Tactic: 10944511 Time: 0.05568
VERBOSE: Fastest Tactic: 655359 Time: 0.047488
VERBOSE: --------------- Timing Runner: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 (CaskConvolution)
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 3145259992339075399
VERBOSE: Tactic: 3145259992339075399 Time: 0.018784
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_interior_nn_v1 Tactic: 4000990898022781625
VERBOSE: Tactic: 4000990898022781625 Time: 0.032576
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 4438325421691896755
VERBOSE: Tactic: 4438325421691896755 Time: 0.0192
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 4581732244273465060
VERBOSE: Tactic: 4581732244273465060 Time: 0.01904
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 4934335053031119367
VERBOSE: Tactic: 4934335053031119367 Time: 0.022496
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 6797040896965118050
VERBOSE: Tactic: 6797040896965118050 Time: 0.033312
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 8006952294591770973
VERBOSE: Tactic: 8006952294591770973 Time: 0.020032
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 8097855305881829878
VERBOSE: Tactic: 8097855305881829878 Time: 0.019392
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -7210942453088153035
VERBOSE: Tactic: -7210942453088153035 Time: 0.03312
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -6282183216199417697
VERBOSE: Tactic: -6282183216199417697 Time: 0.019712
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: -5016725782072253841
VERBOSE: Tactic: -5016725782072253841 Time: 0.021984
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_interior_nn_v1 Tactic: -1543391652455542154
VERBOSE: Tactic: -1543391652455542154 Time: 0.021504
VERBOSE: Fastest Tactic: 3145259992339075399 Time: 0.018784
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 3145259992339075399
VERBOSE: *************** Autotuning format combination: Int8(294912,36864:4,192,1) -> Int8(73728,36864:32,192,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 (CaskConvolution)
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_interior_c32_nn_v1 Tactic: 1025026069226666066
VERBOSE: Tactic: 1025026069226666066 Time: 0.032128
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: 1213457772632185722
VERBOSE: Tactic: 1213457772632185722 Time: 0.022944
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 1713441381477652893
VERBOSE: Tactic: 1713441381477652893 Time: 0.020128
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_interior_c32_nn_v1 Tactic: 2339361327868109050
VERBOSE: Tactic: 2339361327868109050 Time: 0.019136
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 8047041638267142825
VERBOSE: Tactic: 8047041638267142825 Time: 0.01968
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: -7846982807478255793
VERBOSE: Tactic: -7846982807478255793 Time: 0.019232
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_interior_c32_nn_v1 Tactic: -7686150779628967382
VERBOSE: Tactic: -7686150779628967382 Time: 0.021984
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: -6459719113600909000
VERBOSE: Tactic: -6459719113600909000 Time: 0.022112
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: -4573925292554651334
VERBOSE: Tactic: -4573925292554651334 Time: 0.032128
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_interior_c32_nn_v1 Tactic: -4208188808979933945
VERBOSE: Tactic: -4208188808979933945 Time: 0.01952
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: -3566249366964946311
VERBOSE: Tactic: -3566249366964946311 Time: 0.019584
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: -2002418013575043687
VERBOSE: Tactic: -2002418013575043687 Time: 0.032672
VERBOSE: Fastest Tactic: 2339361327868109050 Time: 0.019136
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 2339361327868109050
VERBOSE: *************** Autotuning format combination: Int8(36864,36864:32,192,1) -> Int8(73728,36864:32,192,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 (CudaGroupConvolution)
VERBOSE: CudaGroupConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 (FusedConvActConvolution)
VERBOSE: Tactic: 589823 Time: 0.043968
VERBOSE: Tactic: 655359 Time: 0.034048
VERBOSE: Tactic: 786431 Time: 0.046944
VERBOSE: Tactic: 851967 Time: 0.084608
VERBOSE: Tactic: 1638399 Time: 0.070944
VERBOSE: Tactic: 1835007 Time: 0.049184
VERBOSE: Tactic: 3080191 Time: 0.048512
VERBOSE: Tactic: 3538943 Time: 0.043488
VERBOSE: Tactic: 4063231 Time: 0.083168
VERBOSE: Tactic: 4325375 Time: 0.049664
VERBOSE: Tactic: 5636095 Time: 0.083488
VERBOSE: Tactic: 6225919 Time: 0.045728
VERBOSE: Tactic: 6422527 Time: 0.051104
VERBOSE: Tactic: 7077887 Time: 0.04096
VERBOSE: Tactic: 7405567 Time: 0.064448
VERBOSE: Tactic: 7602175 Time: 0.046432
VERBOSE: Tactic: 7733247 Time: 0.038528
VERBOSE: Tactic: 8191999 Time: 0.056896
VERBOSE: Tactic: 8323071 Time: 0.050784
VERBOSE: Tactic: 8650751 Time: 0.046112
VERBOSE: Tactic: 10747903 Time: 0.039328
VERBOSE: Tactic: 10944511 Time: 0.042432
VERBOSE: Fastest Tactic: 655359 Time: 0.034048
VERBOSE: --------------- Timing Runner: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 (CaskConvolution)
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 68468667201176803
VERBOSE: Tactic: 68468667201176803 Time: 0.016896
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 1230105269624924765
VERBOSE: Tactic: 1230105269624924765 Time: 0.017536
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 2548946449357458230
VERBOSE: Tactic: 2548946449357458230 Time: 0.017472
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 3242897809704328258
VERBOSE: Tactic: 3242897809704328258 Time: 0.0168
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 3312456766204252694
VERBOSE: Tactic: 3312456766204252694 Time: 0.019776
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 3541919052468401776
VERBOSE: Tactic: 3541919052468401776 Time: 0.02416
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 3927509214678622419
VERBOSE: Tactic: 3927509214678622419 Time: 0.01504
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 4871133328510103657
VERBOSE: Tactic: 4871133328510103657 Time: 0.017344
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 8684013308930763400
VERBOSE: Tactic: 8684013308930763400 Time: 0.015936
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -9138158634332993439
VERBOSE: Tactic: -9138158634332993439 Time: 0.031296
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: -8943710627305202139
VERBOSE: Tactic: -8943710627305202139 Time: 0.024576
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: -8859846367886814331
VERBOSE: Tactic: -8859846367886814331 Time: 0.015872
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -8431788508843860955
VERBOSE: Tactic: -8431788508843860955 Time: 0.02672
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: -8382298409581540699
VERBOSE: Tactic: -8382298409581540699 Time: 0.021984
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r1s1 Tactic: -8172318747337038866
VERBOSE: Tactic: -8172318747337038866 Time: 0.029696
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: -7361755530333096258
VERBOSE: Tactic: -7361755530333096258 Time: 0.018816
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: -7106539943789766885
VERBOSE: Tactic: -7106539943789766885 Time: 0.050624
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: -6969478418607271266
VERBOSE: Tactic: -6969478418607271266 Time: 0.028288
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: -6346247605026339453
VERBOSE: Tactic: -6346247605026339453 Time: 0.022816
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -5932545395595141539
VERBOSE: Tactic: -5932545395595141539 Time: 0.024224
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: -5697614955743334137
VERBOSE: Tactic: -5697614955743334137 Time: 0.02544
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_interior_nt_v1 Tactic: -5428851623690564527
VERBOSE: Tactic: -5428851623690564527 Time: 0.023808
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: -5311474420963248369
VERBOSE: Tactic: -5311474420963248369 Time: 0.020096
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: -4260476497340370474
VERBOSE: Tactic: -4260476497340370474 Time: 0.022496
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r1s1 Tactic: -3531681826488401618
VERBOSE: Tactic: -3531681826488401618 Time: 0.053696
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_interior_nt_v1 Tactic: -3065443564418447033
VERBOSE: Tactic: -3065443564418447033 Time: 0.026272
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -1841683966837205309
VERBOSE: Tactic: -1841683966837205309 Time: 0.031168
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: -1494157908358500249
VERBOSE: Tactic: -1494157908358500249 Time: 0.01712
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -1405083410359886949
VERBOSE: Tactic: -1405083410359886949 Time: 0.024384
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_interior_nt_v1 Tactic: -870280893819213650
VERBOSE: Tactic: -870280893819213650 Time: 0.031072
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_interior_nt_v1 Tactic: -545024964146453661
VERBOSE: Tactic: -545024964146453661 Time: 0.017216
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -496455309852654971
VERBOSE: Tactic: -496455309852654971 Time: 0.027008
VERBOSE: Fastest Tactic: 3927509214678622419 Time: 0.01504
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 3927509214678622419
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(2359296,36864,192,1) -> Float(2359296,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: base_level2_project_1_output_quant.1 (Scale)
VERBOSE: Tactic: 0 Time: 0.06336
VERBOSE: Fastest Tactic: 0 Time: 0.06336
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: *************** Autotuning format combination: Int8(589824,36864:4,192,1) -> Float(2359296,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: base_level2_project_1_output_quant.1 (Scale)
VERBOSE: Tactic: 0 Time: 0.077792
VERBOSE: Fastest Tactic: 0 Time: 0.077792
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: *************** Autotuning format combination: Int8(73728,36864:32,192,1) -> Float(2359296,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: base_level2_project_1_output_quant.1 (Scale)
VERBOSE: Tactic: 0 Time: 0.070976
VERBOSE: Fastest Tactic: 0 Time: 0.070976
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(589824,36864:4,192,1), Float(2359296,36864,192,1) -> Float(2359296,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170 (CaskConvolution)
VERBOSE: self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 892787096507693407
VERBOSE: Tactic: 892787096507693407 Time: 0.220896
VERBOSE: self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 1204440019753223942
VERBOSE: Tactic: 1204440019753223942 Time: 0.152064
VERBOSE: self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 1659301557717208403
VERBOSE: Tactic: 1659301557717208403 Time: 0.152
VERBOSE: self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 2057291331119027912
VERBOSE: Tactic: 2057291331119027912 Time: 0.128896
VERBOSE: self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 3275977259705528576
VERBOSE: Tactic: 3275977259705528576 Time: 0.145024
VERBOSE: self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 5623454780463195174
VERBOSE: Tactic: 5623454780463195174 Time: 0.159008
VERBOSE: self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: 8930254200803946944
VERBOSE: Tactic: 8930254200803946944 Time: 0.139904
VERBOSE: self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: -9204333525109552344
VERBOSE: Tactic: -9204333525109552344 Time: 0.12736
VERBOSE: self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -4973811344878172338
VERBOSE: Tactic: -4973811344878172338 Time: 0.220896
VERBOSE: self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: -1228371230285617088
VERBOSE: Tactic: -1228371230285617088 Time: 0.282848
VERBOSE: Fastest Tactic: -9204333525109552344 Time: 0.12736
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -9204333525109552344
VERBOSE: *************** Autotuning format combination: Int8(73728,36864:32,192,1), Float(2359296,36864,192,1) -> Float(2359296,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170 (CaskConvolution)
VERBOSE: CaskConvolution has no valid tactics for this config, skipping
VERBOSE: *************** Autotuning format combination: Int8(73728,36864:32,192,1), Float(73728,36864:32,192,1) -> Float(73728,36864:32,192,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170 (CaskConvolution)
VERBOSE: self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: 394365917225754726
VERBOSE: Tactic: 394365917225754726 Time: 0.123264
VERBOSE: self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 924563784895318224
VERBOSE: Tactic: 924563784895318224 Time: 0.11584
VERBOSE: self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: 3451841782205411203
VERBOSE: Tactic: 3451841782205411203 Time: 0.116192
VERBOSE: self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 6096719469361499298
VERBOSE: Tactic: 6096719469361499298 Time: 0.116448
VERBOSE: self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: 7785217228143857868
VERBOSE: Tactic: 7785217228143857868 Time: 0.13616
VERBOSE: self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: 8315790488934712458
VERBOSE: Tactic: 8315790488934712458 Time: 0.118016
VERBOSE: self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: 9080823067063042887
VERBOSE: Tactic: 9080823067063042887 Time: 0.1312
VERBOSE: self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -8462194455331556195
VERBOSE: Tactic: -8462194455331556195 Time: 0.117792
VERBOSE: self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: -8165074686865110847
VERBOSE: Tactic: -8165074686865110847 Time: 0.116384
VERBOSE: self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -7638944668269666085
VERBOSE: Tactic: -7638944668269666085 Time: 0.136416
VERBOSE: self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: -7185527339793611699
VERBOSE: Tactic: -7185527339793611699 Time: 0.117024
VERBOSE: self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -5979101256828290173
VERBOSE: Tactic: -5979101256828290173 Time: 0.128448
VERBOSE: self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -5763174003249488100
VERBOSE: Tactic: -5763174003249488100 Time: 0.120192
VERBOSE: self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: -5612459945002849429
VERBOSE: Tactic: -5612459945002849429 Time: 0.129504
VERBOSE: self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -5250790590226149674
VERBOSE: Tactic: -5250790590226149674 Time: 0.138784
VERBOSE: self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -4911193113143178408
VERBOSE: Tactic: -4911193113143178408 Time: 0.137472
VERBOSE: self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -4831366370915083630
VERBOSE: Tactic: -4831366370915083630 Time: 0.116704
VERBOSE: self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -4563432698383308679
VERBOSE: Tactic: -4563432698383308679 Time: 0.135008
VERBOSE: self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -4067022988143035981
VERBOSE: Tactic: -4067022988143035981 Time: 0.120576
VERBOSE: self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -3936136542475827126
VERBOSE: Tactic: -3936136542475827126 Time: 0.125344
VERBOSE: self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -3784829056659735491
VERBOSE: Tactic: -3784829056659735491 Time: 0.115296
VERBOSE: self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -3720553804749394441
VERBOSE: Tactic: -3720553804749394441 Time: 0.13856
VERBOSE: self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -2697681528489059028
VERBOSE: Tactic: -2697681528489059028 Time: 0.128384
VERBOSE: self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: -733152064595858464
VERBOSE: Tactic: -733152064595858464 Time: 0.115552
VERBOSE: Fastest Tactic: -3784829056659735491 Time: 0.115296
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -3784829056659735491
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(2359296,36864,192,1) -> Int8(2359296,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: base_level2_tree1_relu_output_quant.1TensorQ (Scale)
VERBOSE: Tactic: 0 Time: 0.050912
VERBOSE: Fastest Tactic: 0 Time: 0.050912
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: *************** Autotuning format combination: Float(2359296,36864,192,1) -> Int8(589824,36864:4,192,1) ***************
VERBOSE: --------------- Timing Runner: base_level2_tree1_relu_output_quant.1TensorQ (Scale)
VERBOSE: Tactic: 0 Time: 0.060544
VERBOSE: Fastest Tactic: 0 Time: 0.060544
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: *************** Autotuning format combination: Float(2359296,36864,192,1) -> Int8(73728,36864:32,192,1) ***************
VERBOSE: --------------- Timing Runner: base_level2_tree1_relu_output_quant.1TensorQ (Scale)
VERBOSE: Tactic: 0 Time: 0.053792
VERBOSE: Fastest Tactic: 0 Time: 0.053792
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(589824,36864:4,192,1) -> Int8(589824,36864:4,192,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 (FusedConvActConvolution)
VERBOSE: Tactic: 524287 Time: 0.123072
VERBOSE: Tactic: 720895 Time: 0.113984
VERBOSE: Tactic: 983039 Time: 0.111168
VERBOSE: Tactic: 1048575 Time: 0.122624
VERBOSE: Tactic: 1703935 Time: 0.125696
VERBOSE: Tactic: 1769471 Time: 0.188352
VERBOSE: Tactic: 2424831 Time: 0.215104
VERBOSE: Tactic: 2621439 Time: 0.145216
VERBOSE: Tactic: 3014655 Time: 0.119488
VERBOSE: Tactic: 3604479 Time: 0.11728
VERBOSE: Tactic: 5046271 Time: 0.121888
VERBOSE: Tactic: 6488063 Time: 0.123872
VERBOSE: Tactic: 7274495 Time: 0.146752
VERBOSE: Tactic: 7864319 Time: 0.14384
VERBOSE: Tactic: 8847359 Time: 0.156384
VERBOSE: Tactic: 9043967 Time: 0.12496
VERBOSE: Tactic: 9961471 Time: 0.213664
VERBOSE: Tactic: 10027007 Time: 0.120192
VERBOSE: Tactic: 10485759 Time: 0.122144
VERBOSE: Tactic: 10682367 Time: 0.13952
VERBOSE: Tactic: 10813439 Time: 0.10752
VERBOSE: Fastest Tactic: 10813439 Time: 0.10752
VERBOSE: --------------- Timing Runner: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 (CaskConvolution)
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 4438325421691896755
VERBOSE: Tactic: 4438325421691896755 Time: 0.112704
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 4581732244273465060
VERBOSE: Tactic: 4581732244273465060 Time: 0.111456
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 4934335053031119367
VERBOSE: Tactic: 4934335053031119367 Time: 0.115136
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 6797040896965118050
VERBOSE: Tactic: 6797040896965118050 Time: 0.2048
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 8006952294591770973
VERBOSE: Tactic: 8006952294591770973 Time: 0.125344
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -7210942453088153035
VERBOSE: Tactic: -7210942453088153035 Time: 0.203744
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -6282183216199417697
VERBOSE: Tactic: -6282183216199417697 Time: 0.121472
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: -5026383765466876607
VERBOSE: Tactic: -5026383765466876607 Time: 0.230592
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: -5016725782072253841
VERBOSE: Tactic: -5016725782072253841 Time: 0.113568
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: -1370999262391786833
VERBOSE: Tactic: -1370999262391786833 Time: 0.110112
VERBOSE: Fastest Tactic: -1370999262391786833 Time: 0.110112
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: FusedConvActConvolution Tactic: 10813439
VERBOSE: *************** Autotuning format combination: Int8(589824,36864:4,192,1) -> Int8(73728,36864:32,192,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 (CaskConvolution)
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: 1213457772632185722
VERBOSE: Tactic: 1213457772632185722 Time: 0.114816
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 1713441381477652893
VERBOSE: Tactic: 1713441381477652893 Time: 0.125152
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_xregs_large_c32_nn_v1 Tactic: 7125598890155666458
VERBOSE: Tactic: 7125598890155666458 Time: 0.111168
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 8047041638267142825
VERBOSE: Tactic: 8047041638267142825 Time: 0.121632
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: -7846982807478255793
VERBOSE: Tactic: -7846982807478255793 Time: 0.112096
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: -6459719113600909000
VERBOSE: Tactic: -6459719113600909000 Time: 0.113248
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: -4573925292554651334
VERBOSE: Tactic: -4573925292554651334 Time: 0.203872
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: -3566249366964946311
VERBOSE: Tactic: -3566249366964946311 Time: 0.113376
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: -2002418013575043687
VERBOSE: Tactic: -2002418013575043687 Time: 0.204832
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_xregs_large_c32_nn_v1 Tactic: -1659631603542281459
VERBOSE: Tactic: -1659631603542281459 Time: 0.23088
VERBOSE: Fastest Tactic: 7125598890155666458 Time: 0.111168
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 7125598890155666458
VERBOSE: *************** Autotuning format combination: Int8(73728,36864:32,192,1) -> Int8(73728,36864:32,192,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 (CudaGroupConvolution)
VERBOSE: CudaGroupConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 (FusedConvActConvolution)
VERBOSE: Tactic: 524287 Time: 0.124512
VERBOSE: Tactic: 720895 Time: 0.110688
VERBOSE: Tactic: 983039 Time: 0.108768
VERBOSE: Tactic: 1048575 Time: 0.122304
VERBOSE: Tactic: 1703935 Time: 0.126912
VERBOSE: Tactic: 1769471 Time: 0.182144
VERBOSE: Tactic: 2424831 Time: 0.240512
VERBOSE: Tactic: 2621439 Time: 0.146304
VERBOSE: Tactic: 3014655 Time: 0.128
VERBOSE: Tactic: 3604479 Time: 0.127456
VERBOSE: Tactic: 5046271 Time: 0.120544
VERBOSE: Tactic: 6488063 Time: 0.125728
VERBOSE: Tactic: 7274495 Time: 0.144608
VERBOSE: Tactic: 7864319 Time: 0.160928
VERBOSE: Tactic: 8847359 Time: 0.15696
VERBOSE: Tactic: 9043967 Time: 0.124352
VERBOSE: Tactic: 9961471 Time: 0.233952
VERBOSE: Tactic: 10027007 Time: 0.12064
VERBOSE: Tactic: 10485759 Time: 0.121216
VERBOSE: Tactic: 10682367 Time: 0.145728
VERBOSE: Tactic: 10813439 Time: 0.106656
VERBOSE: Fastest Tactic: 10813439 Time: 0.106656
VERBOSE: --------------- Timing Runner: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 (CaskConvolution)
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 289888059097454627
VERBOSE: Tactic: 289888059097454627 Time: 0.083968
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 680740992583869928
VERBOSE: Tactic: 680740992583869928 Time: 0.080992
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 1230105269624924765
VERBOSE: Tactic: 1230105269624924765 Time: 0.045152
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 1388866374720163187
VERBOSE: Tactic: 1388866374720163187 Time: 0.074016
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 2133329569091732311
VERBOSE: Tactic: 2133329569091732311 Time: 0.076864
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 2579824863892891529
VERBOSE: Tactic: 2579824863892891529 Time: 0.085376
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: 4099749238296892232
VERBOSE: Tactic: 4099749238296892232 Time: 0.046176
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 4214794893922618058
VERBOSE: Tactic: 4214794893922618058 Time: 0.075328
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 4384868749799132354
VERBOSE: Tactic: 4384868749799132354 Time: 0.085088
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 4871133328510103657
VERBOSE: Tactic: 4871133328510103657 Time: 0.04544
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 4931167631624420067
VERBOSE: Tactic: 4931167631624420067 Time: 0.153888
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 5641967928706599451
VERBOSE: Tactic: 5641967928706599451 Time: 0.100512
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 6394572396369862482
VERBOSE: Tactic: 6394572396369862482 Time: 0.100928
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 7048234086361926570
VERBOSE: Tactic: 7048234086361926570 Time: 0.058144
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -9138158634332993439
VERBOSE: Tactic: -9138158634332993439 Time: 0.080864
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -9108166971364503411
VERBOSE: Tactic: -9108166971364503411 Time: 0.058848
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: -8861822316054763526
VERBOSE: Tactic: -8861822316054763526 Time: 0.085824
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: -8475551154769412306
VERBOSE: Tactic: -8475551154769412306 Time: 0.05184
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -8431788508843860955
VERBOSE: Tactic: -8431788508843860955 Time: 0.071296
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: -6371781333659293809
VERBOSE: Tactic: -6371781333659293809 Time: 0.052096
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -5932545395595141539
VERBOSE: Tactic: -5932545395595141539 Time: 0.058176
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: -5180570335464125033
VERBOSE: Tactic: -5180570335464125033 Time: 0.052352
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: -2965659229411050116
VERBOSE: Tactic: -2965659229411050116 Time: 0.06048
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -2499089240293650188
VERBOSE: Tactic: -2499089240293650188 Time: 0.081664
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -2328318099174473157
VERBOSE: Tactic: -2328318099174473157 Time: 0.053632
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -1841683966837205309
VERBOSE: Tactic: -1841683966837205309 Time: 0.078976
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -1405083410359886949
VERBOSE: Tactic: -1405083410359886949 Time: 0.05952
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -1044618751912149244
VERBOSE: Tactic: -1044618751912149244 Time: 0.0744
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -496455309852654971
VERBOSE: Tactic: -496455309852654971 Time: 0.071456
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -409197672346571968
VERBOSE: Tactic: -409197672346571968 Time: 0.079808
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: -366411318217594794
VERBOSE: Tactic: -366411318217594794 Time: 0.074912
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: -351548418071036983
VERBOSE: Tactic: -351548418071036983 Time: 0.157952
VERBOSE: Fastest Tactic: 1230105269624924765 Time: 0.045152
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 1230105269624924765
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(2359296,36864,192,1) -> Int8(4718592,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: quant_input.18TensorQ_clone_1 (Scale)
VERBOSE: Tactic: 0 Time: 0.051232
VERBOSE: Fastest Tactic: 0 Time: 0.051232
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: *************** Autotuning format combination: Float(2359296,36864,192,1) -> Int8(1179648,36864:4,192,1) ***************
VERBOSE: --------------- Timing Runner: quant_input.18TensorQ_clone_1 (Scale)
VERBOSE: Tactic: 0 Time: 0.060096
VERBOSE: Fastest Tactic: 0 Time: 0.060096
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: *************** Autotuning format combination: Float(2359296,36864,192,1) -> Int8(147456,36864:32,192,1) ***************
VERBOSE: --------------- Timing Runner: quant_input.18TensorQ_clone_1 (Scale)
VERBOSE: Tactic: 0 Time: 0.053952
VERBOSE: Fastest Tactic: 0 Time: 0.053952
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(589824,36864:4,192,1), Int8(589824,36864:4,192,1) -> Int8(589824,36864:4,192,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 (FusedConvActConvolution)
VERBOSE: FusedConvActConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 (CaskConvolution)
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 4438325421691896755
VERBOSE: Tactic: 4438325421691896755 Time: 0.118048
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 4581732244273465060
VERBOSE: Tactic: 4581732244273465060 Time: 0.1168
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 4934335053031119367
VERBOSE: Tactic: 4934335053031119367 Time: 0.12352
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 6797040896965118050
VERBOSE: Tactic: 6797040896965118050 Time: 0.21248
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 8006952294591770973
VERBOSE: Tactic: 8006952294591770973 Time: 0.133792
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -7210942453088153035
VERBOSE: Tactic: -7210942453088153035 Time: 0.211904
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -6282183216199417697
VERBOSE: Tactic: -6282183216199417697 Time: 0.13024
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: -5026383765466876607
VERBOSE: Tactic: -5026383765466876607 Time: 0.24512
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: -5016725782072253841
VERBOSE: Tactic: -5016725782072253841 Time: 0.12112
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: -1370999262391786833
VERBOSE: Tactic: -1370999262391786833 Time: 0.115552
VERBOSE: Fastest Tactic: -1370999262391786833 Time: 0.115552
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -1370999262391786833
VERBOSE: *************** Autotuning format combination: Int8(589824,36864:4,192,1), Int8(73728,36864:32,192,1) -> Int8(73728,36864:32,192,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 (CaskConvolution)
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: 1213457772632185722
VERBOSE: Tactic: 1213457772632185722 Time: 0.122976
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 1713441381477652893
VERBOSE: Tactic: 1713441381477652893 Time: 0.133216
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_xregs_large_c32_nn_v1 Tactic: 7125598890155666458
VERBOSE: Tactic: 7125598890155666458 Time: 0.116288
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 8047041638267142825
VERBOSE: Tactic: 8047041638267142825 Time: 0.130432
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: -7846982807478255793
VERBOSE: Tactic: -7846982807478255793 Time: 0.117024
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: -6459719113600909000
VERBOSE: Tactic: -6459719113600909000 Time: 0.12112
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: -4573925292554651334
VERBOSE: Tactic: -4573925292554651334 Time: 0.211456
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: -3566249366964946311
VERBOSE: Tactic: -3566249366964946311 Time: 0.117984
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: -2002418013575043687
VERBOSE: Tactic: -2002418013575043687 Time: 0.211936
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_xregs_large_c32_nn_v1 Tactic: -1659631603542281459
VERBOSE: Tactic: -1659631603542281459 Time: 0.245344
VERBOSE: Fastest Tactic: 7125598890155666458 Time: 0.116288
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 7125598890155666458
VERBOSE: *************** Autotuning format combination: Int8(73728,36864:32,192,1), Int8(73728,36864:32,192,1) -> Int8(73728,36864:32,192,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 (CudaGroupConvolution)
VERBOSE: CudaGroupConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 (FusedConvActConvolution)
VERBOSE: FusedConvActConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 (CaskConvolution)
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 289888059097454627
VERBOSE: Tactic: 289888059097454627 Time: 0.092
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 680740992583869928
VERBOSE: Tactic: 680740992583869928 Time: 0.08416
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 1230105269624924765
VERBOSE: Tactic: 1230105269624924765 Time: 0.052096
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 1388866374720163187
VERBOSE: Tactic: 1388866374720163187 Time: 0.07712
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 2133329569091732311
VERBOSE: Tactic: 2133329569091732311 Time: 0.084768
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 2579824863892891529
VERBOSE: Tactic: 2579824863892891529 Time: 0.087936
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: 4099749238296892232
VERBOSE: Tactic: 4099749238296892232 Time: 0.052832
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 4214794893922618058
VERBOSE: Tactic: 4214794893922618058 Time: 0.082336
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 4384868749799132354
VERBOSE: Tactic: 4384868749799132354 Time: 0.087424
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 4871133328510103657
VERBOSE: Tactic: 4871133328510103657 Time: 0.05184
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 4931167631624420067
VERBOSE: Tactic: 4931167631624420067 Time: 0.162624
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 5641967928706599451
VERBOSE: Tactic: 5641967928706599451 Time: 0.103264
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 6394572396369862482
VERBOSE: Tactic: 6394572396369862482 Time: 0.103072
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 7048234086361926570
VERBOSE: Tactic: 7048234086361926570 Time: 0.060672
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -9138158634332993439
VERBOSE: Tactic: -9138158634332993439 Time: 0.087584
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -9108166971364503411
VERBOSE: Tactic: -9108166971364503411 Time: 0.0616
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: -8861822316054763526
VERBOSE: Tactic: -8861822316054763526 Time: 0.096704
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: -8475551154769412306
VERBOSE: Tactic: -8475551154769412306 Time: 0.063296
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -8431788508843860955
VERBOSE: Tactic: -8431788508843860955 Time: 0.075616
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: -6371781333659293809
VERBOSE: Tactic: -6371781333659293809 Time: 0.059712
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -5932545395595141539
VERBOSE: Tactic: -5932545395595141539 Time: 0.064896
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: -5180570335464125033
VERBOSE: Tactic: -5180570335464125033 Time: 0.064864
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: -2965659229411050116
VERBOSE: Tactic: -2965659229411050116 Time: 0.067072
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -2499089240293650188
VERBOSE: Tactic: -2499089240293650188 Time: 0.085312
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -2328318099174473157
VERBOSE: Tactic: -2328318099174473157 Time: 0.061888
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -1841683966837205309
VERBOSE: Tactic: -1841683966837205309 Time: 0.085696
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -1405083410359886949
VERBOSE: Tactic: -1405083410359886949 Time: 0.065952
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -1044618751912149244
VERBOSE: Tactic: -1044618751912149244 Time: 0.079584
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -496455309852654971
VERBOSE: Tactic: -496455309852654971 Time: 0.075936
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -409197672346571968
VERBOSE: Tactic: -409197672346571968 Time: 0.086912
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: -366411318217594794
VERBOSE: Tactic: -366411318217594794 Time: 0.077504
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: -351548418071036983
VERBOSE: Tactic: -351548418071036983 Time: 0.168864
VERBOSE: Fastest Tactic: 4871133328510103657 Time: 0.05184
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 4871133328510103657
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(1179648,36864:4,192,1) -> Float(2359296,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 (CaskConvolution)
VERBOSE: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 892787096507693407
VERBOSE: Tactic: 892787096507693407 Time: 0.081568
VERBOSE: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 1204440019753223942
VERBOSE: Tactic: 1204440019753223942 Time: 0.090464
VERBOSE: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 1659301557717208403
VERBOSE: Tactic: 1659301557717208403 Time: 0.084736
VERBOSE: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 2057291331119027912
VERBOSE: Tactic: 2057291331119027912 Time: 0.0736
VERBOSE: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 3275977259705528576
VERBOSE: Tactic: 3275977259705528576 Time: 0.089024
VERBOSE: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 5623454780463195174
VERBOSE: Tactic: 5623454780463195174 Time: 0.08352
VERBOSE: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: -9204333525109552344
VERBOSE: Tactic: -9204333525109552344 Time: 0.072128
VERBOSE: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: -7924103240988931433
VERBOSE: Tactic: -7924103240988931433 Time: 0.07136
VERBOSE: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_xregs_interior_nn_v1 Tactic: -7489650117016530013
VERBOSE: Tactic: -7489650117016530013 Time: 0.0904
VERBOSE: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -4973811344878172338
VERBOSE: Tactic: -4973811344878172338 Time: 0.08032
VERBOSE: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: -3908975881807046106
VERBOSE: Tactic: -3908975881807046106 Time: 0.081568
VERBOSE: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_interior_nn_v1 Tactic: -1765942417666394360
VERBOSE: Tactic: -1765942417666394360 Time: 0.079424
VERBOSE: Fastest Tactic: -7924103240988931433 Time: 0.07136
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -7924103240988931433
VERBOSE: *************** Autotuning format combination: Int8(147456,36864:32,192,1) -> Float(2359296,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 (CaskConvolution)
VERBOSE: CaskConvolution has no valid tactics for this config, skipping
VERBOSE: *************** Autotuning format combination: Int8(147456,36864:32,192,1) -> Float(73728,36864:32,192,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 (CaskConvolution)
VERBOSE: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: 394365917225754726
VERBOSE: Tactic: 394365917225754726 Time: 0.0776
VERBOSE: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 924563784895318224
VERBOSE: Tactic: 924563784895318224 Time: 0.08352
VERBOSE: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_interior_nt_v1 Tactic: 4531028070024747144
VERBOSE: Tactic: 4531028070024747144 Time: 0.076448
VERBOSE: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_interior_nt_v1 Tactic: 4697540470896098800
VERBOSE: Tactic: 4697540470896098800 Time: 0.069696
VERBOSE: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 6096719469361499298
VERBOSE: Tactic: 6096719469361499298 Time: 0.086016
VERBOSE: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_interior_nt_v1 Tactic: 7469355608320515097
VERBOSE: Tactic: 7469355608320515097 Time: 0.076288
VERBOSE: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: 7785217228143857868
VERBOSE: Tactic: 7785217228143857868 Time: 0.075456
VERBOSE: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: 8315790488934712458
VERBOSE: Tactic: 8315790488934712458 Time: 0.07024
VERBOSE: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_interior_nt_v1 Tactic: 9221575372280690678
VERBOSE: Tactic: 9221575372280690678 Time: 0.078112
VERBOSE: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -8462194455331556195
VERBOSE: Tactic: -8462194455331556195 Time: 0.068384
VERBOSE: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -7638944668269666085
VERBOSE: Tactic: -7638944668269666085 Time: 0.074528
VERBOSE: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: -7185527339793611699
VERBOSE: Tactic: -7185527339793611699 Time: 0.083648
VERBOSE: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -5979101256828290173
VERBOSE: Tactic: -5979101256828290173 Time: 0.07904
VERBOSE: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -5250790590226149674
VERBOSE: Tactic: -5250790590226149674 Time: 0.07536
VERBOSE: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -4831366370915083630
VERBOSE: Tactic: -4831366370915083630 Time: 0.070688
VERBOSE: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -4563432698383308679
VERBOSE: Tactic: -4563432698383308679 Time: 0.075808
VERBOSE: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -3936136542475827126
VERBOSE: Tactic: -3936136542475827126 Time: 0.07776
VERBOSE: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -3784829056659735491
VERBOSE: Tactic: -3784829056659735491 Time: 0.069952
VERBOSE: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -2697681528489059028
VERBOSE: Tactic: -2697681528489059028 Time: 0.080192
VERBOSE: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_interior_nt_v1 Tactic: -2621193268472024213
VERBOSE: Tactic: -2621193268472024213 Time: 0.069408
VERBOSE: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_interior_nt_v1 Tactic: -2120707675073643088
VERBOSE: Tactic: -2120707675073643088 Time: 0.083552
VERBOSE: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: -733152064595858464
VERBOSE: Tactic: -733152064595858464 Time: 0.083136
VERBOSE: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_interior_nt_v1 Tactic: -706197824303187656
VERBOSE: Tactic: -706197824303187656 Time: 0.086112
VERBOSE: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_interior_nt_v1 Tactic: -214244313010793854
VERBOSE: Tactic: -214244313010793854 Time: 0.07616
VERBOSE: Fastest Tactic: -8462194455331556195 Time: 0.068384
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -8462194455331556195
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(2359296,36864,192,1) -> Int8(2359296,36864,192,1) ***************
VERBOSE: *************** Autotuning format combination: Float(2359296,36864,192,1) -> Int8(589824,36864:4,192,1) ***************
VERBOSE: *************** Autotuning format combination: Float(2359296,36864,192,1) -> Int8(73728,36864:32,192,1) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(2359296,36864,192,1) -> Int8(2359296,36864,192,1) ***************
VERBOSE: *************** Autotuning format combination: Float(2359296,36864,192,1) -> Int8(589824,36864:4,192,1) ***************
VERBOSE: *************** Autotuning format combination: Float(2359296,36864,192,1) -> Int8(73728,36864:32,192,1) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(589824,36864:4,192,1) -> Int8(147456,9216:4,96,1) ***************
VERBOSE: --------------- Timing Runner: inputs.11 (TiledPooling)
VERBOSE: Tactic: 5505281 Time: 0.01616
VERBOSE: Tactic: 5570817 Time: 0.012448
VERBOSE: Tactic: 5636353 Time: 0.0112
VERBOSE: Tactic: 5701889 Time: 0.011488
VERBOSE: Tactic: 5767425 Time: 0.011616
VERBOSE: Tactic: 5832961 Time: 0.011648
VERBOSE: Tactic: 5898497 Time: 0.012608
VERBOSE: Tactic: 5964033 Time: 0.011488
VERBOSE: Tactic: 6029569 Time: 0.01584
VERBOSE: Tactic: 6095105 Time: 0.011808
VERBOSE: Tactic: 6160641 Time: 0.01056
VERBOSE: Tactic: 6226177 Time: 0.011168
VERBOSE: Tactic: 6291713 Time: 0.011328
VERBOSE: Tactic: 6357249 Time: 0.011136
VERBOSE: Tactic: 6422785 Time: 0.012032
VERBOSE: Tactic: 6488321 Time: 0.011296
VERBOSE: Fastest Tactic: 6160641 Time: 0.01056
VERBOSE: --------------- Timing Runner: inputs.11 (CudaPooling)
VERBOSE: Tactic: -3 Time: 0.009536
VERBOSE: Fastest Tactic: -3 Time: 0.009536
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CudaPooling Tactic: -3
VERBOSE: *************** Autotuning format combination: Int8(73728,36864:32,192,1) -> Int8(18432,9216:32,96,1) ***************
VERBOSE: --------------- Timing Runner: inputs.11 (TiledPooling)
VERBOSE: TiledPooling has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: inputs.11 (CudaPooling)
VERBOSE: Tactic: -4 Time: 0.00944
VERBOSE: Fastest Tactic: -4 Time: 0.00944
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CudaPooling Tactic: -4
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(589824,36864:4,192,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 (FusedConvActConvolution)
VERBOSE: Tactic: 458751 Time: 0.105216
VERBOSE: Fastest Tactic: 458751 Time: 0.105216
VERBOSE: --------------- Timing Runner: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 (CaskConvolution)
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 4438325421691896755
VERBOSE: Tactic: 4438325421691896755 Time: 0.060928
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 4581732244273465060
VERBOSE: Tactic: 4581732244273465060 Time: 0.060096
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 4934335053031119367
VERBOSE: Tactic: 4934335053031119367 Time: 0.068832
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 6797040896965118050
VERBOSE: Tactic: 6797040896965118050 Time: 0.057312
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 8006952294591770973
VERBOSE: Tactic: 8006952294591770973 Time: 0.069088
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -7210942453088153035
VERBOSE: Tactic: -7210942453088153035 Time: 0.05712
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -6282183216199417697
VERBOSE: Tactic: -6282183216199417697 Time: 0.067744
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: -5026383765466876607
VERBOSE: Tactic: -5026383765466876607 Time: 0.060928
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: -5016725782072253841
VERBOSE: Tactic: -5016725782072253841 Time: 0.067072
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: -1370999262391786833
VERBOSE: Tactic: -1370999262391786833 Time: 0.059904
VERBOSE: Fastest Tactic: -7210942453088153035 Time: 0.05712
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -7210942453088153035
VERBOSE: *************** Autotuning format combination: Int8(589824,36864:4,192,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 (CaskConvolution)
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: 1213457772632185722
VERBOSE: Tactic: 1213457772632185722 Time: 0.06864
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 1713441381477652893
VERBOSE: Tactic: 1713441381477652893 Time: 0.06912
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_xregs_large_c32_nn_v1 Tactic: 7125598890155666458
VERBOSE: Tactic: 7125598890155666458 Time: 0.060064
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 8047041638267142825
VERBOSE: Tactic: 8047041638267142825 Time: 0.068256
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: -7846982807478255793
VERBOSE: Tactic: -7846982807478255793 Time: 0.060256
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: -6459719113600909000
VERBOSE: Tactic: -6459719113600909000 Time: 0.066656
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: -4573925292554651334
VERBOSE: Tactic: -4573925292554651334 Time: 0.05776
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: -3566249366964946311
VERBOSE: Tactic: -3566249366964946311 Time: 0.06144
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: -2002418013575043687
VERBOSE: Tactic: -2002418013575043687 Time: 0.058176
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_xregs_large_c32_nn_v1 Tactic: -1659631603542281459
VERBOSE: Tactic: -1659631603542281459 Time: 0.061632
VERBOSE: Fastest Tactic: -4573925292554651334 Time: 0.05776
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -4573925292554651334
VERBOSE: *************** Autotuning format combination: Int8(73728,36864:32,192,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 (CudaGroupConvolution)
VERBOSE: CudaGroupConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 (FusedConvActConvolution)
VERBOSE: Tactic: 458751 Time: 0.117472
VERBOSE: Fastest Tactic: 458751 Time: 0.117472
VERBOSE: --------------- Timing Runner: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 (CaskConvolution)
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 289888059097454627
VERBOSE: Tactic: 289888059097454627 Time: 0.024576
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 680740992583869928
VERBOSE: Tactic: 680740992583869928 Time: 0.030176
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 1230105269624924765
VERBOSE: Tactic: 1230105269624924765 Time: 0.027392
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 1388866374720163187
VERBOSE: Tactic: 1388866374720163187 Time: 0.047232
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 2133329569091732311
VERBOSE: Tactic: 2133329569091732311 Time: 0.026368
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 2579824863892891529
VERBOSE: Tactic: 2579824863892891529 Time: 0.0488
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: 4099749238296892232
VERBOSE: Tactic: 4099749238296892232 Time: 0.02768
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 4214794893922618058
VERBOSE: Tactic: 4214794893922618058 Time: 0.026272
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 4384868749799132354
VERBOSE: Tactic: 4384868749799132354 Time: 0.048576
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 4871133328510103657
VERBOSE: Tactic: 4871133328510103657 Time: 0.027392
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 4931167631624420067
VERBOSE: Tactic: 4931167631624420067 Time: 0.042912
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 5641967928706599451
VERBOSE: Tactic: 5641967928706599451 Time: 0.058976
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 6394572396369862482
VERBOSE: Tactic: 6394572396369862482 Time: 0.059456
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 7048234086361926570
VERBOSE: Tactic: 7048234086361926570 Time: 0.035968
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -9138158634332993439
VERBOSE: Tactic: -9138158634332993439 Time: 0.024288
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -9108166971364503411
VERBOSE: Tactic: -9108166971364503411 Time: 0.036064
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: -8861822316054763526
VERBOSE: Tactic: -8861822316054763526 Time: 0.02576
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: -8475551154769412306
VERBOSE: Tactic: -8475551154769412306 Time: 0.029728
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -8431788508843860955
VERBOSE: Tactic: -8431788508843860955 Time: 0.024544
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: -6371781333659293809
VERBOSE: Tactic: -6371781333659293809 Time: 0.030944
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -5932545395595141539
VERBOSE: Tactic: -5932545395595141539 Time: 0.031232
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: -5180570335464125033
VERBOSE: Tactic: -5180570335464125033 Time: 0.030016
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: -2965659229411050116
VERBOSE: Tactic: -2965659229411050116 Time: 0.032384
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -2499089240293650188
VERBOSE: Tactic: -2499089240293650188 Time: 0.030752
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -2328318099174473157
VERBOSE: Tactic: -2328318099174473157 Time: 0.031744
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -1841683966837205309
VERBOSE: Tactic: -1841683966837205309 Time: 0.023968
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -1405083410359886949
VERBOSE: Tactic: -1405083410359886949 Time: 0.031744
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -1044618751912149244
VERBOSE: Tactic: -1044618751912149244 Time: 0.02544
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -496455309852654971
VERBOSE: Tactic: -496455309852654971 Time: 0.02448
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -409197672346571968
VERBOSE: Tactic: -409197672346571968 Time: 0.024896
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: -366411318217594794
VERBOSE: Tactic: -366411318217594794 Time: 0.048
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: -351548418071036983
VERBOSE: Tactic: -351548418071036983 Time: 0.044352
VERBOSE: Fastest Tactic: -1841683966837205309 Time: 0.023968
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -1841683966837205309
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(147456,9216:4,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 (FusedConvActConvolution)
VERBOSE: Tactic: 589823 Time: 0.043744
VERBOSE: Tactic: 655359 Time: 0.031616
VERBOSE: Tactic: 786431 Time: 0.041536
VERBOSE: Tactic: 851967 Time: 0.0464
VERBOSE: Tactic: 1179647 Time: 0.045344
VERBOSE: Tactic: 1638399 Time: 0.050432
VERBOSE: Tactic: 1835007 Time: 0.042496
VERBOSE: Tactic: 3080191 Time: 0.033088
VERBOSE: Tactic: 3538943 Time: 0.035328
VERBOSE: Tactic: 3997695 Time: 0.045376
VERBOSE: Tactic: 4063231 Time: 0.045568
VERBOSE: Tactic: 4325375 Time: 0.040384
VERBOSE: Tactic: 4587519 Time: 0.0408
VERBOSE: Tactic: 4653055 Time: 0.04624
VERBOSE: Tactic: 4915199 Time: 0.040768
VERBOSE: Tactic: 5177343 Time: 0.048032
VERBOSE: Tactic: 5373951 Time: 0.052768
VERBOSE: Tactic: 5636095 Time: 0.04544
VERBOSE: Tactic: 5898239 Time: 0.045472
VERBOSE: Tactic: 6225919 Time: 0.037056
VERBOSE: Tactic: 6291455 Time: 0.045344
VERBOSE: Tactic: 6422527 Time: 0.03536
VERBOSE: Tactic: 6750207 Time: 0.046976
VERBOSE: Tactic: 7077887 Time: 0.034464
VERBOSE: Tactic: 7340031 Time: 0.046112
VERBOSE: Tactic: 7405567 Time: 0.047104
VERBOSE: Tactic: 7602175 Time: 0.039264
VERBOSE: Tactic: 7733247 Time: 0.034272
VERBOSE: Tactic: 8191999 Time: 0.046112
VERBOSE: Tactic: 8257535 Time: 0.042624
VERBOSE: Tactic: 8323071 Time: 0.04256
VERBOSE: Tactic: 8650751 Time: 0.043904
VERBOSE: Tactic: 9568255 Time: 0.0408
VERBOSE: Tactic: 10354687 Time: 0.0488
VERBOSE: Tactic: 10747903 Time: 0.033632
VERBOSE: Tactic: 10944511 Time: 0.037312
VERBOSE: Fastest Tactic: 655359 Time: 0.031616
VERBOSE: --------------- Timing Runner: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 (CaskConvolution)
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 3145259992339075399
VERBOSE: Tactic: 3145259992339075399 Time: 0.01472
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_interior_nn_v1 Tactic: 4000990898022781625
VERBOSE: Tactic: 4000990898022781625 Time: 0.014688
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 4438325421691896755
VERBOSE: Tactic: 4438325421691896755 Time: 0.015168
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 4581732244273465060
VERBOSE: Tactic: 4581732244273465060 Time: 0.015136
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 4934335053031119367
VERBOSE: Tactic: 4934335053031119367 Time: 0.01904
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 6797040896965118050
VERBOSE: Tactic: 6797040896965118050 Time: 0.014976
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 8006952294591770973
VERBOSE: Tactic: 8006952294591770973 Time: 0.01584
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 8097855305881829878
VERBOSE: Tactic: 8097855305881829878 Time: 0.015392
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -7210942453088153035
VERBOSE: Tactic: -7210942453088153035 Time: 0.014752
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -6282183216199417697
VERBOSE: Tactic: -6282183216199417697 Time: 0.015488
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: -5016725782072253841
VERBOSE: Tactic: -5016725782072253841 Time: 0.018496
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_interior_nn_v1 Tactic: -1543391652455542154
VERBOSE: Tactic: -1543391652455542154 Time: 0.018208
VERBOSE: Fastest Tactic: 4000990898022781625 Time: 0.014688
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 4000990898022781625
VERBOSE: *************** Autotuning format combination: Int8(147456,9216:4,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 (CaskConvolution)
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_interior_c32_nn_v1 Tactic: 1025026069226666066
VERBOSE: Tactic: 1025026069226666066 Time: 0.015808
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: 1213457772632185722
VERBOSE: Tactic: 1213457772632185722 Time: 0.018848
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 1713441381477652893
VERBOSE: Tactic: 1713441381477652893 Time: 0.015744
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_interior_c32_nn_v1 Tactic: 2339361327868109050
VERBOSE: Tactic: 2339361327868109050 Time: 0.014912
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 8047041638267142825
VERBOSE: Tactic: 8047041638267142825 Time: 0.01552
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: -7846982807478255793
VERBOSE: Tactic: -7846982807478255793 Time: 0.015104
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_interior_c32_nn_v1 Tactic: -7686150779628967382
VERBOSE: Tactic: -7686150779628967382 Time: 0.018144
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: -6459719113600909000
VERBOSE: Tactic: -6459719113600909000 Time: 0.018208
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: -4573925292554651334
VERBOSE: Tactic: -4573925292554651334 Time: 0.015776
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_interior_c32_nn_v1 Tactic: -4208188808979933945
VERBOSE: Tactic: -4208188808979933945 Time: 0.015104
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: -3566249366964946311
VERBOSE: Tactic: -3566249366964946311 Time: 0.015328
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: -2002418013575043687
VERBOSE: Tactic: -2002418013575043687 Time: 0.015648
VERBOSE: Fastest Tactic: 2339361327868109050 Time: 0.014912
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 2339361327868109050
VERBOSE: *************** Autotuning format combination: Int8(18432,9216:32,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 (CudaGroupConvolution)
VERBOSE: CudaGroupConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 (FusedConvActConvolution)
VERBOSE: Tactic: 589823 Time: 0.034304
VERBOSE: Tactic: 655359 Time: 0.019456
VERBOSE: Tactic: 786431 Time: 0.037248
VERBOSE: Tactic: 851967 Time: 0.039232
VERBOSE: Tactic: 1179647 Time: 0.039424
VERBOSE: Tactic: 1638399 Time: 0.0456
VERBOSE: Tactic: 1835007 Time: 0.03968
VERBOSE: Tactic: 3080191 Time: 0.025088
VERBOSE: Tactic: 3538943 Time: 0.02992
VERBOSE: Tactic: 3997695 Time: 0.040928
VERBOSE: Tactic: 4063231 Time: 0.037952
VERBOSE: Tactic: 4325375 Time: 0.035712
VERBOSE: Tactic: 4587519 Time: 0.037408
VERBOSE: Tactic: 4653055 Time: 0.040704
VERBOSE: Tactic: 4915199 Time: 0.037088
VERBOSE: Tactic: 5177343 Time: 0.038912
VERBOSE: Tactic: 5373951 Time: 0.043296
VERBOSE: Tactic: 5636095 Time: 0.03792
VERBOSE: Tactic: 5898239 Time: 0.038752
VERBOSE: Tactic: 6225919 Time: 0.031584
VERBOSE: Tactic: 6291455 Time: 0.039296
VERBOSE: Tactic: 6422527 Time: 0.02672
VERBOSE: Tactic: 6750207 Time: 0.044832
VERBOSE: Tactic: 7077887 Time: 0.028192
VERBOSE: Tactic: 7340031 Time: 0.037952
VERBOSE: Tactic: 7405567 Time: 0.040192
VERBOSE: Tactic: 7602175 Time: 0.035808
VERBOSE: Tactic: 7733247 Time: 0.025728
VERBOSE: Tactic: 8191999 Time: 0.046752
VERBOSE: Tactic: 8257535 Time: 0.035328
VERBOSE: Tactic: 8323071 Time: 0.04032
VERBOSE: Tactic: 8650751 Time: 0.035616
VERBOSE: Tactic: 9568255 Time: 0.037056
VERBOSE: Tactic: 10354687 Time: 0.043584
VERBOSE: Tactic: 10747903 Time: 0.026528
VERBOSE: Tactic: 10944511 Time: 0.032864
VERBOSE: Fastest Tactic: 655359 Time: 0.019456
VERBOSE: --------------- Timing Runner: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 (CaskConvolution)
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 68468667201176803
VERBOSE: Tactic: 68468667201176803 Time: 0.011552
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 1230105269624924765
VERBOSE: Tactic: 1230105269624924765 Time: 0.011904
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 2548946449357458230
VERBOSE: Tactic: 2548946449357458230 Time: 0.012736
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 3242897809704328258
VERBOSE: Tactic: 3242897809704328258 Time: 0.012352
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 3312456766204252694
VERBOSE: Tactic: 3312456766204252694 Time: 0.013056
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 3541919052468401776
VERBOSE: Tactic: 3541919052468401776 Time: 0.010912
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 3927509214678622419
VERBOSE: Tactic: 3927509214678622419 Time: 0.011104
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 4871133328510103657
VERBOSE: Tactic: 4871133328510103657 Time: 0.011872
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 8684013308930763400
VERBOSE: Tactic: 8684013308930763400 Time: 0.010912
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -9138158634332993439
VERBOSE: Tactic: -9138158634332993439 Time: 0.0112
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: -8943710627305202139
VERBOSE: Tactic: -8943710627305202139 Time: 0.011744
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: -8859846367886814331
VERBOSE: Tactic: -8859846367886814331 Time: 0.011424
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -8431788508843860955
VERBOSE: Tactic: -8431788508843860955 Time: 0.01152
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: -8382298409581540699
VERBOSE: Tactic: -8382298409581540699 Time: 0.01456
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r1s1 Tactic: -8172318747337038866
VERBOSE: Tactic: -8172318747337038866 Time: 0.01136
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: -7361755530333096258
VERBOSE: Tactic: -7361755530333096258 Time: 0.013536
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: -7106539943789766885
VERBOSE: Tactic: -7106539943789766885 Time: 0.017088
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: -6969478418607271266
VERBOSE: Tactic: -6969478418607271266 Time: 0.011072
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: -6346247605026339453
VERBOSE: Tactic: -6346247605026339453 Time: 0.011232
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -5932545395595141539
VERBOSE: Tactic: -5932545395595141539 Time: 0.01488
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: -5697614955743334137
VERBOSE: Tactic: -5697614955743334137 Time: 0.011552
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_interior_nt_v1 Tactic: -5428851623690564527
VERBOSE: Tactic: -5428851623690564527 Time: 0.014816
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: -5311474420963248369
VERBOSE: Tactic: -5311474420963248369 Time: 0.013248
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: -4260476497340370474
VERBOSE: Tactic: -4260476497340370474 Time: 0.014688
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r1s1 Tactic: -3531681826488401618
VERBOSE: Tactic: -3531681826488401618 Time: 0.017504
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_interior_nt_v1 Tactic: -3065443564418447033
VERBOSE: Tactic: -3065443564418447033 Time: 0.011584
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -1841683966837205309
VERBOSE: Tactic: -1841683966837205309 Time: 0.011392
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: -1494157908358500249
VERBOSE: Tactic: -1494157908358500249 Time: 0.011936
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -1405083410359886949
VERBOSE: Tactic: -1405083410359886949 Time: 0.014912
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_interior_nt_v1 Tactic: -870280893819213650
VERBOSE: Tactic: -870280893819213650 Time: 0.011264
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_interior_nt_v1 Tactic: -545024964146453661
VERBOSE: Tactic: -545024964146453661 Time: 0.011776
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -496455309852654971
VERBOSE: Tactic: -496455309852654971 Time: 0.01136
VERBOSE: Fastest Tactic: 3541919052468401776 Time: 0.010912
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 3541919052468401776
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(1179648,9216,96,1) -> Float(1179648,9216,96,1) ***************
VERBOSE: --------------- Timing Runner: base_level3_tree1_project_1_output_quant.1 (Scale)
VERBOSE: Tactic: 0 Time: 0.032064
VERBOSE: Fastest Tactic: 0 Time: 0.032064
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: *************** Autotuning format combination: Int8(294912,9216:4,96,1) -> Float(1179648,9216,96,1) ***************
VERBOSE: --------------- Timing Runner: base_level3_tree1_project_1_output_quant.1 (Scale)
VERBOSE: Tactic: 0 Time: 0.034304
VERBOSE: Fastest Tactic: 0 Time: 0.034304
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: *************** Autotuning format combination: Int8(36864,9216:32,96,1) -> Float(1179648,9216,96,1) ***************
VERBOSE: --------------- Timing Runner: base_level3_tree1_project_1_output_quant.1 (Scale)
VERBOSE: Tactic: 0 Time: 0.032288
VERBOSE: Fastest Tactic: 0 Time: 0.032288
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(294912,9216:4,96,1), Float(1179648,9216,96,1) -> Float(1179648,9216,96,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 (CaskConvolution)
VERBOSE: self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 892787096507693407
VERBOSE: Tactic: 892787096507693407 Time: 0.122944
VERBOSE: self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 1204440019753223942
VERBOSE: Tactic: 1204440019753223942 Time: 0.139456
VERBOSE: self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 1659301557717208403
VERBOSE: Tactic: 1659301557717208403 Time: 0.1368
VERBOSE: self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 2057291331119027912
VERBOSE: Tactic: 2057291331119027912 Time: 0.123168
VERBOSE: self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 3275977259705528576
VERBOSE: Tactic: 3275977259705528576 Time: 0.133728
VERBOSE: self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 5623454780463195174
VERBOSE: Tactic: 5623454780463195174 Time: 0.141184
VERBOSE: self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: 8930254200803946944
VERBOSE: Tactic: 8930254200803946944 Time: 0.121888
VERBOSE: self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: -9204333525109552344
VERBOSE: Tactic: -9204333525109552344 Time: 0.121536
VERBOSE: self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -4973811344878172338
VERBOSE: Tactic: -4973811344878172338 Time: 0.122208
VERBOSE: self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: -1228371230285617088
VERBOSE: Tactic: -1228371230285617088 Time: 0.141056
VERBOSE: Fastest Tactic: -9204333525109552344 Time: 0.121536
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -9204333525109552344
VERBOSE: *************** Autotuning format combination: Int8(36864,9216:32,96,1), Float(1179648,9216,96,1) -> Float(1179648,9216,96,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 (CaskConvolution)
VERBOSE: CaskConvolution has no valid tactics for this config, skipping
VERBOSE: *************** Autotuning format combination: Int8(36864,9216:32,96,1), Float(36864,9216:32,96,1) -> Float(36864,9216:32,96,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 (CaskConvolution)
VERBOSE: self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: 394365917225754726
VERBOSE: Tactic: 394365917225754726 Time: 0.081312
VERBOSE: self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 924563784895318224
VERBOSE: Tactic: 924563784895318224 Time: 0.067264
VERBOSE: self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: 3451841782205411203
VERBOSE: Tactic: 3451841782205411203 Time: 0.067296
VERBOSE: self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 6096719469361499298
VERBOSE: Tactic: 6096719469361499298 Time: 0.067328
VERBOSE: self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: 7785217228143857868
VERBOSE: Tactic: 7785217228143857868 Time: 0.0784
VERBOSE: self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: 8315790488934712458
VERBOSE: Tactic: 8315790488934712458 Time: 0.067008
VERBOSE: self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: 9080823067063042887
VERBOSE: Tactic: 9080823067063042887 Time: 0.085696
VERBOSE: self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -8462194455331556195
VERBOSE: Tactic: -8462194455331556195 Time: 0.066432
VERBOSE: self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: -8165074686865110847
VERBOSE: Tactic: -8165074686865110847 Time: 0.068032
VERBOSE: self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -7638944668269666085
VERBOSE: Tactic: -7638944668269666085 Time: 0.079232
VERBOSE: self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: -7185527339793611699
VERBOSE: Tactic: -7185527339793611699 Time: 0.068032
VERBOSE: self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -5979101256828290173
VERBOSE: Tactic: -5979101256828290173 Time: 0.084768
VERBOSE: self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -5763174003249488100
VERBOSE: Tactic: -5763174003249488100 Time: 0.068352
VERBOSE: self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: -5612459945002849429
VERBOSE: Tactic: -5612459945002849429 Time: 0.087008
VERBOSE: self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -5250790590226149674
VERBOSE: Tactic: -5250790590226149674 Time: 0.078944
VERBOSE: self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -4911193113143178408
VERBOSE: Tactic: -4911193113143178408 Time: 0.08032
VERBOSE: self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -4831366370915083630
VERBOSE: Tactic: -4831366370915083630 Time: 0.06608
VERBOSE: self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -4563432698383308679
VERBOSE: Tactic: -4563432698383308679 Time: 0.07904
VERBOSE: self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -4067022988143035981
VERBOSE: Tactic: -4067022988143035981 Time: 0.068864
VERBOSE: self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -3936136542475827126
VERBOSE: Tactic: -3936136542475827126 Time: 0.083616
VERBOSE: self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -3784829056659735491
VERBOSE: Tactic: -3784829056659735491 Time: 0.0664
VERBOSE: self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -3720553804749394441
VERBOSE: Tactic: -3720553804749394441 Time: 0.078912
VERBOSE: self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -2697681528489059028
VERBOSE: Tactic: -2697681528489059028 Time: 0.083328
VERBOSE: self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: -733152064595858464
VERBOSE: Tactic: -733152064595858464 Time: 0.067392
VERBOSE: Fastest Tactic: -4831366370915083630 Time: 0.06608
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -4831366370915083630
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(1179648,9216,96,1) -> Int8(1179648,9216,96,1) ***************
VERBOSE: --------------- Timing Runner: base_level3_tree1_tree1_relu_output_quant.1TensorQ (Scale)
VERBOSE: Tactic: 0 Time: 0.024448
VERBOSE: Fastest Tactic: 0 Time: 0.024448
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: *************** Autotuning format combination: Float(1179648,9216,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: --------------- Timing Runner: base_level3_tree1_tree1_relu_output_quant.1TensorQ (Scale)
VERBOSE: Tactic: 0 Time: 0.027968
VERBOSE: Fastest Tactic: 0 Time: 0.027968
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: *************** Autotuning format combination: Float(1179648,9216,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: --------------- Timing Runner: base_level3_tree1_tree1_relu_output_quant.1TensorQ (Scale)
VERBOSE: Tactic: 0 Time: 0.02528
VERBOSE: Fastest Tactic: 0 Time: 0.02528
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(294912,9216:4,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 (FusedConvActConvolution)
VERBOSE: Tactic: 524287 Time: 0.118816
VERBOSE: Tactic: 720895 Time: 0.107424
VERBOSE: Tactic: 983039 Time: 0.104608
VERBOSE: Tactic: 1048575 Time: 0.113344
VERBOSE: Tactic: 1703935 Time: 0.112928
VERBOSE: Tactic: 1769471 Time: 0.147008
VERBOSE: Tactic: 2424831 Time: 0.159008
VERBOSE: Tactic: 2621439 Time: 0.124992
VERBOSE: Tactic: 3014655 Time: 0.109376
VERBOSE: Tactic: 3604479 Time: 0.108512
VERBOSE: Tactic: 5046271 Time: 0.117152
VERBOSE: Tactic: 6488063 Time: 0.114336
VERBOSE: Tactic: 7274495 Time: 0.123648
VERBOSE: Tactic: 7864319 Time: 0.12384
VERBOSE: Tactic: 8847359 Time: 0.126016
VERBOSE: Tactic: 9043967 Time: 0.113504
VERBOSE: Tactic: 9961471 Time: 0.158272
VERBOSE: Tactic: 10027007 Time: 0.113952
VERBOSE: Tactic: 10485759 Time: 0.113184
VERBOSE: Tactic: 10682367 Time: 0.122528
VERBOSE: Tactic: 10813439 Time: 0.104864
VERBOSE: Fastest Tactic: 983039 Time: 0.104608
VERBOSE: --------------- Timing Runner: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 (CaskConvolution)
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 4438325421691896755
VERBOSE: Tactic: 4438325421691896755 Time: 0.112352
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 4581732244273465060
VERBOSE: Tactic: 4581732244273465060 Time: 0.110048
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 4934335053031119367
VERBOSE: Tactic: 4934335053031119367 Time: 0.124032
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 6797040896965118050
VERBOSE: Tactic: 6797040896965118050 Time: 0.104768
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 8006952294591770973
VERBOSE: Tactic: 8006952294591770973 Time: 0.127392
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -7210942453088153035
VERBOSE: Tactic: -7210942453088153035 Time: 0.104064
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -6282183216199417697
VERBOSE: Tactic: -6282183216199417697 Time: 0.124768
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: -5026383765466876607
VERBOSE: Tactic: -5026383765466876607 Time: 0.109344
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: -5016725782072253841
VERBOSE: Tactic: -5016725782072253841 Time: 0.11904
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: -1370999262391786833
VERBOSE: Tactic: -1370999262391786833 Time: 0.10912
VERBOSE: Fastest Tactic: -7210942453088153035 Time: 0.104064
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -7210942453088153035
VERBOSE: *************** Autotuning format combination: Int8(294912,9216:4,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 (CaskConvolution)
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: 1213457772632185722
VERBOSE: Tactic: 1213457772632185722 Time: 0.123584
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 1713441381477652893
VERBOSE: Tactic: 1713441381477652893 Time: 0.127392
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_xregs_large_c32_nn_v1 Tactic: 7125598890155666458
VERBOSE: Tactic: 7125598890155666458 Time: 0.109312
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 8047041638267142825
VERBOSE: Tactic: 8047041638267142825 Time: 0.124992
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: -7846982807478255793
VERBOSE: Tactic: -7846982807478255793 Time: 0.110688
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: -6459719113600909000
VERBOSE: Tactic: -6459719113600909000 Time: 0.1184
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: -4573925292554651334
VERBOSE: Tactic: -4573925292554651334 Time: 0.10464
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: -3566249366964946311
VERBOSE: Tactic: -3566249366964946311 Time: 0.112832
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: -2002418013575043687
VERBOSE: Tactic: -2002418013575043687 Time: 0.105568
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_xregs_large_c32_nn_v1 Tactic: -1659631603542281459
VERBOSE: Tactic: -1659631603542281459 Time: 0.109792
VERBOSE: Fastest Tactic: -4573925292554651334 Time: 0.10464
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -4573925292554651334
VERBOSE: *************** Autotuning format combination: Int8(36864,9216:32,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 (CudaGroupConvolution)
VERBOSE: CudaGroupConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 (FusedConvActConvolution)
VERBOSE: Tactic: 524287 Time: 0.122784
VERBOSE: Tactic: 720895 Time: 0.105856
VERBOSE: Tactic: 983039 Time: 0.104128
VERBOSE: Tactic: 1048575 Time: 0.122496
VERBOSE: Tactic: 1703935 Time: 0.115488
VERBOSE: Tactic: 1769471 Time: 0.140736
VERBOSE: Tactic: 2424831 Time: 0.175808
VERBOSE: Tactic: 2621439 Time: 0.126272
VERBOSE: Tactic: 3014655 Time: 0.112768
VERBOSE: Tactic: 3604479 Time: 0.112064
VERBOSE: Tactic: 5046271 Time: 0.118656
VERBOSE: Tactic: 6488063 Time: 0.124992
VERBOSE: Tactic: 7274495 Time: 0.12
VERBOSE: Tactic: 7864319 Time: 0.128896
VERBOSE: Tactic: 8847359 Time: 0.126976
VERBOSE: Tactic: 9043967 Time: 0.11456
VERBOSE: Tactic: 9961471 Time: 0.168256
VERBOSE: Tactic: 10027007 Time: 0.11536
VERBOSE: Tactic: 10485759 Time: 0.113696
VERBOSE: Tactic: 10682367 Time: 0.1264
VERBOSE: Tactic: 10813439 Time: 0.10512
VERBOSE: Fastest Tactic: 983039 Time: 0.104128
VERBOSE: --------------- Timing Runner: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 (CaskConvolution)
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 289888059097454627
VERBOSE: Tactic: 289888059097454627 Time: 0.037696
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 680740992583869928
VERBOSE: Tactic: 680740992583869928 Time: 0.048832
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 1230105269624924765
VERBOSE: Tactic: 1230105269624924765 Time: 0.039616
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 1388866374720163187
VERBOSE: Tactic: 1388866374720163187 Time: 0.072672
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 2133329569091732311
VERBOSE: Tactic: 2133329569091732311 Time: 0.039904
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 2579824863892891529
VERBOSE: Tactic: 2579824863892891529 Time: 0.083232
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: 4099749238296892232
VERBOSE: Tactic: 4099749238296892232 Time: 0.039776
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 4214794893922618058
VERBOSE: Tactic: 4214794893922618058 Time: 0.040224
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 4384868749799132354
VERBOSE: Tactic: 4384868749799132354 Time: 0.082624
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 4871133328510103657
VERBOSE: Tactic: 4871133328510103657 Time: 0.039552
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 4931167631624420067
VERBOSE: Tactic: 4931167631624420067 Time: 0.068224
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 5641967928706599451
VERBOSE: Tactic: 5641967928706599451 Time: 0.097056
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 6394572396369862482
VERBOSE: Tactic: 6394572396369862482 Time: 0.098016
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 7048234086361926570
VERBOSE: Tactic: 7048234086361926570 Time: 0.057184
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -9138158634332993439
VERBOSE: Tactic: -9138158634332993439 Time: 0.037056
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -9108166971364503411
VERBOSE: Tactic: -9108166971364503411 Time: 0.057376
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: -8861822316054763526
VERBOSE: Tactic: -8861822316054763526 Time: 0.03888
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: -8475551154769412306
VERBOSE: Tactic: -8475551154769412306 Time: 0.043968
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -8431788508843860955
VERBOSE: Tactic: -8431788508843860955 Time: 0.037632
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: -6371781333659293809
VERBOSE: Tactic: -6371781333659293809 Time: 0.047136
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -5932545395595141539
VERBOSE: Tactic: -5932545395595141539 Time: 0.047136
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: -5180570335464125033
VERBOSE: Tactic: -5180570335464125033 Time: 0.044
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: -2965659229411050116
VERBOSE: Tactic: -2965659229411050116 Time: 0.0496
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -2499089240293650188
VERBOSE: Tactic: -2499089240293650188 Time: 0.049344
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -2328318099174473157
VERBOSE: Tactic: -2328318099174473157 Time: 0.047264
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -1841683966837205309
VERBOSE: Tactic: -1841683966837205309 Time: 0.035968
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -1405083410359886949
VERBOSE: Tactic: -1405083410359886949 Time: 0.048928
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -1044618751912149244
VERBOSE: Tactic: -1044618751912149244 Time: 0.039136
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -496455309852654971
VERBOSE: Tactic: -496455309852654971 Time: 0.037344
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -409197672346571968
VERBOSE: Tactic: -409197672346571968 Time: 0.0368
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: -366411318217594794
VERBOSE: Tactic: -366411318217594794 Time: 0.073632
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: -351548418071036983
VERBOSE: Tactic: -351548418071036983 Time: 0.069824
VERBOSE: Fastest Tactic: -1841683966837205309 Time: 0.035968
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -1841683966837205309
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(1179648,9216,96,1) -> Int8(2359296,9216,96,1) ***************
VERBOSE: --------------- Timing Runner: quant_input.32TensorQ_clone_1 (Scale)
VERBOSE: Tactic: 0 Time: 0.027424
VERBOSE: Fastest Tactic: 0 Time: 0.027424
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: *************** Autotuning format combination: Float(1179648,9216,96,1) -> Int8(589824,9216:4,96,1) ***************
VERBOSE: --------------- Timing Runner: quant_input.32TensorQ_clone_1 (Scale)
VERBOSE: Tactic: 0 Time: 0.031616
VERBOSE: Fastest Tactic: 0 Time: 0.031616
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: *************** Autotuning format combination: Float(1179648,9216,96,1) -> Int8(73728,9216:32,96,1) ***************
VERBOSE: --------------- Timing Runner: quant_input.32TensorQ_clone_1 (Scale)
VERBOSE: Tactic: 0 Time: 0.028576
VERBOSE: Fastest Tactic: 0 Time: 0.028576
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(294912,9216:4,96,1), Int8(294912,9216:4,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 (FusedConvActConvolution)
VERBOSE: FusedConvActConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 (CaskConvolution)
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 4438325421691896755
VERBOSE: Tactic: 4438325421691896755 Time: 0.115296
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 4581732244273465060
VERBOSE: Tactic: 4581732244273465060 Time: 0.11344
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 4934335053031119367
VERBOSE: Tactic: 4934335053031119367 Time: 0.12768
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 6797040896965118050
VERBOSE: Tactic: 6797040896965118050 Time: 0.107648
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 8006952294591770973
VERBOSE: Tactic: 8006952294591770973 Time: 0.130112
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -7210942453088153035
VERBOSE: Tactic: -7210942453088153035 Time: 0.107008
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -6282183216199417697
VERBOSE: Tactic: -6282183216199417697 Time: 0.127552
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: -5026383765466876607
VERBOSE: Tactic: -5026383765466876607 Time: 0.112992
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: -5016725782072253841
VERBOSE: Tactic: -5016725782072253841 Time: 0.122528
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: -1370999262391786833
VERBOSE: Tactic: -1370999262391786833 Time: 0.11232
VERBOSE: Fastest Tactic: -7210942453088153035 Time: 0.107008
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -7210942453088153035
VERBOSE: *************** Autotuning format combination: Int8(294912,9216:4,96,1), Int8(36864,9216:32,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 (CaskConvolution)
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: 1213457772632185722
VERBOSE: Tactic: 1213457772632185722 Time: 0.127872
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 1713441381477652893
VERBOSE: Tactic: 1713441381477652893 Time: 0.130432
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_xregs_large_c32_nn_v1 Tactic: 7125598890155666458
VERBOSE: Tactic: 7125598890155666458 Time: 0.112864
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 8047041638267142825
VERBOSE: Tactic: 8047041638267142825 Time: 0.127808
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: -7846982807478255793
VERBOSE: Tactic: -7846982807478255793 Time: 0.113856
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: -6459719113600909000
VERBOSE: Tactic: -6459719113600909000 Time: 0.122496
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: -4573925292554651334
VERBOSE: Tactic: -4573925292554651334 Time: 0.108352
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: -3566249366964946311
VERBOSE: Tactic: -3566249366964946311 Time: 0.116096
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: -2002418013575043687
VERBOSE: Tactic: -2002418013575043687 Time: 0.10912
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_xregs_large_c32_nn_v1 Tactic: -1659631603542281459
VERBOSE: Tactic: -1659631603542281459 Time: 0.11424
VERBOSE: Fastest Tactic: -4573925292554651334 Time: 0.108352
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -4573925292554651334
VERBOSE: *************** Autotuning format combination: Int8(36864,9216:32,96,1), Int8(36864,9216:32,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 (CudaGroupConvolution)
VERBOSE: CudaGroupConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 (FusedConvActConvolution)
VERBOSE: FusedConvActConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 (CaskConvolution)
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 289888059097454627
VERBOSE: Tactic: 289888059097454627 Time: 0.038912
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 680740992583869928
VERBOSE: Tactic: 680740992583869928 Time: 0.050464
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 1230105269624924765
VERBOSE: Tactic: 1230105269624924765 Time: 0.041152
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 1388866374720163187
VERBOSE: Tactic: 1388866374720163187 Time: 0.075584
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 2133329569091732311
VERBOSE: Tactic: 2133329569091732311 Time: 0.043136
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 2579824863892891529
VERBOSE: Tactic: 2579824863892891529 Time: 0.084064
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: 4099749238296892232
VERBOSE: Tactic: 4099749238296892232 Time: 0.041664
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 4214794893922618058
VERBOSE: Tactic: 4214794893922618058 Time: 0.04336
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 4384868749799132354
VERBOSE: Tactic: 4384868749799132354 Time: 0.083488
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 4871133328510103657
VERBOSE: Tactic: 4871133328510103657 Time: 0.041056
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 4931167631624420067
VERBOSE: Tactic: 4931167631624420067 Time: 0.070624
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 5641967928706599451
VERBOSE: Tactic: 5641967928706599451 Time: 0.098048
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 6394572396369862482
VERBOSE: Tactic: 6394572396369862482 Time: 0.098336
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 7048234086361926570
VERBOSE: Tactic: 7048234086361926570 Time: 0.059328
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -9138158634332993439
VERBOSE: Tactic: -9138158634332993439 Time: 0.038848
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -9108166971364503411
VERBOSE: Tactic: -9108166971364503411 Time: 0.058848
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: -8861822316054763526
VERBOSE: Tactic: -8861822316054763526 Time: 0.040576
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: -8475551154769412306
VERBOSE: Tactic: -8475551154769412306 Time: 0.046336
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -8431788508843860955
VERBOSE: Tactic: -8431788508843860955 Time: 0.03904
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: -6371781333659293809
VERBOSE: Tactic: -6371781333659293809 Time: 0.048864
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -5932545395595141539
VERBOSE: Tactic: -5932545395595141539 Time: 0.049504
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: -5180570335464125033
VERBOSE: Tactic: -5180570335464125033 Time: 0.046464
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: -2965659229411050116
VERBOSE: Tactic: -2965659229411050116 Time: 0.051904
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -2499089240293650188
VERBOSE: Tactic: -2499089240293650188 Time: 0.050624
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -2328318099174473157
VERBOSE: Tactic: -2328318099174473157 Time: 0.049376
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -1841683966837205309
VERBOSE: Tactic: -1841683966837205309 Time: 0.037696
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -1405083410359886949
VERBOSE: Tactic: -1405083410359886949 Time: 0.051488
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -1044618751912149244
VERBOSE: Tactic: -1044618751912149244 Time: 0.040928
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -496455309852654971
VERBOSE: Tactic: -496455309852654971 Time: 0.039104
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -409197672346571968
VERBOSE: Tactic: -409197672346571968 Time: 0.03872
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: -366411318217594794
VERBOSE: Tactic: -366411318217594794 Time: 0.076032
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: -351548418071036983
VERBOSE: Tactic: -351548418071036983 Time: 0.073248
VERBOSE: Fastest Tactic: -1841683966837205309 Time: 0.037696
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -1841683966837205309
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(589824,9216:4,96,1) -> Float(1179648,9216,96,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 (CaskConvolution)
VERBOSE: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 892787096507693407
VERBOSE: Tactic: 892787096507693407 Time: 0.03904
VERBOSE: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 1204440019753223942
VERBOSE: Tactic: 1204440019753223942 Time: 0.056256
VERBOSE: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 1659301557717208403
VERBOSE: Tactic: 1659301557717208403 Time: 0.043936
VERBOSE: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 2057291331119027912
VERBOSE: Tactic: 2057291331119027912 Time: 0.039648
VERBOSE: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 3275977259705528576
VERBOSE: Tactic: 3275977259705528576 Time: 0.055552
VERBOSE: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 5623454780463195174
VERBOSE: Tactic: 5623454780463195174 Time: 0.046368
VERBOSE: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: -9204333525109552344
VERBOSE: Tactic: -9204333525109552344 Time: 0.040672
VERBOSE: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: -7924103240988931433
VERBOSE: Tactic: -7924103240988931433 Time: 0.04064
VERBOSE: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_xregs_interior_nn_v1 Tactic: -7489650117016530013
VERBOSE: Tactic: -7489650117016530013 Time: 0.054464
VERBOSE: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -4973811344878172338
VERBOSE: Tactic: -4973811344878172338 Time: 0.03952
VERBOSE: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: -3908975881807046106
VERBOSE: Tactic: -3908975881807046106 Time: 0.045568
VERBOSE: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_interior_nn_v1 Tactic: -1765942417666394360
VERBOSE: Tactic: -1765942417666394360 Time: 0.039904
VERBOSE: Fastest Tactic: 892787096507693407 Time: 0.03904
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 892787096507693407
VERBOSE: *************** Autotuning format combination: Int8(73728,9216:32,96,1) -> Float(1179648,9216,96,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 (CaskConvolution)
VERBOSE: CaskConvolution has no valid tactics for this config, skipping
VERBOSE: *************** Autotuning format combination: Int8(73728,9216:32,96,1) -> Float(36864,9216:32,96,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 (CaskConvolution)
VERBOSE: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: 394365917225754726
VERBOSE: Tactic: 394365917225754726 Time: 0.047584
VERBOSE: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 924563784895318224
VERBOSE: Tactic: 924563784895318224 Time: 0.041632
VERBOSE: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_interior_nt_v1 Tactic: 4531028070024747144
VERBOSE: Tactic: 4531028070024747144 Time: 0.050304
VERBOSE: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_interior_nt_v1 Tactic: 4697540470896098800
VERBOSE: Tactic: 4697540470896098800 Time: 0.043008
VERBOSE: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 6096719469361499298
VERBOSE: Tactic: 6096719469361499298 Time: 0.041664
VERBOSE: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_interior_nt_v1 Tactic: 7469355608320515097
VERBOSE: Tactic: 7469355608320515097 Time: 0.049632
VERBOSE: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: 7785217228143857868
VERBOSE: Tactic: 7785217228143857868 Time: 0.049376
VERBOSE: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: 8315790488934712458
VERBOSE: Tactic: 8315790488934712458 Time: 0.04224
VERBOSE: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_interior_nt_v1 Tactic: 9221575372280690678
VERBOSE: Tactic: 9221575372280690678 Time: 0.050176
VERBOSE: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -8462194455331556195
VERBOSE: Tactic: -8462194455331556195 Time: 0.0432
VERBOSE: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -7638944668269666085
VERBOSE: Tactic: -7638944668269666085 Time: 0.04848
VERBOSE: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: -7185527339793611699
VERBOSE: Tactic: -7185527339793611699 Time: 0.041984
VERBOSE: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -5979101256828290173
VERBOSE: Tactic: -5979101256828290173 Time: 0.051328
VERBOSE: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -5250790590226149674
VERBOSE: Tactic: -5250790590226149674 Time: 0.048544
VERBOSE: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -4831366370915083630
VERBOSE: Tactic: -4831366370915083630 Time: 0.0424
VERBOSE: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -4563432698383308679
VERBOSE: Tactic: -4563432698383308679 Time: 0.048224
VERBOSE: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -3936136542475827126
VERBOSE: Tactic: -3936136542475827126 Time: 0.04992
VERBOSE: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -3784829056659735491
VERBOSE: Tactic: -3784829056659735491 Time: 0.042816
VERBOSE: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -2697681528489059028
VERBOSE: Tactic: -2697681528489059028 Time: 0.0472
VERBOSE: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_interior_nt_v1 Tactic: -2621193268472024213
VERBOSE: Tactic: -2621193268472024213 Time: 0.041984
VERBOSE: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_interior_nt_v1 Tactic: -2120707675073643088
VERBOSE: Tactic: -2120707675073643088 Time: 0.044672
VERBOSE: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: -733152064595858464
VERBOSE: Tactic: -733152064595858464 Time: 0.04304
VERBOSE: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_interior_nt_v1 Tactic: -706197824303187656
VERBOSE: Tactic: -706197824303187656 Time: 0.04192
VERBOSE: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_interior_nt_v1 Tactic: -214244313010793854
VERBOSE: Tactic: -214244313010793854 Time: 0.049792
VERBOSE: Fastest Tactic: 924563784895318224 Time: 0.041632
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 924563784895318224
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(1179648,9216,96,1) -> Int8(1179648,9216,96,1) ***************
VERBOSE: *************** Autotuning format combination: Float(1179648,9216,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: *************** Autotuning format combination: Float(1179648,9216,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(1179648,9216,96,1) -> Float(1179648,9216,96,1) ***************
VERBOSE: *************** Autotuning format combination: Int8(294912,9216:4,96,1) -> Float(1179648,9216,96,1) ***************
VERBOSE: *************** Autotuning format combination: Int8(36864,9216:32,96,1) -> Float(1179648,9216,96,1) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(294912,9216:4,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: *************** Autotuning format combination: Int8(294912,9216:4,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning format combination: Int8(36864,9216:32,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(294912,9216:4,96,1), Float(1179648,9216,96,1) -> Float(1179648,9216,96,1) ***************
VERBOSE: *************** Autotuning format combination: Int8(36864,9216:32,96,1), Float(1179648,9216,96,1) -> Float(1179648,9216,96,1) ***************
VERBOSE: *************** Autotuning format combination: Int8(36864,9216:32,96,1), Float(36864,9216:32,96,1) -> Float(36864,9216:32,96,1) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(1179648,9216,96,1) -> Int8(1179648,9216,96,1) ***************
VERBOSE: *************** Autotuning format combination: Float(1179648,9216,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: *************** Autotuning format combination: Float(1179648,9216,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(294912,9216:4,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: *************** Autotuning format combination: Int8(294912,9216:4,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning format combination: Int8(36864,9216:32,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(1179648,9216,96,1) -> Int8(4128768,9216,96,1) ***************
VERBOSE: --------------- Timing Runner: quant_input.42TensorQ_clone_3 (Scale)
VERBOSE: Tactic: 0 Time: 0.030176
VERBOSE: Fastest Tactic: 0 Time: 0.030176
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: *************** Autotuning format combination: Float(1179648,9216,96,1) -> Int8(1032192,9216:4,96,1) ***************
VERBOSE: --------------- Timing Runner: quant_input.42TensorQ_clone_3 (Scale)
VERBOSE: Tactic: 0 Time: 0.035616
VERBOSE: Fastest Tactic: 0 Time: 0.035616
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: *************** Autotuning format combination: Float(1179648,9216,96,1) -> Int8(129024,9216:32,96,1) ***************
VERBOSE: --------------- Timing Runner: quant_input.42TensorQ_clone_3 (Scale)
VERBOSE: Tactic: 0 Time: 0.033184
VERBOSE: Fastest Tactic: 0 Time: 0.033184
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(589824,36864:4,192,1) -> Int8(1032192,9216:4,96,1) ***************
VERBOSE: --------------- Timing Runner: inputs.9 (TiledPooling)
VERBOSE: Tactic: 5505281 Time: 0.025728
VERBOSE: Tactic: 5570817 Time: 0.01936
VERBOSE: Tactic: 5636353 Time: 0.017504
VERBOSE: Tactic: 5701889 Time: 0.018048
VERBOSE: Tactic: 5767425 Time: 0.018336
VERBOSE: Tactic: 5832961 Time: 0.018368
VERBOSE: Tactic: 5898497 Time: 0.019936
VERBOSE: Tactic: 5964033 Time: 0.0184
VERBOSE: Tactic: 6029569 Time: 0.025184
VERBOSE: Tactic: 6095105 Time: 0.018048
VERBOSE: Tactic: 6160641 Time: 0.016512
VERBOSE: Tactic: 6226177 Time: 0.017568
VERBOSE: Tactic: 6291713 Time: 0.017536
VERBOSE: Tactic: 6357249 Time: 0.017856
VERBOSE: Tactic: 6422785 Time: 0.018912
VERBOSE: Tactic: 6488321 Time: 0.017824
VERBOSE: Fastest Tactic: 6160641 Time: 0.016512
VERBOSE: --------------- Timing Runner: inputs.9 (CudaPooling)
VERBOSE: Tactic: -3 Time: 0.015136
VERBOSE: Fastest Tactic: -3 Time: 0.015136
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CudaPooling Tactic: -3
VERBOSE: *************** Autotuning format combination: Int8(73728,36864:32,192,1) -> Int8(129024,9216:32,96,1) ***************
VERBOSE: --------------- Timing Runner: inputs.9 (TiledPooling)
VERBOSE: TiledPooling has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: inputs.9 (CudaPooling)
VERBOSE: Tactic: -4 Time: 0.014656
VERBOSE: Fastest Tactic: -4 Time: 0.014656
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CudaPooling Tactic: -4
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(1179648,9216,96,1) -> Int8(4128768,9216,96,1) ***************
VERBOSE: *************** Autotuning format combination: Float(1179648,9216,96,1) -> Int8(1032192,9216:4,96,1) ***************
VERBOSE: *************** Autotuning format combination: Float(1179648,9216,96,1) -> Int8(129024,9216:32,96,1) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(294912,9216:4,96,1), Int8(294912,9216:4,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: *************** Autotuning format combination: Int8(294912,9216:4,96,1), Int8(36864,9216:32,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: *************** Autotuning format combination: Int8(36864,9216:32,96,1), Int8(36864,9216:32,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(1032192,9216:4,96,1) -> Float(1179648,9216,96,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 (CaskConvolution)
VERBOSE: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 892787096507693407
VERBOSE: Tactic: 892787096507693407 Time: 0.0904
VERBOSE: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 1204440019753223942
VERBOSE: Tactic: 1204440019753223942 Time: 0.113152
VERBOSE: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 1659301557717208403
VERBOSE: Tactic: 1659301557717208403 Time: 0.103424
VERBOSE: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 2057291331119027912
VERBOSE: Tactic: 2057291331119027912 Time: 0.096288
VERBOSE: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 3275977259705528576
VERBOSE: Tactic: 3275977259705528576 Time: 0.108672
VERBOSE: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 5623454780463195174
VERBOSE: Tactic: 5623454780463195174 Time: 0.10704
VERBOSE: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: -9204333525109552344
VERBOSE: Tactic: -9204333525109552344 Time: 0.095552
VERBOSE: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: -7924103240988931433
VERBOSE: Tactic: -7924103240988931433 Time: 0.091104
VERBOSE: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_xregs_interior_nn_v1 Tactic: -7489650117016530013
VERBOSE: Tactic: -7489650117016530013 Time: 0.106784
VERBOSE: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -4973811344878172338
VERBOSE: Tactic: -4973811344878172338 Time: 0.090048
VERBOSE: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: -3908975881807046106
VERBOSE: Tactic: -3908975881807046106 Time: 0.099168
VERBOSE: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_interior_nn_v1 Tactic: -1765942417666394360
VERBOSE: Tactic: -1765942417666394360 Time: 0.090016
VERBOSE: Fastest Tactic: -1765942417666394360 Time: 0.090016
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -1765942417666394360
VERBOSE: *************** Autotuning format combination: Int8(129024,9216:32,96,1) -> Float(1179648,9216,96,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 (CaskConvolution)
VERBOSE: CaskConvolution has no valid tactics for this config, skipping
VERBOSE: *************** Autotuning format combination: Int8(129024,9216:32,96,1) -> Float(36864,9216:32,96,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 (CaskConvolution)
VERBOSE: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: 394365917225754726
VERBOSE: Tactic: 394365917225754726 Time: 0.074784
VERBOSE: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 924563784895318224
VERBOSE: Tactic: 924563784895318224 Time: 0.053184
VERBOSE: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_interior_nt_v1 Tactic: 4531028070024747144
VERBOSE: Tactic: 4531028070024747144 Time: 0.074464
VERBOSE: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_interior_nt_v1 Tactic: 4697540470896098800
VERBOSE: Tactic: 4697540470896098800 Time: 0.052064
VERBOSE: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 6096719469361499298
VERBOSE: Tactic: 6096719469361499298 Time: 0.052352
VERBOSE: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_interior_nt_v1 Tactic: 7469355608320515097
VERBOSE: Tactic: 7469355608320515097 Time: 0.0632
VERBOSE: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: 7785217228143857868
VERBOSE: Tactic: 7785217228143857868 Time: 0.063424
VERBOSE: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: 8315790488934712458
VERBOSE: Tactic: 8315790488934712458 Time: 0.050464
VERBOSE: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_interior_nt_v1 Tactic: 9221575372280690678
VERBOSE: Tactic: 9221575372280690678 Time: 0.07424
VERBOSE: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -8462194455331556195
VERBOSE: Tactic: -8462194455331556195 Time: 0.052032
VERBOSE: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -7638944668269666085
VERBOSE: Tactic: -7638944668269666085 Time: 0.063072
VERBOSE: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: -7185527339793611699
VERBOSE: Tactic: -7185527339793611699 Time: 0.052736
VERBOSE: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -5979101256828290173
VERBOSE: Tactic: -5979101256828290173 Time: 0.075168
VERBOSE: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -5250790590226149674
VERBOSE: Tactic: -5250790590226149674 Time: 0.062208
VERBOSE: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -4831366370915083630
VERBOSE: Tactic: -4831366370915083630 Time: 0.050816
VERBOSE: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -4563432698383308679
VERBOSE: Tactic: -4563432698383308679 Time: 0.062624
VERBOSE: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -3936136542475827126
VERBOSE: Tactic: -3936136542475827126 Time: 0.07568
VERBOSE: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -3784829056659735491
VERBOSE: Tactic: -3784829056659735491 Time: 0.05136
VERBOSE: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -2697681528489059028
VERBOSE: Tactic: -2697681528489059028 Time: 0.07504
VERBOSE: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_interior_nt_v1 Tactic: -2621193268472024213
VERBOSE: Tactic: -2621193268472024213 Time: 0.050784
VERBOSE: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_interior_nt_v1 Tactic: -2120707675073643088
VERBOSE: Tactic: -2120707675073643088 Time: 0.051008
VERBOSE: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: -733152064595858464
VERBOSE: Tactic: -733152064595858464 Time: 0.05296
VERBOSE: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_interior_nt_v1 Tactic: -706197824303187656
VERBOSE: Tactic: -706197824303187656 Time: 0.05248
VERBOSE: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_interior_nt_v1 Tactic: -214244313010793854
VERBOSE: Tactic: -214244313010793854 Time: 0.063072
VERBOSE: Fastest Tactic: 8315790488934712458 Time: 0.050464
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 8315790488934712458
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(1179648,9216,96,1) -> Int8(1179648,9216,96,1) ***************
VERBOSE: *************** Autotuning format combination: Float(1179648,9216,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: *************** Autotuning format combination: Float(1179648,9216,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(1179648,9216,96,1) -> Int8(1179648,9216,96,1) ***************
VERBOSE: *************** Autotuning format combination: Float(1179648,9216,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: *************** Autotuning format combination: Float(1179648,9216,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(294912,9216:4,96,1) -> Int8(73728,2304:4,48,1) ***************
VERBOSE: --------------- Timing Runner: inputs.17 (TiledPooling)
VERBOSE: Tactic: 5505281 Time: 0.020032
VERBOSE: Tactic: 5570817 Time: 0.01552
VERBOSE: Tactic: 5636353 Time: 0.016064
VERBOSE: Tactic: 5701889 Time: 0.015744
VERBOSE: Tactic: 5767425 Time: 0.015232
VERBOSE: Tactic: 5832961 Time: 0.015808
VERBOSE: Tactic: 5898497 Time: 0.016416
VERBOSE: Tactic: 5964033 Time: 0.016672
VERBOSE: Tactic: 6029569 Time: 0.016896
VERBOSE: Tactic: 6095105 Time: 0.012928
VERBOSE: Tactic: 6160641 Time: 0.013216
VERBOSE: Tactic: 6226177 Time: 0.012448
VERBOSE: Tactic: 6291713 Time: 0.012992
VERBOSE: Tactic: 6357249 Time: 0.014112
VERBOSE: Tactic: 6422785 Time: 0.014368
VERBOSE: Tactic: 6488321 Time: 0.012704
VERBOSE: Fastest Tactic: 6226177 Time: 0.012448
VERBOSE: --------------- Timing Runner: inputs.17 (CudaPooling)
VERBOSE: Tactic: -3 Time: 0.011552
VERBOSE: Fastest Tactic: -3 Time: 0.011552
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CudaPooling Tactic: -3
VERBOSE: *************** Autotuning format combination: Int8(36864,9216:32,96,1) -> Int8(9216,2304:32,48,1) ***************
VERBOSE: --------------- Timing Runner: inputs.17 (TiledPooling)
VERBOSE: TiledPooling has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: inputs.17 (CudaPooling)
VERBOSE: Tactic: -4 Time: 0.011264
VERBOSE: Fastest Tactic: -4 Time: 0.011264
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CudaPooling Tactic: -4
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(294912,9216:4,96,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 (FusedConvActConvolution)
VERBOSE: Tactic: 458751 Time: 0.20176
VERBOSE: Fastest Tactic: 458751 Time: 0.20176
VERBOSE: --------------- Timing Runner: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 (CaskConvolution)
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 4438325421691896755
VERBOSE: Tactic: 4438325421691896755 Time: 0.110112
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 4581732244273465060
VERBOSE: Tactic: 4581732244273465060 Time: 0.106368
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 4934335053031119367
VERBOSE: Tactic: 4934335053031119367 Time: 0.1544
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 6797040896965118050
VERBOSE: Tactic: 6797040896965118050 Time: 0.099392
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 8006952294591770973
VERBOSE: Tactic: 8006952294591770973 Time: 0.130208
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -7210942453088153035
VERBOSE: Tactic: -7210942453088153035 Time: 0.098208
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -6282183216199417697
VERBOSE: Tactic: -6282183216199417697 Time: 0.127808
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: -5026383765466876607
VERBOSE: Tactic: -5026383765466876607 Time: 0.100352
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: -5016725782072253841
VERBOSE: Tactic: -5016725782072253841 Time: 0.152288
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: -1370999262391786833
VERBOSE: Tactic: -1370999262391786833 Time: 0.103712
VERBOSE: Fastest Tactic: -7210942453088153035 Time: 0.098208
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -7210942453088153035
VERBOSE: *************** Autotuning format combination: Int8(294912,9216:4,96,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 (CaskConvolution)
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: 1213457772632185722
VERBOSE: Tactic: 1213457772632185722 Time: 0.16416
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 1713441381477652893
VERBOSE: Tactic: 1713441381477652893 Time: 0.138688
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_xregs_large_c32_nn_v1 Tactic: 7125598890155666458
VERBOSE: Tactic: 7125598890155666458 Time: 0.110816
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 8047041638267142825
VERBOSE: Tactic: 8047041638267142825 Time: 0.135584
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: -7846982807478255793
VERBOSE: Tactic: -7846982807478255793 Time: 0.114016
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: -6459719113600909000
VERBOSE: Tactic: -6459719113600909000 Time: 0.162176
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: -4573925292554651334
VERBOSE: Tactic: -4573925292554651334 Time: 0.1056
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: -3566249366964946311
VERBOSE: Tactic: -3566249366964946311 Time: 0.117568
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: -2002418013575043687
VERBOSE: Tactic: -2002418013575043687 Time: 0.10672
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_xregs_large_c32_nn_v1 Tactic: -1659631603542281459
VERBOSE: Tactic: -1659631603542281459 Time: 0.10768
VERBOSE: Fastest Tactic: -4573925292554651334 Time: 0.1056
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -4573925292554651334
VERBOSE: *************** Autotuning format combination: Int8(36864,9216:32,96,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 (CudaGroupConvolution)
VERBOSE: CudaGroupConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 (FusedConvActConvolution)
VERBOSE: Tactic: 458751 Time: 0.234464
VERBOSE: Fastest Tactic: 458751 Time: 0.234464
VERBOSE: --------------- Timing Runner: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 (CaskConvolution)
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 289888059097454627
VERBOSE: Tactic: 289888059097454627 Time: 0.063488
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 680740992583869928
VERBOSE: Tactic: 680740992583869928 Time: 0.05264
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 1230105269624924765
VERBOSE: Tactic: 1230105269624924765 Time: 0.048864
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 1388866374720163187
VERBOSE: Tactic: 1388866374720163187 Time: 0.083904
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 2133329569091732311
VERBOSE: Tactic: 2133329569091732311 Time: 0.04944
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 2579824863892891529
VERBOSE: Tactic: 2579824863892891529 Time: 0.090432
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: 4099749238296892232
VERBOSE: Tactic: 4099749238296892232 Time: 0.04928
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 4214794893922618058
VERBOSE: Tactic: 4214794893922618058 Time: 0.049088
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 4384868749799132354
VERBOSE: Tactic: 4384868749799132354 Time: 0.091904
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 4871133328510103657
VERBOSE: Tactic: 4871133328510103657 Time: 0.048288
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 4931167631624420067
VERBOSE: Tactic: 4931167631624420067 Time: 0.062016
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 5641967928706599451
VERBOSE: Tactic: 5641967928706599451 Time: 0.112768
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 6394572396369862482
VERBOSE: Tactic: 6394572396369862482 Time: 0.110496
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 7048234086361926570
VERBOSE: Tactic: 7048234086361926570 Time: 0.064736
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -9138158634332993439
VERBOSE: Tactic: -9138158634332993439 Time: 0.063232
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -9108166971364503411
VERBOSE: Tactic: -9108166971364503411 Time: 0.065568
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: -8861822316054763526
VERBOSE: Tactic: -8861822316054763526 Time: 0.064672
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: -8475551154769412306
VERBOSE: Tactic: -8475551154769412306 Time: 0.0568
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -8431788508843860955
VERBOSE: Tactic: -8431788508843860955 Time: 0.043744
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: -6371781333659293809
VERBOSE: Tactic: -6371781333659293809 Time: 0.053856
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -5932545395595141539
VERBOSE: Tactic: -5932545395595141539 Time: 0.050304
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: -5180570335464125033
VERBOSE: Tactic: -5180570335464125033 Time: 0.056576
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: -2965659229411050116
VERBOSE: Tactic: -2965659229411050116 Time: 0.05184
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -2499089240293650188
VERBOSE: Tactic: -2499089240293650188 Time: 0.052896
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -2328318099174473157
VERBOSE: Tactic: -2328318099174473157 Time: 0.054464
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -1841683966837205309
VERBOSE: Tactic: -1841683966837205309 Time: 0.063776
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -1405083410359886949
VERBOSE: Tactic: -1405083410359886949 Time: 0.051968
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -1044618751912149244
VERBOSE: Tactic: -1044618751912149244 Time: 0.04672
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -496455309852654971
VERBOSE: Tactic: -496455309852654971 Time: 0.044
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -409197672346571968
VERBOSE: Tactic: -409197672346571968 Time: 0.064224
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: -366411318217594794
VERBOSE: Tactic: -366411318217594794 Time: 0.084704
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: -351548418071036983
VERBOSE: Tactic: -351548418071036983 Time: 0.063104
VERBOSE: Fastest Tactic: -8431788508843860955 Time: 0.043744
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -8431788508843860955
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(294912,9216:4,96,1) -> Float(589824,9216,96,1) ***************
VERBOSE: --------------- Timing Runner: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 (CaskConvolution)
VERBOSE: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 892787096507693407
VERBOSE: Tactic: 892787096507693407 Time: 0.045344
VERBOSE: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 1204440019753223942
VERBOSE: Tactic: 1204440019753223942 Time: 0.030944
VERBOSE: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 1659301557717208403
VERBOSE: Tactic: 1659301557717208403 Time: 0.031456
VERBOSE: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 2057291331119027912
VERBOSE: Tactic: 2057291331119027912 Time: 0.030912
VERBOSE: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 3275977259705528576
VERBOSE: Tactic: 3275977259705528576 Time: 0.036288
VERBOSE: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 5623454780463195174
VERBOSE: Tactic: 5623454780463195174 Time: 0.032064
VERBOSE: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: -9204333525109552344
VERBOSE: Tactic: -9204333525109552344 Time: 0.030848
VERBOSE: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: -7924103240988931433
VERBOSE: Tactic: -7924103240988931433 Time: 0.029568
VERBOSE: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_xregs_interior_nn_v1 Tactic: -7489650117016530013
VERBOSE: Tactic: -7489650117016530013 Time: 0.036384
VERBOSE: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -4973811344878172338
VERBOSE: Tactic: -4973811344878172338 Time: 0.045344
VERBOSE: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: -3908975881807046106
VERBOSE: Tactic: -3908975881807046106 Time: 0.03072
VERBOSE: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_interior_nn_v1 Tactic: -1765942417666394360
VERBOSE: Tactic: -1765942417666394360 Time: 0.044864
VERBOSE: Fastest Tactic: -7924103240988931433 Time: 0.029568
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -7924103240988931433
VERBOSE: *************** Autotuning format combination: Int8(36864,9216:32,96,1) -> Float(589824,9216,96,1) ***************
VERBOSE: --------------- Timing Runner: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 (CaskConvolution)
VERBOSE: CaskConvolution has no valid tactics for this config, skipping
VERBOSE: *************** Autotuning format combination: Int8(36864,9216:32,96,1) -> Float(18432,9216:32,96,1) ***************
VERBOSE: --------------- Timing Runner: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 (CaskConvolution)
VERBOSE: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: 394365917225754726
VERBOSE: Tactic: 394365917225754726 Time: 0.02672
VERBOSE: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 924563784895318224
VERBOSE: Tactic: 924563784895318224 Time: 0.026496
VERBOSE: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_interior_nt_v1 Tactic: 4531028070024747144
VERBOSE: Tactic: 4531028070024747144 Time: 0.02656
VERBOSE: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_interior_nt_v1 Tactic: 4697540470896098800
VERBOSE: Tactic: 4697540470896098800 Time: 0.021984
VERBOSE: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 6096719469361499298
VERBOSE: Tactic: 6096719469361499298 Time: 0.026592
VERBOSE: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_interior_nt_v1 Tactic: 7469355608320515097
VERBOSE: Tactic: 7469355608320515097 Time: 0.024256
VERBOSE: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: 7785217228143857868
VERBOSE: Tactic: 7785217228143857868 Time: 0.02448
VERBOSE: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: 8315790488934712458
VERBOSE: Tactic: 8315790488934712458 Time: 0.0216
VERBOSE: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_interior_nt_v1 Tactic: 9221575372280690678
VERBOSE: Tactic: 9221575372280690678 Time: 0.026272
VERBOSE: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -8462194455331556195
VERBOSE: Tactic: -8462194455331556195 Time: 0.021952
VERBOSE: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -7638944668269666085
VERBOSE: Tactic: -7638944668269666085 Time: 0.024224
VERBOSE: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: -7185527339793611699
VERBOSE: Tactic: -7185527339793611699 Time: 0.026464
VERBOSE: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -5979101256828290173
VERBOSE: Tactic: -5979101256828290173 Time: 0.027008
VERBOSE: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -5250790590226149674
VERBOSE: Tactic: -5250790590226149674 Time: 0.02496
VERBOSE: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -4831366370915083630
VERBOSE: Tactic: -4831366370915083630 Time: 0.022144
VERBOSE: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -4563432698383308679
VERBOSE: Tactic: -4563432698383308679 Time: 0.024992
VERBOSE: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -3936136542475827126
VERBOSE: Tactic: -3936136542475827126 Time: 0.026432
VERBOSE: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -3784829056659735491
VERBOSE: Tactic: -3784829056659735491 Time: 0.021888
VERBOSE: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -2697681528489059028
VERBOSE: Tactic: -2697681528489059028 Time: 0.026976
VERBOSE: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_interior_nt_v1 Tactic: -2621193268472024213
VERBOSE: Tactic: -2621193268472024213 Time: 0.021696
VERBOSE: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_interior_nt_v1 Tactic: -2120707675073643088
VERBOSE: Tactic: -2120707675073643088 Time: 0.026592
VERBOSE: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: -733152064595858464
VERBOSE: Tactic: -733152064595858464 Time: 0.02672
VERBOSE: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_interior_nt_v1 Tactic: -706197824303187656
VERBOSE: Tactic: -706197824303187656 Time: 0.026848
VERBOSE: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_interior_nt_v1 Tactic: -214244313010793854
VERBOSE: Tactic: -214244313010793854 Time: 0.024256
VERBOSE: Fastest Tactic: 8315790488934712458 Time: 0.0216
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 8315790488934712458
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(73728,2304:4,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 (FusedConvActConvolution)
VERBOSE: Tactic: 589823 Time: 0.068576
VERBOSE: Tactic: 655359 Time: 0.03968
VERBOSE: Tactic: 786431 Time: 0.056992
VERBOSE: Tactic: 851967 Time: 0.054976
VERBOSE: Tactic: 1179647 Time: 0.054208
VERBOSE: Tactic: 1638399 Time: 0.062016
VERBOSE: Tactic: 1835007 Time: 0.058368
VERBOSE: Tactic: 2097151 Time: 0.068416
VERBOSE: Tactic: 3080191 Time: 0.040544
VERBOSE: Tactic: 3538943 Time: 0.045984
VERBOSE: Tactic: 3997695 Time: 0.059424
VERBOSE: Tactic: 4063231 Time: 0.0544
VERBOSE: Tactic: 4259839 Time: 0.064288
VERBOSE: Tactic: 4325375 Time: 0.053024
VERBOSE: Tactic: 4587519 Time: 0.056896
VERBOSE: Tactic: 4653055 Time: 0.058016
VERBOSE: Tactic: 4915199 Time: 0.056448
VERBOSE: Tactic: 5177343 Time: 0.060736
VERBOSE: Tactic: 5373951 Time: 0.067232
VERBOSE: Tactic: 5636095 Time: 0.05728
VERBOSE: Tactic: 5898239 Time: 0.057792
VERBOSE: Tactic: 6225919 Time: 0.047744
VERBOSE: Tactic: 6291455 Time: 0.056704
VERBOSE: Tactic: 6422527 Time: 0.045152
VERBOSE: Tactic: 6750207 Time: 0.065472
VERBOSE: Tactic: 7012351 Time: 0.072064
VERBOSE: Tactic: 7077887 Time: 0.044864
VERBOSE: Tactic: 7340031 Time: 0.059008
VERBOSE: Tactic: 7405567 Time: 0.05952
VERBOSE: Tactic: 7602175 Time: 0.056608
VERBOSE: Tactic: 7733247 Time: 0.04624
VERBOSE: Tactic: 8191999 Time: 0.067168
VERBOSE: Tactic: 8257535 Time: 0.05904
VERBOSE: Tactic: 8323071 Time: 0.060736
VERBOSE: Tactic: 8650751 Time: 0.0632
VERBOSE: Tactic: 9109503 Time: 0.074624
VERBOSE: Tactic: 9568255 Time: 0.05648
VERBOSE: Tactic: 10354687 Time: 0.064256
VERBOSE: Tactic: 10747903 Time: 0.044768
VERBOSE: Tactic: 10944511 Time: 0.052384
VERBOSE: Fastest Tactic: 655359 Time: 0.03968
VERBOSE: --------------- Timing Runner: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 (CaskConvolution)
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 3145259992339075399
VERBOSE: Tactic: 3145259992339075399 Time: 0.025664
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_interior_nn_v1 Tactic: 4000990898022781625
VERBOSE: Tactic: 4000990898022781625 Time: 0.025344
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 4438325421691896755
VERBOSE: Tactic: 4438325421691896755 Time: 0.026688
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 4581732244273465060
VERBOSE: Tactic: 4581732244273465060 Time: 0.026368
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 4934335053031119367
VERBOSE: Tactic: 4934335053031119367 Time: 0.033184
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 6797040896965118050
VERBOSE: Tactic: 6797040896965118050 Time: 0.025536
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 8006952294591770973
VERBOSE: Tactic: 8006952294591770973 Time: 0.028928
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 8097855305881829878
VERBOSE: Tactic: 8097855305881829878 Time: 0.027584
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -7210942453088153035
VERBOSE: Tactic: -7210942453088153035 Time: 0.025504
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -6282183216199417697
VERBOSE: Tactic: -6282183216199417697 Time: 0.028032
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: -5016725782072253841
VERBOSE: Tactic: -5016725782072253841 Time: 0.032608
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_interior_nn_v1 Tactic: -1543391652455542154
VERBOSE: Tactic: -1543391652455542154 Time: 0.026784
VERBOSE: Fastest Tactic: 4000990898022781625 Time: 0.025344
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 4000990898022781625
VERBOSE: *************** Autotuning format combination: Int8(73728,2304:4,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 (CaskConvolution)
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_interior_c32_nn_v1 Tactic: 1025026069226666066
VERBOSE: Tactic: 1025026069226666066 Time: 0.02624
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: 1213457772632185722
VERBOSE: Tactic: 1213457772632185722 Time: 0.027264
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 1713441381477652893
VERBOSE: Tactic: 1713441381477652893 Time: 0.028352
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_interior_c32_nn_v1 Tactic: 2339361327868109050
VERBOSE: Tactic: 2339361327868109050 Time: 0.025664
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 8047041638267142825
VERBOSE: Tactic: 8047041638267142825 Time: 0.028384
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: -7846982807478255793
VERBOSE: Tactic: -7846982807478255793 Time: 0.026368
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_interior_c32_nn_v1 Tactic: -7686150779628967382
VERBOSE: Tactic: -7686150779628967382 Time: 0.03216
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: -6459719113600909000
VERBOSE: Tactic: -6459719113600909000 Time: 0.026624
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: -4573925292554651334
VERBOSE: Tactic: -4573925292554651334 Time: 0.026112
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_interior_c32_nn_v1 Tactic: -4208188808979933945
VERBOSE: Tactic: -4208188808979933945 Time: 0.02704
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: -3566249366964946311
VERBOSE: Tactic: -3566249366964946311 Time: 0.026656
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: -2002418013575043687
VERBOSE: Tactic: -2002418013575043687 Time: 0.026368
VERBOSE: Fastest Tactic: 2339361327868109050 Time: 0.025664
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 2339361327868109050
VERBOSE: *************** Autotuning format combination: Int8(9216,2304:32,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 (CudaGroupConvolution)
VERBOSE: CudaGroupConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 (FusedConvActConvolution)
VERBOSE: Tactic: 589823 Time: 0.064
VERBOSE: Tactic: 655359 Time: 0.031136
VERBOSE: Tactic: 786431 Time: 0.063104
VERBOSE: Tactic: 851967 Time: 0.049824
VERBOSE: Tactic: 1179647 Time: 0.05376
VERBOSE: Tactic: 1638399 Time: 0.0656
VERBOSE: Tactic: 1835007 Time: 0.067904
VERBOSE: Tactic: 2097151 Time: 0.071872
VERBOSE: Tactic: 3080191 Time: 0.03536
VERBOSE: Tactic: 3538943 Time: 0.047552
VERBOSE: Tactic: 3997695 Time: 0.065088
VERBOSE: Tactic: 4063231 Time: 0.049856
VERBOSE: Tactic: 4259839 Time: 0.06624
VERBOSE: Tactic: 4325375 Time: 0.056288
VERBOSE: Tactic: 4587519 Time: 0.06224
VERBOSE: Tactic: 4653055 Time: 0.05776
VERBOSE: Tactic: 4915199 Time: 0.057408
VERBOSE: Tactic: 5177343 Time: 0.053536
VERBOSE: Tactic: 5373951 Time: 0.06144
VERBOSE: Tactic: 5636095 Time: 0.050112
VERBOSE: Tactic: 5898239 Time: 0.052384
VERBOSE: Tactic: 6225919 Time: 0.048032
VERBOSE: Tactic: 6291455 Time: 0.053824
VERBOSE: Tactic: 6422527 Time: 0.038208
VERBOSE: Tactic: 6750207 Time: 0.073568
VERBOSE: Tactic: 7012351 Time: 0.072128
VERBOSE: Tactic: 7077887 Time: 0.042752
VERBOSE: Tactic: 7340031 Time: 0.050944
VERBOSE: Tactic: 7405567 Time: 0.055168
VERBOSE: Tactic: 7602175 Time: 0.060576
VERBOSE: Tactic: 7733247 Time: 0.039072
VERBOSE: Tactic: 8191999 Time: 0.0832
VERBOSE: Tactic: 8257535 Time: 0.054624
VERBOSE: Tactic: 8323071 Time: 0.0696
VERBOSE: Tactic: 8650751 Time: 0.0608
VERBOSE: Tactic: 9109503 Time: 0.06848
VERBOSE: Tactic: 9568255 Time: 0.057408
VERBOSE: Tactic: 10354687 Time: 0.063104
VERBOSE: Tactic: 10747903 Time: 0.040768
VERBOSE: Tactic: 10944511 Time: 0.056288
VERBOSE: Fastest Tactic: 655359 Time: 0.031136
VERBOSE: --------------- Timing Runner: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 (CaskConvolution)
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 68468667201176803
VERBOSE: Tactic: 68468667201176803 Time: 0.019936
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 1230105269624924765
VERBOSE: Tactic: 1230105269624924765 Time: 0.018816
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 2548946449357458230
VERBOSE: Tactic: 2548946449357458230 Time: 0.01936
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 3242897809704328258
VERBOSE: Tactic: 3242897809704328258 Time: 0.017632
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 3312456766204252694
VERBOSE: Tactic: 3312456766204252694 Time: 0.02048
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 3541919052468401776
VERBOSE: Tactic: 3541919052468401776 Time: 0.018464
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 3927509214678622419
VERBOSE: Tactic: 3927509214678622419 Time: 0.01648
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 4871133328510103657
VERBOSE: Tactic: 4871133328510103657 Time: 0.018816
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 8684013308930763400
VERBOSE: Tactic: 8684013308930763400 Time: 0.018784
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -9138158634332993439
VERBOSE: Tactic: -9138158634332993439 Time: 0.022432
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: -8943710627305202139
VERBOSE: Tactic: -8943710627305202139 Time: 0.017312
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: -8859846367886814331
VERBOSE: Tactic: -8859846367886814331 Time: 0.017056
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -8431788508843860955
VERBOSE: Tactic: -8431788508843860955 Time: 0.01792
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: -8382298409581540699
VERBOSE: Tactic: -8382298409581540699 Time: 0.023296
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r1s1 Tactic: -8172318747337038866
VERBOSE: Tactic: -8172318747337038866 Time: 0.022144
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: -7361755530333096258
VERBOSE: Tactic: -7361755530333096258 Time: 0.019904
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: -7106539943789766885
VERBOSE: Tactic: -7106539943789766885 Time: 0.0216
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: -6969478418607271266
VERBOSE: Tactic: -6969478418607271266 Time: 0.02128
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: -6346247605026339453
VERBOSE: Tactic: -6346247605026339453 Time: 0.01648
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -5932545395595141539
VERBOSE: Tactic: -5932545395595141539 Time: 0.01904
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: -5697614955743334137
VERBOSE: Tactic: -5697614955743334137 Time: 0.018432
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_interior_nt_v1 Tactic: -5428851623690564527
VERBOSE: Tactic: -5428851623690564527 Time: 0.019104
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: -5311474420963248369
VERBOSE: Tactic: -5311474420963248369 Time: 0.02192
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: -4260476497340370474
VERBOSE: Tactic: -4260476497340370474 Time: 0.02496
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r1s1 Tactic: -3531681826488401618
VERBOSE: Tactic: -3531681826488401618 Time: 0.023008
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_interior_nt_v1 Tactic: -3065443564418447033
VERBOSE: Tactic: -3065443564418447033 Time: 0.018912
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -1841683966837205309
VERBOSE: Tactic: -1841683966837205309 Time: 0.023328
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: -1494157908358500249
VERBOSE: Tactic: -1494157908358500249 Time: 0.018464
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -1405083410359886949
VERBOSE: Tactic: -1405083410359886949 Time: 0.019936
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_interior_nt_v1 Tactic: -870280893819213650
VERBOSE: Tactic: -870280893819213650 Time: 0.023392
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_interior_nt_v1 Tactic: -545024964146453661
VERBOSE: Tactic: -545024964146453661 Time: 0.019744
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -496455309852654971
VERBOSE: Tactic: -496455309852654971 Time: 0.018816
VERBOSE: Fastest Tactic: 3927509214678622419 Time: 0.01648
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 3927509214678622419
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(589824,9216,96,1) -> Float(2359296,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: dla_up_ida_2_up_1.1 (CudnnDeconvolution)
VERBOSE: Tactic: 0 Time: 0.567424
VERBOSE: Tactic: 1 Time: 0.56768
VERBOSE: Tactic: 3 Time: 64.5733
VERBOSE: Tactic: 24 Time: 1.72422
VERBOSE: Tactic: 25 Time: 4.31853
VERBOSE: Tactic: 27 Time: 34.2485
VERBOSE: Fastest Tactic: 0 Time: 0.567424
VERBOSE: --------------- Timing Runner: dla_up_ida_2_up_1.1 (GemmDeconvolution)
VERBOSE: Tactic: 0 Time: 0.713312
VERBOSE: Fastest Tactic: 0 Time: 0.713312
VERBOSE: --------------- Timing Runner: dla_up_ida_2_up_1.1 (CaskDeconvolution)
VERBOSE: CaskDeconvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: dla_up_ida_2_up_1.1 (CaskDeconvolutionV2)
VERBOSE: dla_up_ida_2_up_1.1 Set Tactic Name: sm50_xmma_deconv_generic_f32f32_f32_f32_nchwkcrs_nchw Tactic: 1108745107350470483
VERBOSE: Tactic: 1108745107350470483 Time: 10000
VERBOSE: Fastest Tactic: 1108745107350470483 Time: 10000
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CudnnDeconvolution Tactic: 0
VERBOSE: *************** Autotuning format combination: Float(589824,1,6144,64) -> Float(2359296,1,12288,64) ***************
VERBOSE: --------------- Timing Runner: dla_up_ida_2_up_1.1 (CudnnDeconvolution)
VERBOSE: CudnnDeconvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: dla_up_ida_2_up_1.1 (GemmDeconvolution)
VERBOSE: GemmDeconvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: dla_up_ida_2_up_1.1 (CaskDeconvolution)
VERBOSE: CaskDeconvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: dla_up_ida_2_up_1.1 (CaskDeconvolutionV2)
VERBOSE: dla_up_ida_2_up_1.1 Set Tactic Name: sm70_xmma_deconv_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage1_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: -6927323623127951136
VERBOSE: Tactic: -6927323623127951136 Time: 6.6017
VERBOSE: dla_up_ida_2_up_1.1 Set Tactic Name: sm70_xmma_deconv_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x128x8_stage1_warpsize2x4x1_g1_ffma_aligna4_alignc4 Tactic: -5811489873080635378
VERBOSE: Tactic: -5811489873080635378 Time: 17.5022
VERBOSE: dla_up_ida_2_up_1.1 Set Tactic Name: sm70_xmma_deconv_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x64x8_stage1_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: -500373307302467644
VERBOSE: Tactic: -500373307302467644 Time: 10.266
VERBOSE: Fastest Tactic: -6927323623127951136 Time: 6.6017
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskDeconvolutionV2 Tactic: -6927323623127951136
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(589824,2304,48,1) -> Float(589824,2304,48,1) ***************
VERBOSE: --------------- Timing Runner: base_level4_tree1_project_1_output_quant.1 (Scale)
VERBOSE: Tactic: 0 Time: 0.008896
VERBOSE: Fastest Tactic: 0 Time: 0.008896
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: *************** Autotuning format combination: Int8(147456,2304:4,48,1) -> Float(589824,2304,48,1) ***************
VERBOSE: --------------- Timing Runner: base_level4_tree1_project_1_output_quant.1 (Scale)
VERBOSE: Tactic: 0 Time: 0.013408
VERBOSE: Fastest Tactic: 0 Time: 0.013408
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: *************** Autotuning format combination: Int8(18432,2304:32,48,1) -> Float(589824,2304,48,1) ***************
VERBOSE: --------------- Timing Runner: base_level4_tree1_project_1_output_quant.1 (Scale)
VERBOSE: Tactic: 0 Time: 0.011872
VERBOSE: Fastest Tactic: 0 Time: 0.011872
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(2359296,36864,192,1) -> Int8(4718592,36864,192,1) ***************
VERBOSE: *************** Autotuning format combination: Float(2359296,36864,192,1) -> Int8(1179648,36864:4,192,1) ***************
VERBOSE: *************** Autotuning format combination: Float(2359296,36864,192,1) -> Int8(147456,36864:32,192,1) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(2359296,36864,192,1) -> Int8(4718592,36864,192,1) ***************
VERBOSE: *************** Autotuning format combination: Float(2359296,36864,192,1) -> Int8(1179648,36864:4,192,1) ***************
VERBOSE: *************** Autotuning format combination: Float(2359296,36864,192,1) -> Int8(147456,36864:32,192,1) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(147456,2304:4,48,1), Float(589824,2304,48,1) -> Float(589824,2304,48,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 (CaskConvolution)
VERBOSE: self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 892787096507693407
VERBOSE: Tactic: 892787096507693407 Time: 0.12256
VERBOSE: self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 1204440019753223942
VERBOSE: Tactic: 1204440019753223942 Time: 0.176448
VERBOSE: self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 1659301557717208403
VERBOSE: Tactic: 1659301557717208403 Time: 0.146304
VERBOSE: self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 2057291331119027912
VERBOSE: Tactic: 2057291331119027912 Time: 0.128928
VERBOSE: self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 3275977259705528576
VERBOSE: Tactic: 3275977259705528576 Time: 0.170912
VERBOSE: self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 5623454780463195174
VERBOSE: Tactic: 5623454780463195174 Time: 0.148032
VERBOSE: self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: 8930254200803946944
VERBOSE: Tactic: 8930254200803946944 Time: 0.120736
VERBOSE: self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: -9204333525109552344
VERBOSE: Tactic: -9204333525109552344 Time: 0.124352
VERBOSE: self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -4973811344878172338
VERBOSE: Tactic: -4973811344878172338 Time: 0.118272
VERBOSE: self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: -1228371230285617088
VERBOSE: Tactic: -1228371230285617088 Time: 0.120832
VERBOSE: Fastest Tactic: -4973811344878172338 Time: 0.118272
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -4973811344878172338
VERBOSE: *************** Autotuning format combination: Int8(18432,2304:32,48,1), Float(589824,2304,48,1) -> Float(589824,2304,48,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 (CaskConvolution)
VERBOSE: CaskConvolution has no valid tactics for this config, skipping
VERBOSE: *************** Autotuning format combination: Int8(18432,2304:32,48,1), Float(18432,2304:32,48,1) -> Float(18432,2304:32,48,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 (CaskConvolution)
VERBOSE: self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: 394365917225754726
VERBOSE: Tactic: 394365917225754726 Time: 0.05808
VERBOSE: self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 924563784895318224
VERBOSE: Tactic: 924563784895318224 Time: 0.055776
VERBOSE: self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: 3451841782205411203
VERBOSE: Tactic: 3451841782205411203 Time: 0.05648
VERBOSE: self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 6096719469361499298
VERBOSE: Tactic: 6096719469361499298 Time: 0.053984
VERBOSE: self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: 7785217228143857868
VERBOSE: Tactic: 7785217228143857868 Time: 0.075776
VERBOSE: self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: 8315790488934712458
VERBOSE: Tactic: 8315790488934712458 Time: 0.056
VERBOSE: self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: 9080823067063042887
VERBOSE: Tactic: 9080823067063042887 Time: 0.06272
VERBOSE: self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -8462194455331556195
VERBOSE: Tactic: -8462194455331556195 Time: 0.055424
VERBOSE: self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: -8165074686865110847
VERBOSE: Tactic: -8165074686865110847 Time: 0.057984
VERBOSE: self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -7638944668269666085
VERBOSE: Tactic: -7638944668269666085 Time: 0.076736
VERBOSE: self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: -7185527339793611699
VERBOSE: Tactic: -7185527339793611699 Time: 0.056608
VERBOSE: self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -5979101256828290173
VERBOSE: Tactic: -5979101256828290173 Time: 0.062112
VERBOSE: self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -5763174003249488100
VERBOSE: Tactic: -5763174003249488100 Time: 0.061184
VERBOSE: self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: -5612459945002849429
VERBOSE: Tactic: -5612459945002849429 Time: 0.063168
VERBOSE: self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -5250790590226149674
VERBOSE: Tactic: -5250790590226149674 Time: 0.075776
VERBOSE: self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -4911193113143178408
VERBOSE: Tactic: -4911193113143178408 Time: 0.077504
VERBOSE: self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -4831366370915083630
VERBOSE: Tactic: -4831366370915083630 Time: 0.052864
VERBOSE: self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -4563432698383308679
VERBOSE: Tactic: -4563432698383308679 Time: 0.075392
VERBOSE: self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -4067022988143035981
VERBOSE: Tactic: -4067022988143035981 Time: 0.0608
VERBOSE: self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -3936136542475827126
VERBOSE: Tactic: -3936136542475827126 Time: 0.059392
VERBOSE: self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -3784829056659735491
VERBOSE: Tactic: -3784829056659735491 Time: 0.052672
VERBOSE: self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -3720553804749394441
VERBOSE: Tactic: -3720553804749394441 Time: 0.076384
VERBOSE: self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -2697681528489059028
VERBOSE: Tactic: -2697681528489059028 Time: 0.061504
VERBOSE: self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: -733152064595858464
VERBOSE: Tactic: -733152064595858464 Time: 0.054848
VERBOSE: Fastest Tactic: -3784829056659735491 Time: 0.052672
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -3784829056659735491
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(1179648,36864:4,192,1) -> Int8(1179648,36864:4,192,1) ***************
VERBOSE: --------------- Timing Runner: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 (FusedConvActConvolution)
VERBOSE: Tactic: 524287 Time: 0.222176
VERBOSE: Tactic: 720895 Time: 0.208704
VERBOSE: Tactic: 983039 Time: 0.204288
VERBOSE: Tactic: 1048575 Time: 0.218784
VERBOSE: Tactic: 1703935 Time: 0.216832
VERBOSE: Tactic: 1769471 Time: 0.30048
VERBOSE: Tactic: 2424831 Time: 0.320512
VERBOSE: Tactic: 2621439 Time: 0.240768
VERBOSE: Tactic: 3014655 Time: 0.21328
VERBOSE: Tactic: 3604479 Time: 0.210944
VERBOSE: Tactic: 5046271 Time: 0.223232
VERBOSE: Tactic: 6488063 Time: 0.224896
VERBOSE: Tactic: 7274495 Time: 0.251424
VERBOSE: Tactic: 7864319 Time: 0.238592
VERBOSE: Tactic: 8847359 Time: 0.252608
VERBOSE: Tactic: 9043967 Time: 0.221536
VERBOSE: Tactic: 9961471 Time: 0.319136
VERBOSE: Tactic: 10027007 Time: 0.219552
VERBOSE: Tactic: 10485759 Time: 0.217312
VERBOSE: Tactic: 10682367 Time: 0.235808
VERBOSE: Tactic: 10813439 Time: 0.19984
VERBOSE: Fastest Tactic: 10813439 Time: 0.19984
VERBOSE: --------------- Timing Runner: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 (CaskConvolution)
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 4438325421691896755
VERBOSE: Tactic: 4438325421691896755 Time: 0.216544
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 4581732244273465060
VERBOSE: Tactic: 4581732244273465060 Time: 0.213664
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 4934335053031119367
VERBOSE: Tactic: 4934335053031119367 Time: 0.221728
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 6797040896965118050
VERBOSE: Tactic: 6797040896965118050 Time: 0.395488
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 8006952294591770973
VERBOSE: Tactic: 8006952294591770973 Time: 0.24704
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -7210942453088153035
VERBOSE: Tactic: -7210942453088153035 Time: 0.394368
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -6282183216199417697
VERBOSE: Tactic: -6282183216199417697 Time: 0.239552
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: -5026383765466876607
VERBOSE: Tactic: -5026383765466876607 Time: 0.425248
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: -5016725782072253841
VERBOSE: Tactic: -5016725782072253841 Time: 0.219456
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: -1370999262391786833
VERBOSE: Tactic: -1370999262391786833 Time: 0.209248
VERBOSE: Fastest Tactic: -1370999262391786833 Time: 0.209248
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: FusedConvActConvolution Tactic: 10813439
VERBOSE: *************** Autotuning format combination: Int8(1179648,36864:4,192,1) -> Int8(147456,36864:32,192,1) ***************
VERBOSE: --------------- Timing Runner: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 (CaskConvolution)
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: 1213457772632185722
VERBOSE: Tactic: 1213457772632185722 Time: 0.222144
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 1713441381477652893
VERBOSE: Tactic: 1713441381477652893 Time: 0.247712
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_xregs_large_c32_nn_v1 Tactic: 7125598890155666458
VERBOSE: Tactic: 7125598890155666458 Time: 0.209248
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 8047041638267142825
VERBOSE: Tactic: 8047041638267142825 Time: 0.241472
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: -7846982807478255793
VERBOSE: Tactic: -7846982807478255793 Time: 0.21408
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: -6459719113600909000
VERBOSE: Tactic: -6459719113600909000 Time: 0.219968
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: -4573925292554651334
VERBOSE: Tactic: -4573925292554651334 Time: 0.393088
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: -3566249366964946311
VERBOSE: Tactic: -3566249366964946311 Time: 0.216736
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: -2002418013575043687
VERBOSE: Tactic: -2002418013575043687 Time: 0.395008
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_xregs_large_c32_nn_v1 Tactic: -1659631603542281459
VERBOSE: Tactic: -1659631603542281459 Time: 0.423552
VERBOSE: Fastest Tactic: 7125598890155666458 Time: 0.209248
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 7125598890155666458
VERBOSE: *************** Autotuning format combination: Int8(147456,36864:32,192,1) -> Int8(147456,36864:32,192,1) ***************
VERBOSE: --------------- Timing Runner: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 (CudaGroupConvolution)
VERBOSE: CudaGroupConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 (FusedConvActConvolution)
VERBOSE: Tactic: 524287 Time: 0.226112
VERBOSE: Tactic: 720895 Time: 0.20416
VERBOSE: Tactic: 983039 Time: 0.2032
VERBOSE: Tactic: 1048575 Time: 0.223904
VERBOSE: Tactic: 1703935 Time: 0.222688
VERBOSE: Tactic: 1769471 Time: 0.290784
VERBOSE: Tactic: 2424831 Time: 0.375328
VERBOSE: Tactic: 2621439 Time: 0.242848
VERBOSE: Tactic: 3014655 Time: 0.21904
VERBOSE: Tactic: 3604479 Time: 0.219264
VERBOSE: Tactic: 5046271 Time: 0.219616
VERBOSE: Tactic: 6488063 Time: 0.231552
VERBOSE: Tactic: 7274495 Time: 0.241984
VERBOSE: Tactic: 7864319 Time: 0.256992
VERBOSE: Tactic: 8847359 Time: 0.253568
VERBOSE: Tactic: 9043967 Time: 0.222368
VERBOSE: Tactic: 9961471 Time: 0.365536
VERBOSE: Tactic: 10027007 Time: 0.221824
VERBOSE: Tactic: 10485759 Time: 0.218176
VERBOSE: Tactic: 10682367 Time: 0.241568
VERBOSE: Tactic: 10813439 Time: 0.200128
VERBOSE: Fastest Tactic: 10813439 Time: 0.200128
VERBOSE: --------------- Timing Runner: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 (CaskConvolution)
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 289888059097454627
VERBOSE: Tactic: 289888059097454627 Time: 0.137344
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 680740992583869928
VERBOSE: Tactic: 680740992583869928 Time: 0.14784
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 1230105269624924765
VERBOSE: Tactic: 1230105269624924765 Time: 0.07664
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 1388866374720163187
VERBOSE: Tactic: 1388866374720163187 Time: 0.137408
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 2133329569091732311
VERBOSE: Tactic: 2133329569091732311 Time: 0.130976
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 2579824863892891529
VERBOSE: Tactic: 2579824863892891529 Time: 0.157248
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: 4099749238296892232
VERBOSE: Tactic: 4099749238296892232 Time: 0.077024
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 4214794893922618058
VERBOSE: Tactic: 4214794893922618058 Time: 0.129632
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 4384868749799132354
VERBOSE: Tactic: 4384868749799132354 Time: 0.156768
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 4871133328510103657
VERBOSE: Tactic: 4871133328510103657 Time: 0.076064
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 4931167631624420067
VERBOSE: Tactic: 4931167631624420067 Time: 0.260736
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 5641967928706599451
VERBOSE: Tactic: 5641967928706599451 Time: 0.185696
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 6394572396369862482
VERBOSE: Tactic: 6394572396369862482 Time: 0.18704
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 7048234086361926570
VERBOSE: Tactic: 7048234086361926570 Time: 0.104832
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -9138158634332993439
VERBOSE: Tactic: -9138158634332993439 Time: 0.133728
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -9108166971364503411
VERBOSE: Tactic: -9108166971364503411 Time: 0.105696
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: -8861822316054763526
VERBOSE: Tactic: -8861822316054763526 Time: 0.140832
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: -8475551154769412306
VERBOSE: Tactic: -8475551154769412306 Time: 0.0856
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -8431788508843860955
VERBOSE: Tactic: -8431788508843860955 Time: 0.124576
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: -6371781333659293809
VERBOSE: Tactic: -6371781333659293809 Time: 0.088064
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -5932545395595141539
VERBOSE: Tactic: -5932545395595141539 Time: 0.09216
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: -5180570335464125033
VERBOSE: Tactic: -5180570335464125033 Time: 0.083776
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: -2965659229411050116
VERBOSE: Tactic: -2965659229411050116 Time: 0.096704
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -2499089240293650188
VERBOSE: Tactic: -2499089240293650188 Time: 0.148128
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -2328318099174473157
VERBOSE: Tactic: -2328318099174473157 Time: 0.089504
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -1841683966837205309
VERBOSE: Tactic: -1841683966837205309 Time: 0.12912
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -1405083410359886949
VERBOSE: Tactic: -1405083410359886949 Time: 0.095488
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -1044618751912149244
VERBOSE: Tactic: -1044618751912149244 Time: 0.130016
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -496455309852654971
VERBOSE: Tactic: -496455309852654971 Time: 0.125024
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -409197672346571968
VERBOSE: Tactic: -409197672346571968 Time: 0.13232
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: -366411318217594794
VERBOSE: Tactic: -366411318217594794 Time: 0.137056
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: -351548418071036983
VERBOSE: Tactic: -351548418071036983 Time: 0.266176
VERBOSE: Fastest Tactic: 4871133328510103657 Time: 0.076064
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 4871133328510103657
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(589824,2304,48,1) -> Int8(589824,2304,48,1) ***************
VERBOSE: --------------- Timing Runner: base_level4_tree1_tree1_relu_output_quant.1TensorQ (Scale)
VERBOSE: Tactic: 0 Time: 0.008032
VERBOSE: Fastest Tactic: 0 Time: 0.008032
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: *************** Autotuning format combination: Float(589824,2304,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: --------------- Timing Runner: base_level4_tree1_tree1_relu_output_quant.1TensorQ (Scale)
VERBOSE: Tactic: 0 Time: 0.010912
VERBOSE: Fastest Tactic: 0 Time: 0.010912
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: *************** Autotuning format combination: Float(589824,2304,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: --------------- Timing Runner: base_level4_tree1_tree1_relu_output_quant.1TensorQ (Scale)
VERBOSE: Tactic: 0 Time: 0.01088
VERBOSE: Fastest Tactic: 0 Time: 0.01088
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(147456,2304:4,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 (FusedConvActConvolution)
VERBOSE: Tactic: 524287 Time: 0.131744
VERBOSE: Tactic: 720895 Time: 0.108096
VERBOSE: Tactic: 983039 Time: 0.107456
VERBOSE: Tactic: 1048575 Time: 0.110784
VERBOSE: Tactic: 1703935 Time: 0.107808
VERBOSE: Tactic: 1769471 Time: 0.1264
VERBOSE: Tactic: 2424831 Time: 0.130944
VERBOSE: Tactic: 2621439 Time: 0.11472
VERBOSE: Tactic: 3014655 Time: 0.111296
VERBOSE: Tactic: 3604479 Time: 0.111104
VERBOSE: Tactic: 5046271 Time: 0.117376
VERBOSE: Tactic: 6488063 Time: 0.131776
VERBOSE: Tactic: 7274495 Time: 0.1176
VERBOSE: Tactic: 7864319 Time: 0.117792
VERBOSE: Tactic: 8847359 Time: 0.11536
VERBOSE: Tactic: 9043967 Time: 0.109664
VERBOSE: Tactic: 9961471 Time: 0.13584
VERBOSE: Tactic: 10027007 Time: 0.126208
VERBOSE: Tactic: 10485759 Time: 0.110432
VERBOSE: Tactic: 10682367 Time: 0.11344
VERBOSE: Tactic: 10813439 Time: 0.108896
VERBOSE: Fastest Tactic: 983039 Time: 0.107456
VERBOSE: --------------- Timing Runner: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 (CaskConvolution)
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 4438325421691896755
VERBOSE: Tactic: 4438325421691896755 Time: 0.117088
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 4581732244273465060
VERBOSE: Tactic: 4581732244273465060 Time: 0.111712
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 4934335053031119367
VERBOSE: Tactic: 4934335053031119367 Time: 0.160352
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 6797040896965118050
VERBOSE: Tactic: 6797040896965118050 Time: 0.104896
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 8006952294591770973
VERBOSE: Tactic: 8006952294591770973 Time: 0.135616
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -7210942453088153035
VERBOSE: Tactic: -7210942453088153035 Time: 0.102848
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -6282183216199417697
VERBOSE: Tactic: -6282183216199417697 Time: 0.13072
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: -5026383765466876607
VERBOSE: Tactic: -5026383765466876607 Time: 0.105344
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: -5016725782072253841
VERBOSE: Tactic: -5016725782072253841 Time: 0.157216
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: -1370999262391786833
VERBOSE: Tactic: -1370999262391786833 Time: 0.109024
VERBOSE: Fastest Tactic: -7210942453088153035 Time: 0.102848
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -7210942453088153035
VERBOSE: *************** Autotuning format combination: Int8(147456,2304:4,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 (CaskConvolution)
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: 1213457772632185722
VERBOSE: Tactic: 1213457772632185722 Time: 0.160192
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 1713441381477652893
VERBOSE: Tactic: 1713441381477652893 Time: 0.13584
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_xregs_large_c32_nn_v1 Tactic: 7125598890155666458
VERBOSE: Tactic: 7125598890155666458 Time: 0.109312
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 8047041638267142825
VERBOSE: Tactic: 8047041638267142825 Time: 0.129792
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: -7846982807478255793
VERBOSE: Tactic: -7846982807478255793 Time: 0.111808
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: -6459719113600909000
VERBOSE: Tactic: -6459719113600909000 Time: 0.157216
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: -4573925292554651334
VERBOSE: Tactic: -4573925292554651334 Time: 0.103072
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: -3566249366964946311
VERBOSE: Tactic: -3566249366964946311 Time: 0.116928
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: -2002418013575043687
VERBOSE: Tactic: -2002418013575043687 Time: 0.1048
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_xregs_large_c32_nn_v1 Tactic: -1659631603542281459
VERBOSE: Tactic: -1659631603542281459 Time: 0.105408
VERBOSE: Fastest Tactic: -4573925292554651334 Time: 0.103072
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -4573925292554651334
VERBOSE: *************** Autotuning format combination: Int8(18432,2304:32,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 (CudaGroupConvolution)
VERBOSE: CudaGroupConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 (FusedConvActConvolution)
VERBOSE: Tactic: 524287 Time: 0.133984
VERBOSE: Tactic: 720895 Time: 0.10688
VERBOSE: Tactic: 983039 Time: 0.108224
VERBOSE: Tactic: 1048575 Time: 0.129312
VERBOSE: Tactic: 1703935 Time: 0.110624
VERBOSE: Tactic: 1769471 Time: 0.12336
VERBOSE: Tactic: 2424831 Time: 0.148224
VERBOSE: Tactic: 2621439 Time: 0.115296
VERBOSE: Tactic: 3014655 Time: 0.114208
VERBOSE: Tactic: 3604479 Time: 0.114016
VERBOSE: Tactic: 5046271 Time: 0.1184
VERBOSE: Tactic: 6488063 Time: 0.13504
VERBOSE: Tactic: 7274495 Time: 0.11472
VERBOSE: Tactic: 7864319 Time: 0.121152
VERBOSE: Tactic: 8847359 Time: 0.115488
VERBOSE: Tactic: 9043967 Time: 0.112
VERBOSE: Tactic: 9961471 Time: 0.147168
VERBOSE: Tactic: 10027007 Time: 0.128672
VERBOSE: Tactic: 10485759 Time: 0.110976
VERBOSE: Tactic: 10682367 Time: 0.114688
VERBOSE: Tactic: 10813439 Time: 0.110208
VERBOSE: Fastest Tactic: 720895 Time: 0.10688
VERBOSE: --------------- Timing Runner: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 (CaskConvolution)
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 289888059097454627
VERBOSE: Tactic: 289888059097454627 Time: 0.058208
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 680740992583869928
VERBOSE: Tactic: 680740992583869928 Time: 0.046752
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 1230105269624924765
VERBOSE: Tactic: 1230105269624924765 Time: 0.039712
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 1388866374720163187
VERBOSE: Tactic: 1388866374720163187 Time: 0.0704
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 2133329569091732311
VERBOSE: Tactic: 2133329569091732311 Time: 0.042912
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 2579824863892891529
VERBOSE: Tactic: 2579824863892891529 Time: 0.08272
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: 4099749238296892232
VERBOSE: Tactic: 4099749238296892232 Time: 0.040448
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 4214794893922618058
VERBOSE: Tactic: 4214794893922618058 Time: 0.042688
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 4384868749799132354
VERBOSE: Tactic: 4384868749799132354 Time: 0.084736
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 4871133328510103657
VERBOSE: Tactic: 4871133328510103657 Time: 0.038912
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 4931167631624420067
VERBOSE: Tactic: 4931167631624420067 Time: 0.056864
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 5641967928706599451
VERBOSE: Tactic: 5641967928706599451 Time: 0.100128
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 6394572396369862482
VERBOSE: Tactic: 6394572396369862482 Time: 0.10192
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 7048234086361926570
VERBOSE: Tactic: 7048234086361926570 Time: 0.05728
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -9138158634332993439
VERBOSE: Tactic: -9138158634332993439 Time: 0.058112
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -9108166971364503411
VERBOSE: Tactic: -9108166971364503411 Time: 0.05792
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: -8861822316054763526
VERBOSE: Tactic: -8861822316054763526 Time: 0.058816
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: -8475551154769412306
VERBOSE: Tactic: -8475551154769412306 Time: 0.047392
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -8431788508843860955
VERBOSE: Tactic: -8431788508843860955 Time: 0.037248
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: -6371781333659293809
VERBOSE: Tactic: -6371781333659293809 Time: 0.045472
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -5932545395595141539
VERBOSE: Tactic: -5932545395595141539 Time: 0.0424
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: -5180570335464125033
VERBOSE: Tactic: -5180570335464125033 Time: 0.047744
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: -2965659229411050116
VERBOSE: Tactic: -2965659229411050116 Time: 0.04512
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -2499089240293650188
VERBOSE: Tactic: -2499089240293650188 Time: 0.046784
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -2328318099174473157
VERBOSE: Tactic: -2328318099174473157 Time: 0.045568
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -1841683966837205309
VERBOSE: Tactic: -1841683966837205309 Time: 0.058688
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -1405083410359886949
VERBOSE: Tactic: -1405083410359886949 Time: 0.04464
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -1044618751912149244
VERBOSE: Tactic: -1044618751912149244 Time: 0.04
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -496455309852654971
VERBOSE: Tactic: -496455309852654971 Time: 0.037056
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -409197672346571968
VERBOSE: Tactic: -409197672346571968 Time: 0.057856
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: -366411318217594794
VERBOSE: Tactic: -366411318217594794 Time: 0.071168
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: -351548418071036983
VERBOSE: Tactic: -351548418071036983 Time: 0.057728
VERBOSE: Fastest Tactic: -496455309852654971 Time: 0.037056
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -496455309852654971
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(589824,2304,48,1) -> Int8(1179648,2304,48,1) ***************
VERBOSE: --------------- Timing Runner: quant_input.56TensorQ_clone_1 (Scale)
VERBOSE: Tactic: 0 Time: 0.00864
VERBOSE: Fastest Tactic: 0 Time: 0.00864
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: *************** Autotuning format combination: Float(589824,2304,48,1) -> Int8(294912,2304:4,48,1) ***************
VERBOSE: --------------- Timing Runner: quant_input.56TensorQ_clone_1 (Scale)
VERBOSE: Tactic: 0 Time: 0.011072
VERBOSE: Fastest Tactic: 0 Time: 0.011072
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: *************** Autotuning format combination: Float(589824,2304,48,1) -> Int8(36864,2304:32,48,1) ***************
VERBOSE: --------------- Timing Runner: quant_input.56TensorQ_clone_1 (Scale)
VERBOSE: Tactic: 0 Time: 0.01088
VERBOSE: Fastest Tactic: 0 Time: 0.01088
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(147456,2304:4,48,1), Int8(147456,2304:4,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 (FusedConvActConvolution)
VERBOSE: FusedConvActConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 (CaskConvolution)
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 4438325421691896755
VERBOSE: Tactic: 4438325421691896755 Time: 0.11904
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 4581732244273465060
VERBOSE: Tactic: 4581732244273465060 Time: 0.113952
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 4934335053031119367
VERBOSE: Tactic: 4934335053031119367 Time: 0.164192
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 6797040896965118050
VERBOSE: Tactic: 6797040896965118050 Time: 0.106528
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 8006952294591770973
VERBOSE: Tactic: 8006952294591770973 Time: 0.1384
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -7210942453088153035
VERBOSE: Tactic: -7210942453088153035 Time: 0.104512
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -6282183216199417697
VERBOSE: Tactic: -6282183216199417697 Time: 0.13216
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: -5026383765466876607
VERBOSE: Tactic: -5026383765466876607 Time: 0.107072
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: -5016725782072253841
VERBOSE: Tactic: -5016725782072253841 Time: 0.159328
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: -1370999262391786833
VERBOSE: Tactic: -1370999262391786833 Time: 0.111104
VERBOSE: Fastest Tactic: -7210942453088153035 Time: 0.104512
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -7210942453088153035
VERBOSE: *************** Autotuning format combination: Int8(147456,2304:4,48,1), Int8(18432,2304:32,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 (CaskConvolution)
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: 1213457772632185722
VERBOSE: Tactic: 1213457772632185722 Time: 0.162272
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 1713441381477652893
VERBOSE: Tactic: 1713441381477652893 Time: 0.137696
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_xregs_large_c32_nn_v1 Tactic: 7125598890155666458
VERBOSE: Tactic: 7125598890155666458 Time: 0.11168
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 8047041638267142825
VERBOSE: Tactic: 8047041638267142825 Time: 0.132352
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: -7846982807478255793
VERBOSE: Tactic: -7846982807478255793 Time: 0.114208
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: -6459719113600909000
VERBOSE: Tactic: -6459719113600909000 Time: 0.160032
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: -4573925292554651334
VERBOSE: Tactic: -4573925292554651334 Time: 0.105376
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: -3566249366964946311
VERBOSE: Tactic: -3566249366964946311 Time: 0.11952
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: -2002418013575043687
VERBOSE: Tactic: -2002418013575043687 Time: 0.1072
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_xregs_large_c32_nn_v1 Tactic: -1659631603542281459
VERBOSE: Tactic: -1659631603542281459 Time: 0.108
VERBOSE: Fastest Tactic: -4573925292554651334 Time: 0.105376
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -4573925292554651334
VERBOSE: *************** Autotuning format combination: Int8(18432,2304:32,48,1), Int8(18432,2304:32,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 (CudaGroupConvolution)
VERBOSE: CudaGroupConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 (FusedConvActConvolution)
VERBOSE: FusedConvActConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 (CaskConvolution)
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 289888059097454627
VERBOSE: Tactic: 289888059097454627 Time: 0.059456
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 680740992583869928
VERBOSE: Tactic: 680740992583869928 Time: 0.047264
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 1230105269624924765
VERBOSE: Tactic: 1230105269624924765 Time: 0.041056
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 1388866374720163187
VERBOSE: Tactic: 1388866374720163187 Time: 0.07088
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 2133329569091732311
VERBOSE: Tactic: 2133329569091732311 Time: 0.04496
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 2579824863892891529
VERBOSE: Tactic: 2579824863892891529 Time: 0.084832
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: 4099749238296892232
VERBOSE: Tactic: 4099749238296892232 Time: 0.041472
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 4214794893922618058
VERBOSE: Tactic: 4214794893922618058 Time: 0.044928
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 4384868749799132354
VERBOSE: Tactic: 4384868749799132354 Time: 0.082656
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 4871133328510103657
VERBOSE: Tactic: 4871133328510103657 Time: 0.04
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 4931167631624420067
VERBOSE: Tactic: 4931167631624420067 Time: 0.0584
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 5641967928706599451
VERBOSE: Tactic: 5641967928706599451 Time: 0.099744
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 6394572396369862482
VERBOSE: Tactic: 6394572396369862482 Time: 0.101696
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 7048234086361926570
VERBOSE: Tactic: 7048234086361926570 Time: 0.05792
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -9138158634332993439
VERBOSE: Tactic: -9138158634332993439 Time: 0.059776
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -9108166971364503411
VERBOSE: Tactic: -9108166971364503411 Time: 0.05856
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: -8861822316054763526
VERBOSE: Tactic: -8861822316054763526 Time: 0.061216
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: -8475551154769412306
VERBOSE: Tactic: -8475551154769412306 Time: 0.049664
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -8431788508843860955
VERBOSE: Tactic: -8431788508843860955 Time: 0.038464
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: -6371781333659293809
VERBOSE: Tactic: -6371781333659293809 Time: 0.046752
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -5932545395595141539
VERBOSE: Tactic: -5932545395595141539 Time: 0.04368
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: -5180570335464125033
VERBOSE: Tactic: -5180570335464125033 Time: 0.050112
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: -2965659229411050116
VERBOSE: Tactic: -2965659229411050116 Time: 0.046336
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -2499089240293650188
VERBOSE: Tactic: -2499089240293650188 Time: 0.048256
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -2328318099174473157
VERBOSE: Tactic: -2328318099174473157 Time: 0.04672
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -1841683966837205309
VERBOSE: Tactic: -1841683966837205309 Time: 0.060448
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -1405083410359886949
VERBOSE: Tactic: -1405083410359886949 Time: 0.045984
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -1044618751912149244
VERBOSE: Tactic: -1044618751912149244 Time: 0.041152
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -496455309852654971
VERBOSE: Tactic: -496455309852654971 Time: 0.03824
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -409197672346571968
VERBOSE: Tactic: -409197672346571968 Time: 0.059392
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: -366411318217594794
VERBOSE: Tactic: -366411318217594794 Time: 0.071456
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: -351548418071036983
VERBOSE: Tactic: -351548418071036983 Time: 0.059584
VERBOSE: Fastest Tactic: -496455309852654971 Time: 0.03824
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -496455309852654971
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(294912,2304:4,48,1) -> Float(589824,2304,48,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 (CaskConvolution)
VERBOSE: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 892787096507693407
VERBOSE: Tactic: 892787096507693407 Time: 0.032544
VERBOSE: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 1204440019753223942
VERBOSE: Tactic: 1204440019753223942 Time: 0.045472
VERBOSE: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 1659301557717208403
VERBOSE: Tactic: 1659301557717208403 Time: 0.038976
VERBOSE: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 2057291331119027912
VERBOSE: Tactic: 2057291331119027912 Time: 0.03504
VERBOSE: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 3275977259705528576
VERBOSE: Tactic: 3275977259705528576 Time: 0.034592
VERBOSE: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 5623454780463195174
VERBOSE: Tactic: 5623454780463195174 Time: 0.039296
VERBOSE: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: -9204333525109552344
VERBOSE: Tactic: -9204333525109552344 Time: 0.034688
VERBOSE: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: -7924103240988931433
VERBOSE: Tactic: -7924103240988931433 Time: 0.032992
VERBOSE: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_xregs_interior_nn_v1 Tactic: -7489650117016530013
VERBOSE: Tactic: -7489650117016530013 Time: 0.034336
VERBOSE: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -4973811344878172338
VERBOSE: Tactic: -4973811344878172338 Time: 0.032384
VERBOSE: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: -3908975881807046106
VERBOSE: Tactic: -3908975881807046106 Time: 0.036064
VERBOSE: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_interior_nn_v1 Tactic: -1765942417666394360
VERBOSE: Tactic: -1765942417666394360 Time: 0.032288
VERBOSE: Fastest Tactic: -1765942417666394360 Time: 0.032288
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -1765942417666394360
VERBOSE: *************** Autotuning format combination: Int8(36864,2304:32,48,1) -> Float(589824,2304,48,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 (CaskConvolution)
VERBOSE: CaskConvolution has no valid tactics for this config, skipping
VERBOSE: *************** Autotuning format combination: Int8(36864,2304:32,48,1) -> Float(18432,2304:32,48,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 (CaskConvolution)
VERBOSE: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: 394365917225754726
VERBOSE: Tactic: 394365917225754726 Time: 0.020384
VERBOSE: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 924563784895318224
VERBOSE: Tactic: 924563784895318224 Time: 0.02
VERBOSE: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_interior_nt_v1 Tactic: 4531028070024747144
VERBOSE: Tactic: 4531028070024747144 Time: 0.020736
VERBOSE: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_interior_nt_v1 Tactic: 4697540470896098800
VERBOSE: Tactic: 4697540470896098800 Time: 0.019456
VERBOSE: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 6096719469361499298
VERBOSE: Tactic: 6096719469361499298 Time: 0.020288
VERBOSE: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_interior_nt_v1 Tactic: 7469355608320515097
VERBOSE: Tactic: 7469355608320515097 Time: 0.023904
VERBOSE: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: 7785217228143857868
VERBOSE: Tactic: 7785217228143857868 Time: 0.023936
VERBOSE: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: 8315790488934712458
VERBOSE: Tactic: 8315790488934712458 Time: 0.018944
VERBOSE: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_interior_nt_v1 Tactic: 9221575372280690678
VERBOSE: Tactic: 9221575372280690678 Time: 0.020672
VERBOSE: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -8462194455331556195
VERBOSE: Tactic: -8462194455331556195 Time: 0.018528
VERBOSE: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -7638944668269666085
VERBOSE: Tactic: -7638944668269666085 Time: 0.024
VERBOSE: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: -7185527339793611699
VERBOSE: Tactic: -7185527339793611699 Time: 0.019776
VERBOSE: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -5979101256828290173
VERBOSE: Tactic: -5979101256828290173 Time: 0.021664
VERBOSE: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -5250790590226149674
VERBOSE: Tactic: -5250790590226149674 Time: 0.02384
VERBOSE: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -4831366370915083630
VERBOSE: Tactic: -4831366370915083630 Time: 0.019264
VERBOSE: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -4563432698383308679
VERBOSE: Tactic: -4563432698383308679 Time: 0.02384
VERBOSE: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -3936136542475827126
VERBOSE: Tactic: -3936136542475827126 Time: 0.020384
VERBOSE: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -3784829056659735491
VERBOSE: Tactic: -3784829056659735491 Time: 0.019072
VERBOSE: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -2697681528489059028
VERBOSE: Tactic: -2697681528489059028 Time: 0.020608
VERBOSE: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_interior_nt_v1 Tactic: -2621193268472024213
VERBOSE: Tactic: -2621193268472024213 Time: 0.01936
VERBOSE: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_interior_nt_v1 Tactic: -2120707675073643088
VERBOSE: Tactic: -2120707675073643088 Time: 0.020288
VERBOSE: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: -733152064595858464
VERBOSE: Tactic: -733152064595858464 Time: 0.019936
VERBOSE: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_interior_nt_v1 Tactic: -706197824303187656
VERBOSE: Tactic: -706197824303187656 Time: 0.020096
VERBOSE: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_interior_nt_v1 Tactic: -214244313010793854
VERBOSE: Tactic: -214244313010793854 Time: 0.024064
VERBOSE: Fastest Tactic: -8462194455331556195 Time: 0.018528
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -8462194455331556195
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(589824,2304,48,1) -> Int8(589824,2304,48,1) ***************
VERBOSE: *************** Autotuning format combination: Float(589824,2304,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: *************** Autotuning format combination: Float(589824,2304,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(589824,2304,48,1) -> Float(589824,2304,48,1) ***************
VERBOSE: *************** Autotuning format combination: Int8(147456,2304:4,48,1) -> Float(589824,2304,48,1) ***************
VERBOSE: *************** Autotuning format combination: Int8(18432,2304:32,48,1) -> Float(589824,2304,48,1) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(147456,2304:4,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: *************** Autotuning format combination: Int8(147456,2304:4,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: *************** Autotuning format combination: Int8(18432,2304:32,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(147456,2304:4,48,1), Float(589824,2304,48,1) -> Float(589824,2304,48,1) ***************
VERBOSE: *************** Autotuning format combination: Int8(18432,2304:32,48,1), Float(589824,2304,48,1) -> Float(589824,2304,48,1) ***************
VERBOSE: *************** Autotuning format combination: Int8(18432,2304:32,48,1), Float(18432,2304:32,48,1) -> Float(18432,2304:32,48,1) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(589824,2304,48,1) -> Int8(589824,2304,48,1) ***************
VERBOSE: *************** Autotuning format combination: Float(589824,2304,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: *************** Autotuning format combination: Float(589824,2304,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(147456,2304:4,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: *************** Autotuning format combination: Int8(147456,2304:4,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: *************** Autotuning format combination: Int8(18432,2304:32,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(589824,2304,48,1) -> Int8(2064384,2304,48,1) ***************
VERBOSE: --------------- Timing Runner: quant_input.66TensorQ_clone_3 (Scale)
VERBOSE: Tactic: 0 Time: 0.008544
VERBOSE: Fastest Tactic: 0 Time: 0.008544
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: *************** Autotuning format combination: Float(589824,2304,48,1) -> Int8(516096,2304:4,48,1) ***************
VERBOSE: --------------- Timing Runner: quant_input.66TensorQ_clone_3 (Scale)
VERBOSE: Tactic: 0 Time: 0.011072
VERBOSE: Fastest Tactic: 0 Time: 0.011072
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: *************** Autotuning format combination: Float(589824,2304,48,1) -> Int8(64512,2304:32,48,1) ***************
VERBOSE: --------------- Timing Runner: quant_input.66TensorQ_clone_3 (Scale)
VERBOSE: Tactic: 0 Time: 0.010912
VERBOSE: Fastest Tactic: 0 Time: 0.010912
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(294912,9216:4,96,1) -> Int8(516096,2304:4,48,1) ***************
VERBOSE: --------------- Timing Runner: inputs.15 (TiledPooling)
VERBOSE: Tactic: 5505281 Time: 0.011936
VERBOSE: Tactic: 5570817 Time: 0.009376
VERBOSE: Tactic: 5636353 Time: 0.009504
VERBOSE: Tactic: 5701889 Time: 0.009344
VERBOSE: Tactic: 5767425 Time: 0.009024
VERBOSE: Tactic: 5832961 Time: 0.009312
VERBOSE: Tactic: 5898497 Time: 0.009824
VERBOSE: Tactic: 5964033 Time: 0.00992
VERBOSE: Tactic: 6029569 Time: 0.010016
VERBOSE: Tactic: 6095105 Time: 0.007904
VERBOSE: Tactic: 6160641 Time: 0.007904
VERBOSE: Tactic: 6226177 Time: 0.007584
VERBOSE: Tactic: 6291713 Time: 0.007904
VERBOSE: Tactic: 6357249 Time: 0.008544
VERBOSE: Tactic: 6422785 Time: 0.008512
VERBOSE: Tactic: 6488321 Time: 0.007584
VERBOSE: Fastest Tactic: 6226177 Time: 0.007584
VERBOSE: --------------- Timing Runner: inputs.15 (CudaPooling)
VERBOSE: Tactic: -3 Time: 0.006944
VERBOSE: Fastest Tactic: -3 Time: 0.006944
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CudaPooling Tactic: -3
VERBOSE: *************** Autotuning format combination: Int8(36864,9216:32,96,1) -> Int8(64512,2304:32,48,1) ***************
VERBOSE: --------------- Timing Runner: inputs.15 (TiledPooling)
VERBOSE: TiledPooling has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: inputs.15 (CudaPooling)
VERBOSE: Tactic: -4 Time: 0.006944
VERBOSE: Fastest Tactic: -4 Time: 0.006944
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CudaPooling Tactic: -4
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(589824,2304,48,1) -> Int8(2064384,2304,48,1) ***************
VERBOSE: *************** Autotuning format combination: Float(589824,2304,48,1) -> Int8(516096,2304:4,48,1) ***************
VERBOSE: *************** Autotuning format combination: Float(589824,2304,48,1) -> Int8(64512,2304:32,48,1) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(147456,2304:4,48,1), Int8(147456,2304:4,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: *************** Autotuning format combination: Int8(147456,2304:4,48,1), Int8(18432,2304:32,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: *************** Autotuning format combination: Int8(18432,2304:32,48,1), Int8(18432,2304:32,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(516096,2304:4,48,1) -> Float(589824,2304,48,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 (CaskConvolution)
VERBOSE: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 892787096507693407
VERBOSE: Tactic: 892787096507693407 Time: 0.049184
VERBOSE: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 1204440019753223942
VERBOSE: Tactic: 1204440019753223942 Time: 0.071072
VERBOSE: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 1659301557717208403
VERBOSE: Tactic: 1659301557717208403 Time: 0.058912
VERBOSE: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 2057291331119027912
VERBOSE: Tactic: 2057291331119027912 Time: 0.0536
VERBOSE: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 3275977259705528576
VERBOSE: Tactic: 3275977259705528576 Time: 0.05376
VERBOSE: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 5623454780463195174
VERBOSE: Tactic: 5623454780463195174 Time: 0.060544
VERBOSE: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: -9204333525109552344
VERBOSE: Tactic: -9204333525109552344 Time: 0.05296
VERBOSE: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: -7924103240988931433
VERBOSE: Tactic: -7924103240988931433 Time: 0.050176
VERBOSE: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_xregs_interior_nn_v1 Tactic: -7489650117016530013
VERBOSE: Tactic: -7489650117016530013 Time: 0.069472
VERBOSE: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -4973811344878172338
VERBOSE: Tactic: -4973811344878172338 Time: 0.048704
VERBOSE: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: -3908975881807046106
VERBOSE: Tactic: -3908975881807046106 Time: 0.055328
VERBOSE: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_interior_nn_v1 Tactic: -1765942417666394360
VERBOSE: Tactic: -1765942417666394360 Time: 0.048672
VERBOSE: Fastest Tactic: -1765942417666394360 Time: 0.048672
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -1765942417666394360
VERBOSE: *************** Autotuning format combination: Int8(64512,2304:32,48,1) -> Float(589824,2304,48,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 (CaskConvolution)
VERBOSE: CaskConvolution has no valid tactics for this config, skipping
VERBOSE: *************** Autotuning format combination: Int8(64512,2304:32,48,1) -> Float(18432,2304:32,48,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 (CaskConvolution)
VERBOSE: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: 394365917225754726
VERBOSE: Tactic: 394365917225754726 Time: 0.028672
VERBOSE: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 924563784895318224
VERBOSE: Tactic: 924563784895318224 Time: 0.027648
VERBOSE: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_interior_nt_v1 Tactic: 4531028070024747144
VERBOSE: Tactic: 4531028070024747144 Time: 0.028192
VERBOSE: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_interior_nt_v1 Tactic: 4697540470896098800
VERBOSE: Tactic: 4697540470896098800 Time: 0.026304
VERBOSE: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 6096719469361499298
VERBOSE: Tactic: 6096719469361499298 Time: 0.02656
VERBOSE: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_interior_nt_v1 Tactic: 7469355608320515097
VERBOSE: Tactic: 7469355608320515097 Time: 0.03376
VERBOSE: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: 7785217228143857868
VERBOSE: Tactic: 7785217228143857868 Time: 0.033632
VERBOSE: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: 8315790488934712458
VERBOSE: Tactic: 8315790488934712458 Time: 0.027296
VERBOSE: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_interior_nt_v1 Tactic: 9221575372280690678
VERBOSE: Tactic: 9221575372280690678 Time: 0.027648
VERBOSE: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -8462194455331556195
VERBOSE: Tactic: -8462194455331556195 Time: 0.026976
VERBOSE: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -7638944668269666085
VERBOSE: Tactic: -7638944668269666085 Time: 0.033376
VERBOSE: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: -7185527339793611699
VERBOSE: Tactic: -7185527339793611699 Time: 0.027296
VERBOSE: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -5979101256828290173
VERBOSE: Tactic: -5979101256828290173 Time: 0.029088
VERBOSE: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -5250790590226149674
VERBOSE: Tactic: -5250790590226149674 Time: 0.032832
VERBOSE: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -4831366370915083630
VERBOSE: Tactic: -4831366370915083630 Time: 0.02528
VERBOSE: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -4563432698383308679
VERBOSE: Tactic: -4563432698383308679 Time: 0.032832
VERBOSE: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -3936136542475827126
VERBOSE: Tactic: -3936136542475827126 Time: 0.028224
VERBOSE: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -3784829056659735491
VERBOSE: Tactic: -3784829056659735491 Time: 0.025312
VERBOSE: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -2697681528489059028
VERBOSE: Tactic: -2697681528489059028 Time: 0.028032
VERBOSE: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_interior_nt_v1 Tactic: -2621193268472024213
VERBOSE: Tactic: -2621193268472024213 Time: 0.026272
VERBOSE: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_interior_nt_v1 Tactic: -2120707675073643088
VERBOSE: Tactic: -2120707675073643088 Time: 0.026368
VERBOSE: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: -733152064595858464
VERBOSE: Tactic: -733152064595858464 Time: 0.027008
VERBOSE: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_interior_nt_v1 Tactic: -706197824303187656
VERBOSE: Tactic: -706197824303187656 Time: 0.02672
VERBOSE: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_interior_nt_v1 Tactic: -214244313010793854
VERBOSE: Tactic: -214244313010793854 Time: 0.03392
VERBOSE: Fastest Tactic: -4831366370915083630 Time: 0.02528
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -4831366370915083630
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(589824,2304,48,1) -> Float(147456,576,24,1) ***************
VERBOSE: --------------- Timing Runner: inputs.21 (TiledPooling)
VERBOSE: Tactic: 5505281 Time: 0.014912
VERBOSE: Tactic: 5570817 Time: 0.010368
VERBOSE: Tactic: 5636353 Time: 0.009216
VERBOSE: Tactic: 5701889 Time: 0.008576
VERBOSE: Tactic: 5767425 Time: 0.008256
VERBOSE: Tactic: 5832961 Time: 0.008416
VERBOSE: Tactic: 5898497 Time: 0.008064
VERBOSE: Tactic: 5964033 Time: 0.007936
VERBOSE: Tactic: 6029569 Time: 0.014624
VERBOSE: Tactic: 6095105 Time: 0.010048
VERBOSE: Tactic: 6160641 Time: 0.008928
VERBOSE: Tactic: 6226177 Time: 0.008256
VERBOSE: Tactic: 6291713 Time: 0.00784
VERBOSE: Tactic: 6357249 Time: 0.007776
VERBOSE: Tactic: 6422785 Time: 0.007776
VERBOSE: Tactic: 6488321 Time: 0.007744
VERBOSE: Fastest Tactic: 6488321 Time: 0.007744
VERBOSE: --------------- Timing Runner: inputs.21 (CudnnPooling)
VERBOSE: Tactic: -1 Time: 0.007648
VERBOSE: Fastest Tactic: -1 Time: 0.007648
VERBOSE: --------------- Timing Runner: inputs.21 (CaskPooling)
VERBOSE: inputs.21 Set Tactic Name: sm50_xmma_pooling_fw_4d_FP32FP32NCHW_Max Tactic: -5359392427276731246
VERBOSE: Tactic: -5359392427276731246 Time: 0.008192
VERBOSE: Fastest Tactic: -5359392427276731246 Time: 0.008192
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CudnnPooling Tactic: -1
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(589824,2304,48,1) -> Int8(589824,2304,48,1) ***************
VERBOSE: *************** Autotuning format combination: Float(589824,2304,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: *************** Autotuning format combination: Float(589824,2304,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(147456,576,24,1) -> Int8(147456,576,24,1) ***************
VERBOSE: --------------- Timing Runner: quant_input.68TensorQ (Scale)
VERBOSE: Tactic: 0 Time: 0.006144
VERBOSE: Fastest Tactic: 0 Time: 0.006144
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: *************** Autotuning format combination: Float(147456,576,24,1) -> Int8(36864,576:4,24,1) ***************
VERBOSE: --------------- Timing Runner: quant_input.68TensorQ (Scale)
VERBOSE: Tactic: 0 Time: 0.007104
VERBOSE: Fastest Tactic: 0 Time: 0.007104
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: *************** Autotuning format combination: Float(147456,576,24,1) -> Int8(4608,576:32,24,1) ***************
VERBOSE: --------------- Timing Runner: quant_input.68TensorQ (Scale)
VERBOSE: Tactic: 0 Time: 0.006912
VERBOSE: Fastest Tactic: 0 Time: 0.006912
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(147456,2304:4,48,1) -> Int8(73728,576:4,24,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 (FusedConvActConvolution)
VERBOSE: Tactic: 458751 Time: 0.109536
VERBOSE: Fastest Tactic: 458751 Time: 0.109536
VERBOSE: --------------- Timing Runner: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 (CaskConvolution)
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 4438325421691896755
VERBOSE: Tactic: 4438325421691896755 Time: 0.070304
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 4581732244273465060
VERBOSE: Tactic: 4581732244273465060 Time: 0.064704
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 4934335053031119367
VERBOSE: Tactic: 4934335053031119367 Time: 0.110368
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 6797040896965118050
VERBOSE: Tactic: 6797040896965118050 Time: 0.103616
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 8006952294591770973
VERBOSE: Tactic: 8006952294591770973 Time: 0.124864
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -7210942453088153035
VERBOSE: Tactic: -7210942453088153035 Time: 0.101536
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -6282183216199417697
VERBOSE: Tactic: -6282183216199417697 Time: 0.11728
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: -5026383765466876607
VERBOSE: Tactic: -5026383765466876607 Time: 0.104416
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: -5016725782072253841
VERBOSE: Tactic: -5016725782072253841 Time: 0.110304
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: -1370999262391786833
VERBOSE: Tactic: -1370999262391786833 Time: 0.068224
VERBOSE: Fastest Tactic: 4581732244273465060 Time: 0.064704
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 4581732244273465060
VERBOSE: *************** Autotuning format combination: Int8(147456,2304:4,48,1) -> Int8(9216,576:32,24,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 (CaskConvolution)
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: 1213457772632185722
VERBOSE: Tactic: 1213457772632185722 Time: 0.11024
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 1713441381477652893
VERBOSE: Tactic: 1713441381477652893 Time: 0.125024
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_xregs_large_c32_nn_v1 Tactic: 7125598890155666458
VERBOSE: Tactic: 7125598890155666458 Time: 0.06848
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 8047041638267142825
VERBOSE: Tactic: 8047041638267142825 Time: 0.117152
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: -7846982807478255793
VERBOSE: Tactic: -7846982807478255793 Time: 0.064512
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: -6459719113600909000
VERBOSE: Tactic: -6459719113600909000 Time: 0.108832
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: -4573925292554651334
VERBOSE: Tactic: -4573925292554651334 Time: 0.10176
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: -3566249366964946311
VERBOSE: Tactic: -3566249366964946311 Time: 0.070592
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: -2002418013575043687
VERBOSE: Tactic: -2002418013575043687 Time: 0.103552
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_xregs_large_c32_nn_v1 Tactic: -1659631603542281459
VERBOSE: Tactic: -1659631603542281459 Time: 0.104448
VERBOSE: Fastest Tactic: -7846982807478255793 Time: 0.064512
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -7846982807478255793
VERBOSE: *************** Autotuning format combination: Int8(18432,2304:32,48,1) -> Int8(9216,576:32,24,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 (CudaGroupConvolution)
VERBOSE: CudaGroupConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 (FusedConvActConvolution)
VERBOSE: Tactic: 458751 Time: 0.119584
VERBOSE: Fastest Tactic: 458751 Time: 0.119584
VERBOSE: --------------- Timing Runner: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 (CaskConvolution)
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 289888059097454627
VERBOSE: Tactic: 289888059097454627 Time: 0.05824
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 680740992583869928
VERBOSE: Tactic: 680740992583869928 Time: 0.030464
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 1230105269624924765
VERBOSE: Tactic: 1230105269624924765 Time: 0.042848
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 1388866374720163187
VERBOSE: Tactic: 1388866374720163187 Time: 0.04384
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 2133329569091732311
VERBOSE: Tactic: 2133329569091732311 Time: 0.040096
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 2579824863892891529
VERBOSE: Tactic: 2579824863892891529 Time: 0.047776
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: 4099749238296892232
VERBOSE: Tactic: 4099749238296892232 Time: 0.043392
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 4214794893922618058
VERBOSE: Tactic: 4214794893922618058 Time: 0.039328
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 4384868749799132354
VERBOSE: Tactic: 4384868749799132354 Time: 0.047264
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 4871133328510103657
VERBOSE: Tactic: 4871133328510103657 Time: 0.042336
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 4931167631624420067
VERBOSE: Tactic: 4931167631624420067 Time: 0.0568
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 5641967928706599451
VERBOSE: Tactic: 5641967928706599451 Time: 0.05616
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 6394572396369862482
VERBOSE: Tactic: 6394572396369862482 Time: 0.057664
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 7048234086361926570
VERBOSE: Tactic: 7048234086361926570 Time: 0.034464
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -9138158634332993439
VERBOSE: Tactic: -9138158634332993439 Time: 0.058368
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -9108166971364503411
VERBOSE: Tactic: -9108166971364503411 Time: 0.034784
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: -8861822316054763526
VERBOSE: Tactic: -8861822316054763526 Time: 0.058912
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: -8475551154769412306
VERBOSE: Tactic: -8475551154769412306 Time: 0.047936
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -8431788508843860955
VERBOSE: Tactic: -8431788508843860955 Time: 0.03504
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: -6371781333659293809
VERBOSE: Tactic: -6371781333659293809 Time: 0.030272
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -5932545395595141539
VERBOSE: Tactic: -5932545395595141539 Time: 0.043744
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: -5180570335464125033
VERBOSE: Tactic: -5180570335464125033 Time: 0.04672
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: -2965659229411050116
VERBOSE: Tactic: -2965659229411050116 Time: 0.045152
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -2499089240293650188
VERBOSE: Tactic: -2499089240293650188 Time: 0.030944
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -2328318099174473157
VERBOSE: Tactic: -2328318099174473157 Time: 0.0304
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -1841683966837205309
VERBOSE: Tactic: -1841683966837205309 Time: 0.058656
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -1405083410359886949
VERBOSE: Tactic: -1405083410359886949 Time: 0.045088
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -1044618751912149244
VERBOSE: Tactic: -1044618751912149244 Time: 0.038528
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -496455309852654971
VERBOSE: Tactic: -496455309852654971 Time: 0.036096
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -409197672346571968
VERBOSE: Tactic: -409197672346571968 Time: 0.058752
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: -366411318217594794
VERBOSE: Tactic: -366411318217594794 Time: 0.044608
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: -351548418071036983
VERBOSE: Tactic: -351548418071036983 Time: 0.05744
VERBOSE: Fastest Tactic: -6371781333659293809 Time: 0.030272
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -6371781333659293809
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(147456,2304:4,48,1) -> Float(294912,2304,48,1) ***************
VERBOSE: --------------- Timing Runner: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 (CaskConvolution)
VERBOSE: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 892787096507693407
VERBOSE: Tactic: 892787096507693407 Time: 0.021312
VERBOSE: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 1204440019753223942
VERBOSE: Tactic: 1204440019753223942 Time: 0.021504
VERBOSE: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 1659301557717208403
VERBOSE: Tactic: 1659301557717208403 Time: 0.022144
VERBOSE: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 2057291331119027912
VERBOSE: Tactic: 2057291331119027912 Time: 0.016544
VERBOSE: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 3275977259705528576
VERBOSE: Tactic: 3275977259705528576 Time: 0.02128
VERBOSE: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 5623454780463195174
VERBOSE: Tactic: 5623454780463195174 Time: 0.022464
VERBOSE: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: -9204333525109552344
VERBOSE: Tactic: -9204333525109552344 Time: 0.01616
VERBOSE: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: -7924103240988931433
VERBOSE: Tactic: -7924103240988931433 Time: 0.015744
VERBOSE: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_xregs_interior_nn_v1 Tactic: -7489650117016530013
VERBOSE: Tactic: -7489650117016530013 Time: 0.021216
VERBOSE: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -4973811344878172338
VERBOSE: Tactic: -4973811344878172338 Time: 0.02112
VERBOSE: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: -3908975881807046106
VERBOSE: Tactic: -3908975881807046106 Time: 0.020864
VERBOSE: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_interior_nn_v1 Tactic: -1765942417666394360
VERBOSE: Tactic: -1765942417666394360 Time: 0.021088
VERBOSE: Fastest Tactic: -7924103240988931433 Time: 0.015744
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -7924103240988931433
VERBOSE: *************** Autotuning format combination: Int8(18432,2304:32,48,1) -> Float(294912,2304,48,1) ***************
VERBOSE: --------------- Timing Runner: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 (CaskConvolution)
VERBOSE: CaskConvolution has no valid tactics for this config, skipping
VERBOSE: *************** Autotuning format combination: Int8(18432,2304:32,48,1) -> Float(9216,2304:32,48,1) ***************
VERBOSE: --------------- Timing Runner: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 (CaskConvolution)
VERBOSE: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: 394365917225754726
VERBOSE: Tactic: 394365917225754726 Time: 0.011648
VERBOSE: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 924563784895318224
VERBOSE: Tactic: 924563784895318224 Time: 0.01152
VERBOSE: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_interior_nt_v1 Tactic: 4531028070024747144
VERBOSE: Tactic: 4531028070024747144 Time: 0.011744
VERBOSE: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_interior_nt_v1 Tactic: 4697540470896098800
VERBOSE: Tactic: 4697540470896098800 Time: 0.011616
VERBOSE: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 6096719469361499298
VERBOSE: Tactic: 6096719469361499298 Time: 0.01152
VERBOSE: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_interior_nt_v1 Tactic: 7469355608320515097
VERBOSE: Tactic: 7469355608320515097 Time: 0.01808
VERBOSE: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: 7785217228143857868
VERBOSE: Tactic: 7785217228143857868 Time: 0.01808
VERBOSE: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: 8315790488934712458
VERBOSE: Tactic: 8315790488934712458 Time: 0.011392
VERBOSE: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_interior_nt_v1 Tactic: 9221575372280690678
VERBOSE: Tactic: 9221575372280690678 Time: 0.011712
VERBOSE: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -8462194455331556195
VERBOSE: Tactic: -8462194455331556195 Time: 0.01136
VERBOSE: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -7638944668269666085
VERBOSE: Tactic: -7638944668269666085 Time: 0.018208
VERBOSE: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: -7185527339793611699
VERBOSE: Tactic: -7185527339793611699 Time: 0.011488
VERBOSE: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -5979101256828290173
VERBOSE: Tactic: -5979101256828290173 Time: 0.012128
VERBOSE: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -5250790590226149674
VERBOSE: Tactic: -5250790590226149674 Time: 0.01808
VERBOSE: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -4831366370915083630
VERBOSE: Tactic: -4831366370915083630 Time: 0.011168
VERBOSE: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -4563432698383308679
VERBOSE: Tactic: -4563432698383308679 Time: 0.01808
VERBOSE: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -3936136542475827126
VERBOSE: Tactic: -3936136542475827126 Time: 0.01168
VERBOSE: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -3784829056659735491
VERBOSE: Tactic: -3784829056659735491 Time: 0.011168
VERBOSE: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -2697681528489059028
VERBOSE: Tactic: -2697681528489059028 Time: 0.011808
VERBOSE: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_interior_nt_v1 Tactic: -2621193268472024213
VERBOSE: Tactic: -2621193268472024213 Time: 0.011712
VERBOSE: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_interior_nt_v1 Tactic: -2120707675073643088
VERBOSE: Tactic: -2120707675073643088 Time: 0.011584
VERBOSE: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: -733152064595858464
VERBOSE: Tactic: -733152064595858464 Time: 0.011488
VERBOSE: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_interior_nt_v1 Tactic: -706197824303187656
VERBOSE: Tactic: -706197824303187656 Time: 0.011584
VERBOSE: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_interior_nt_v1 Tactic: -214244313010793854
VERBOSE: Tactic: -214244313010793854 Time: 0.01824
VERBOSE: Fastest Tactic: -4831366370915083630 Time: 0.011168
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -4831366370915083630
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(36864,576:4,24,1) -> Int8(73728,576:4,24,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 (FusedConvActConvolution)
VERBOSE: Tactic: 589823 Time: 0.028
VERBOSE: Tactic: 655359 Time: 0.021056
VERBOSE: Tactic: 786431 Time: 0.026368
VERBOSE: Tactic: 851967 Time: 0.02128
VERBOSE: Tactic: 1179647 Time: 0.021472
VERBOSE: Tactic: 1638399 Time: 0.029152
VERBOSE: Tactic: 1835007 Time: 0.027872
VERBOSE: Tactic: 2097151 Time: 0.029376
VERBOSE: Tactic: 3080191 Time: 0.019744
VERBOSE: Tactic: 3538943 Time: 0.019712
VERBOSE: Tactic: 3997695 Time: 0.024672
VERBOSE: Tactic: 4063231 Time: 0.02384
VERBOSE: Tactic: 4259839 Time: 0.025504
VERBOSE: Tactic: 4325375 Time: 0.026592
VERBOSE: Tactic: 4587519 Time: 0.02464
VERBOSE: Tactic: 4653055 Time: 0.021856
VERBOSE: Tactic: 4915199 Time: 0.049792
VERBOSE: Tactic: 5177343 Time: 0.042816
VERBOSE: Tactic: 5373951 Time: 0.045696
VERBOSE: Tactic: 5636095 Time: 0.046752
VERBOSE: Tactic: 5898239 Time: 0.04368
VERBOSE: Tactic: 6225919 Time: 0.039104
VERBOSE: Tactic: 6291455 Time: 0.041088
VERBOSE: Tactic: 6422527 Time: 0.036544
VERBOSE: Tactic: 6750207 Time: 0.05632
VERBOSE: Tactic: 7012351 Time: 0.058176
VERBOSE: Tactic: 7077887 Time: 0.034304
VERBOSE: Tactic: 7340031 Time: 0.044512
VERBOSE: Tactic: 7405567 Time: 0.047296
VERBOSE: Tactic: 7602175 Time: 0.046112
VERBOSE: Tactic: 7733247 Time: 0.036832
VERBOSE: Tactic: 8191999 Time: 0.05376
VERBOSE: Tactic: 8257535 Time: 0.050656
VERBOSE: Tactic: 8323071 Time: 0.05408
VERBOSE: Tactic: 8650751 Time: 0.049536
VERBOSE: Tactic: 9109503 Time: 0.05904
VERBOSE: Tactic: 9568255 Time: 0.049568
VERBOSE: Tactic: 10354687 Time: 0.049248
VERBOSE: Tactic: 10747903 Time: 0.038528
VERBOSE: Tactic: 10944511 Time: 0.05264
VERBOSE: Fastest Tactic: 3538943 Time: 0.019712
VERBOSE: --------------- Timing Runner: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 (CaskConvolution)
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 3145259992339075399
VERBOSE: Tactic: 3145259992339075399 Time: 0.025344
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_interior_nn_v1 Tactic: 4000990898022781625
VERBOSE: Tactic: 4000990898022781625 Time: 0.035808
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 4438325421691896755
VERBOSE: Tactic: 4438325421691896755 Time: 0.027008
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 4581732244273465060
VERBOSE: Tactic: 4581732244273465060 Time: 0.026144
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 4934335053031119367
VERBOSE: Tactic: 4934335053031119367 Time: 0.038048
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 6797040896965118050
VERBOSE: Tactic: 6797040896965118050 Time: 0.03632
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 8006952294591770973
VERBOSE: Tactic: 8006952294591770973 Time: 0.03968
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 8097855305881829878
VERBOSE: Tactic: 8097855305881829878 Time: 0.036128
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -7210942453088153035
VERBOSE: Tactic: -7210942453088153035 Time: 0.036
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -6282183216199417697
VERBOSE: Tactic: -6282183216199417697 Time: 0.038528
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: -5016725782072253841
VERBOSE: Tactic: -5016725782072253841 Time: 0.037408
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_interior_nn_v1 Tactic: -1543391652455542154
VERBOSE: Tactic: -1543391652455542154 Time: 0.037472
VERBOSE: Fastest Tactic: 3145259992339075399 Time: 0.025344
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: FusedConvActConvolution Tactic: 3538943
VERBOSE: *************** Autotuning format combination: Int8(36864,576:4,24,1) -> Int8(9216,576:32,24,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 (CaskConvolution)
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_interior_c32_nn_v1 Tactic: 1025026069226666066
VERBOSE: Tactic: 1025026069226666066 Time: 0.035808
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: 1213457772632185722
VERBOSE: Tactic: 1213457772632185722 Time: 0.03744
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 1713441381477652893
VERBOSE: Tactic: 1713441381477652893 Time: 0.039392
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_interior_c32_nn_v1 Tactic: 2339361327868109050
VERBOSE: Tactic: 2339361327868109050 Time: 0.025376
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 8047041638267142825
VERBOSE: Tactic: 8047041638267142825 Time: 0.038208
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: -7846982807478255793
VERBOSE: Tactic: -7846982807478255793 Time: 0.0264
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_interior_c32_nn_v1 Tactic: -7686150779628967382
VERBOSE: Tactic: -7686150779628967382 Time: 0.03696
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: -6459719113600909000
VERBOSE: Tactic: -6459719113600909000 Time: 0.036864
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: -4573925292554651334
VERBOSE: Tactic: -4573925292554651334 Time: 0.035904
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_interior_c32_nn_v1 Tactic: -4208188808979933945
VERBOSE: Tactic: -4208188808979933945 Time: 0.035808
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: -3566249366964946311
VERBOSE: Tactic: -3566249366964946311 Time: 0.026976
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: -2002418013575043687
VERBOSE: Tactic: -2002418013575043687 Time: 0.036288
VERBOSE: Fastest Tactic: 2339361327868109050 Time: 0.025376
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 2339361327868109050
VERBOSE: *************** Autotuning format combination: Int8(4608,576:32,24,1) -> Int8(9216,576:32,24,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 (CudaGroupConvolution)
VERBOSE: CudaGroupConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 (FusedConvActConvolution)
VERBOSE: Tactic: 589823 Time: 0.054112
VERBOSE: Tactic: 655359 Time: 0.03712
VERBOSE: Tactic: 786431 Time: 0.05888
VERBOSE: Tactic: 851967 Time: 0.038016
VERBOSE: Tactic: 1179647 Time: 0.044032
VERBOSE: Tactic: 1638399 Time: 0.06336
VERBOSE: Tactic: 1835007 Time: 0.069344
VERBOSE: Tactic: 2097151 Time: 0.063584
VERBOSE: Tactic: 3080191 Time: 0.034368
VERBOSE: Tactic: 3538943 Time: 0.04336
VERBOSE: Tactic: 3997695 Time: 0.05856
VERBOSE: Tactic: 4063231 Time: 0.043392
VERBOSE: Tactic: 4259839 Time: 0.05408
VERBOSE: Tactic: 4325375 Time: 0.058304
VERBOSE: Tactic: 4587519 Time: 0.057088
VERBOSE: Tactic: 4653055 Time: 0.043904
VERBOSE: Tactic: 4915199 Time: 0.056288
VERBOSE: Tactic: 5177343 Time: 0.043616
VERBOSE: Tactic: 5373951 Time: 0.0504
VERBOSE: Tactic: 5636095 Time: 0.044192
VERBOSE: Tactic: 5898239 Time: 0.044736
VERBOSE: Tactic: 6225919 Time: 0.047584
VERBOSE: Tactic: 6291455 Time: 0.044192
VERBOSE: Tactic: 6422527 Time: 0.03488
VERBOSE: Tactic: 6750207 Time: 0.072704
VERBOSE: Tactic: 7012351 Time: 0.063968
VERBOSE: Tactic: 7077887 Time: 0.038496
VERBOSE: Tactic: 7340031 Time: 0.04384
VERBOSE: Tactic: 7405567 Time: 0.048704
VERBOSE: Tactic: 7602175 Time: 0.058304
VERBOSE: Tactic: 7733247 Time: 0.037376
VERBOSE: Tactic: 8191999 Time: 0.08096
VERBOSE: Tactic: 8257535 Time: 0.052896
VERBOSE: Tactic: 8323071 Time: 0.07184
VERBOSE: Tactic: 8650751 Time: 0.058016
VERBOSE: Tactic: 9109503 Time: 0.0608
VERBOSE: Tactic: 9568255 Time: 0.056288
VERBOSE: Tactic: 10354687 Time: 0.054336
VERBOSE: Tactic: 10747903 Time: 0.039936
VERBOSE: Tactic: 10944511 Time: 0.061696
VERBOSE: Fastest Tactic: 3080191 Time: 0.034368
VERBOSE: --------------- Timing Runner: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 (CaskConvolution)
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 68468667201176803
VERBOSE: Tactic: 68468667201176803 Time: 0.022784
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 1230105269624924765
VERBOSE: Tactic: 1230105269624924765 Time: 0.023072
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 2548946449357458230
VERBOSE: Tactic: 2548946449357458230 Time: 0.018752
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 3242897809704328258
VERBOSE: Tactic: 3242897809704328258 Time: 0.018144
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 3312456766204252694
VERBOSE: Tactic: 3312456766204252694 Time: 0.019648
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 3541919052468401776
VERBOSE: Tactic: 3541919052468401776 Time: 0.0216
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 3927509214678622419
VERBOSE: Tactic: 3927509214678622419 Time: 0.017344
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 4871133328510103657
VERBOSE: Tactic: 4871133328510103657 Time: 0.023264
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 8684013308930763400
VERBOSE: Tactic: 8684013308930763400 Time: 0.022016
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -9138158634332993439
VERBOSE: Tactic: -9138158634332993439 Time: 0.02976
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: -8943710627305202139
VERBOSE: Tactic: -8943710627305202139 Time: 0.017888
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: -8859846367886814331
VERBOSE: Tactic: -8859846367886814331 Time: 0.016704
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -8431788508843860955
VERBOSE: Tactic: -8431788508843860955 Time: 0.021248
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: -8382298409581540699
VERBOSE: Tactic: -8382298409581540699 Time: 0.021536
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r1s1 Tactic: -8172318747337038866
VERBOSE: Tactic: -8172318747337038866 Time: 0.028896
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: -7361755530333096258
VERBOSE: Tactic: -7361755530333096258 Time: 0.02016
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: -7106539943789766885
VERBOSE: Tactic: -7106539943789766885 Time: 0.02864
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: -6969478418607271266
VERBOSE: Tactic: -6969478418607271266 Time: 0.027776
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: -6346247605026339453
VERBOSE: Tactic: -6346247605026339453 Time: 0.017344
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -5932545395595141539
VERBOSE: Tactic: -5932545395595141539 Time: 0.023264
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: -5697614955743334137
VERBOSE: Tactic: -5697614955743334137 Time: 0.021824
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_interior_nt_v1 Tactic: -5428851623690564527
VERBOSE: Tactic: -5428851623690564527 Time: 0.02368
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: -5311474420963248369
VERBOSE: Tactic: -5311474420963248369 Time: 0.019968
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: -4260476497340370474
VERBOSE: Tactic: -4260476497340370474 Time: 0.022464
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r1s1 Tactic: -3531681826488401618
VERBOSE: Tactic: -3531681826488401618 Time: 0.028928
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_interior_nt_v1 Tactic: -3065443564418447033
VERBOSE: Tactic: -3065443564418447033 Time: 0.021952
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -1841683966837205309
VERBOSE: Tactic: -1841683966837205309 Time: 0.029792
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: -1494157908358500249
VERBOSE: Tactic: -1494157908358500249 Time: 0.017312
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -1405083410359886949
VERBOSE: Tactic: -1405083410359886949 Time: 0.02384
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_interior_nt_v1 Tactic: -870280893819213650
VERBOSE: Tactic: -870280893819213650 Time: 0.029664
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_interior_nt_v1 Tactic: -545024964146453661
VERBOSE: Tactic: -545024964146453661 Time: 0.023104
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -496455309852654971
VERBOSE: Tactic: -496455309852654971 Time: 0.021856
VERBOSE: Fastest Tactic: -8859846367886814331 Time: 0.016704
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -8859846367886814331
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(294912,2304,48,1) -> Float(1179648,9216,96,1) ***************
VERBOSE: --------------- Timing Runner: dla_up_ida_1_up_1.1 (CudnnDeconvolution)
VERBOSE: Tactic: 0 Time: 0.253696
VERBOSE: Tactic: 1 Time: 0.253792
VERBOSE: Tactic: 3 Time: 41.4232
VERBOSE: Tactic: 24 Time: 1.67382
VERBOSE: Tactic: 25 Time: 6.90621
VERBOSE: Tactic: 27 Time: 23.9992
VERBOSE: Fastest Tactic: 0 Time: 0.253696
VERBOSE: --------------- Timing Runner: dla_up_ida_1_up_1.1 (GemmDeconvolution)
VERBOSE: Tactic: 0 Time: 1.30218
VERBOSE: Fastest Tactic: 0 Time: 1.30218
VERBOSE: --------------- Timing Runner: dla_up_ida_1_up_1.1 (CaskDeconvolution)
VERBOSE: CaskDeconvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: dla_up_ida_1_up_1.1 (CaskDeconvolutionV2)
VERBOSE: dla_up_ida_1_up_1.1 Set Tactic Name: sm50_xmma_deconv_generic_f32f32_f32_f32_nchwkcrs_nchw Tactic: 1108745107350470483
VERBOSE: Tactic: 1108745107350470483 Time: 10000
VERBOSE: Fastest Tactic: 1108745107350470483 Time: 10000
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CudnnDeconvolution Tactic: 0
VERBOSE: *************** Autotuning format combination: Float(294912,1,6144,128) -> Float(1179648,1,12288,128) ***************
VERBOSE: --------------- Timing Runner: dla_up_ida_1_up_1.1 (CudnnDeconvolution)
VERBOSE: CudnnDeconvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: dla_up_ida_1_up_1.1 (GemmDeconvolution)
VERBOSE: GemmDeconvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: dla_up_ida_1_up_1.1 (CaskDeconvolution)
VERBOSE: CaskDeconvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: dla_up_ida_1_up_1.1 (CaskDeconvolutionV2)
VERBOSE: dla_up_ida_1_up_1.1 Set Tactic Name: sm70_xmma_deconv_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage1_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: -6927323623127951136
VERBOSE: Tactic: -6927323623127951136 Time: 4.54538
VERBOSE: dla_up_ida_1_up_1.1 Set Tactic Name: sm70_xmma_deconv_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x128x8_stage1_warpsize2x4x1_g1_ffma_aligna4_alignc4 Tactic: -5811489873080635378
VERBOSE: Tactic: -5811489873080635378 Time: 10.2654
VERBOSE: dla_up_ida_1_up_1.1 Set Tactic Name: sm70_xmma_deconv_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x64x8_stage1_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: -500373307302467644
VERBOSE: Tactic: -500373307302467644 Time: 7.224
VERBOSE: Fastest Tactic: -6927323623127951136 Time: 4.54538
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskDeconvolutionV2 Tactic: -6927323623127951136
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(294912,576,24,1) -> Float(294912,576,24,1) ***************
VERBOSE: --------------- Timing Runner: base_level5_project_1_output_quant.1 (Scale)
VERBOSE: Tactic: 0 Time: 0.008544
VERBOSE: Fastest Tactic: 0 Time: 0.008544
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: *************** Autotuning format combination: Int8(73728,576:4,24,1) -> Float(294912,576,24,1) ***************
VERBOSE: --------------- Timing Runner: base_level5_project_1_output_quant.1 (Scale)
VERBOSE: Tactic: 0 Time: 0.010624
VERBOSE: Fastest Tactic: 0 Time: 0.010624
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: *************** Autotuning format combination: Int8(9216,576:32,24,1) -> Float(294912,576,24,1) ***************
VERBOSE: --------------- Timing Runner: base_level5_project_1_output_quant.1 (Scale)
VERBOSE: Tactic: 0 Time: 0.009792
VERBOSE: Fastest Tactic: 0 Time: 0.009792
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(1179648,9216,96,1) -> Int8(2359296,9216,96,1) ***************
VERBOSE: *************** Autotuning format combination: Float(1179648,9216,96,1) -> Int8(589824,9216:4,96,1) ***************
VERBOSE: *************** Autotuning format combination: Float(1179648,9216,96,1) -> Int8(73728,9216:32,96,1) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(1179648,9216,96,1) -> Int8(2359296,9216,96,1) ***************
VERBOSE: *************** Autotuning format combination: Float(1179648,9216,96,1) -> Int8(589824,9216:4,96,1) ***************
VERBOSE: *************** Autotuning format combination: Float(1179648,9216,96,1) -> Int8(73728,9216:32,96,1) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(73728,576:4,24,1), Float(294912,576,24,1) -> Float(294912,576,24,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 (CaskConvolution)
VERBOSE: self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 892787096507693407
VERBOSE: Tactic: 892787096507693407 Time: 0.208832
VERBOSE: self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 1204440019753223942
VERBOSE: Tactic: 1204440019753223942 Time: 0.216896
VERBOSE: self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 1659301557717208403
VERBOSE: Tactic: 1659301557717208403 Time: 0.24112
VERBOSE: self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 2057291331119027912
VERBOSE: Tactic: 2057291331119027912 Time: 0.145312
VERBOSE: self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 3275977259705528576
VERBOSE: Tactic: 3275977259705528576 Time: 0.211904
VERBOSE: self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 5623454780463195174
VERBOSE: Tactic: 5623454780463195174 Time: 0.253056
VERBOSE: self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: 8930254200803946944
VERBOSE: Tactic: 8930254200803946944 Time: 0.141664
VERBOSE: self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: -9204333525109552344
VERBOSE: Tactic: -9204333525109552344 Time: 0.130592
VERBOSE: self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -4973811344878172338
VERBOSE: Tactic: -4973811344878172338 Time: 0.202912
VERBOSE: self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: -1228371230285617088
VERBOSE: Tactic: -1228371230285617088 Time: 0.210272
VERBOSE: Fastest Tactic: -9204333525109552344 Time: 0.130592
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -9204333525109552344
VERBOSE: *************** Autotuning format combination: Int8(9216,576:32,24,1), Float(294912,576,24,1) -> Float(294912,576,24,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 (CaskConvolution)
VERBOSE: CaskConvolution has no valid tactics for this config, skipping
VERBOSE: *************** Autotuning format combination: Int8(9216,576:32,24,1), Float(9216,576:32,24,1) -> Float(9216,576:32,24,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 (CaskConvolution)
VERBOSE: self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: 394365917225754726
VERBOSE: Tactic: 394365917225754726 Time: 0.083872
VERBOSE: self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 924563784895318224
VERBOSE: Tactic: 924563784895318224 Time: 0.076896
VERBOSE: self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: 3451841782205411203
VERBOSE: Tactic: 3451841782205411203 Time: 0.078272
VERBOSE: self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 6096719469361499298
VERBOSE: Tactic: 6096719469361499298 Time: 0.073344
VERBOSE: self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: 7785217228143857868
VERBOSE: Tactic: 7785217228143857868 Time: 0.115392
VERBOSE: self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: 8315790488934712458
VERBOSE: Tactic: 8315790488934712458 Time: 0.080352
VERBOSE: self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: 9080823067063042887
VERBOSE: Tactic: 9080823067063042887 Time: 0.092768
VERBOSE: self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -8462194455331556195
VERBOSE: Tactic: -8462194455331556195 Time: 0.079008
VERBOSE: self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: -8165074686865110847
VERBOSE: Tactic: -8165074686865110847 Time: 0.079328
VERBOSE: self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -7638944668269666085
VERBOSE: Tactic: -7638944668269666085 Time: 0.115744
VERBOSE: self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: -7185527339793611699
VERBOSE: Tactic: -7185527339793611699 Time: 0.078144
VERBOSE: self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -5979101256828290173
VERBOSE: Tactic: -5979101256828290173 Time: 0.090016
VERBOSE: self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -5763174003249488100
VERBOSE: Tactic: -5763174003249488100 Time: 0.090368
VERBOSE: self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: -5612459945002849429
VERBOSE: Tactic: -5612459945002849429 Time: 0.091552
VERBOSE: self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -5250790590226149674
VERBOSE: Tactic: -5250790590226149674 Time: 0.113504
VERBOSE: self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -4911193113143178408
VERBOSE: Tactic: -4911193113143178408 Time: 0.117568
VERBOSE: self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -4831366370915083630
VERBOSE: Tactic: -4831366370915083630 Time: 0.073664
VERBOSE: self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -4563432698383308679
VERBOSE: Tactic: -4563432698383308679 Time: 0.114784
VERBOSE: self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -4067022988143035981
VERBOSE: Tactic: -4067022988143035981 Time: 0.08864
VERBOSE: self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -3936136542475827126
VERBOSE: Tactic: -3936136542475827126 Time: 0.08592
VERBOSE: self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -3784829056659735491
VERBOSE: Tactic: -3784829056659735491 Time: 0.073984
VERBOSE: self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -3720553804749394441
VERBOSE: Tactic: -3720553804749394441 Time: 0.116
VERBOSE: self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -2697681528489059028
VERBOSE: Tactic: -2697681528489059028 Time: 0.090592
VERBOSE: self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: -733152064595858464
VERBOSE: Tactic: -733152064595858464 Time: 0.074368
VERBOSE: Fastest Tactic: 6096719469361499298 Time: 0.073344
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 6096719469361499298
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(589824,9216:4,96,1) -> Float(1179648,9216,96,1) ***************
VERBOSE: --------------- Timing Runner: self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 (CaskConvolution)
VERBOSE: self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 892787096507693407
VERBOSE: Tactic: 892787096507693407 Time: 0.205152
VERBOSE: self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 1204440019753223942
VERBOSE: Tactic: 1204440019753223942 Time: 0.244672
VERBOSE: self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 1659301557717208403
VERBOSE: Tactic: 1659301557717208403 Time: 0.243392
VERBOSE: self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 2057291331119027912
VERBOSE: Tactic: 2057291331119027912 Time: 0.222848
VERBOSE: self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 3275977259705528576
VERBOSE: Tactic: 3275977259705528576 Time: 0.233728
VERBOSE: self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 5623454780463195174
VERBOSE: Tactic: 5623454780463195174 Time: 0.250336
VERBOSE: self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: 8930254200803946944
VERBOSE: Tactic: 8930254200803946944 Time: 0.215136
VERBOSE: self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: -9204333525109552344
VERBOSE: Tactic: -9204333525109552344 Time: 0.217856
VERBOSE: self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -4973811344878172338
VERBOSE: Tactic: -4973811344878172338 Time: 0.203584
VERBOSE: self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: -1228371230285617088
VERBOSE: Tactic: -1228371230285617088 Time: 0.214048
VERBOSE: Fastest Tactic: -4973811344878172338 Time: 0.203584
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -4973811344878172338
VERBOSE: *************** Autotuning format combination: Int8(73728,9216:32,96,1) -> Float(1179648,9216,96,1) ***************
VERBOSE: --------------- Timing Runner: self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 (CaskConvolution)
VERBOSE: CaskConvolution has no valid tactics for this config, skipping
VERBOSE: *************** Autotuning format combination: Int8(73728,9216:32,96,1) -> Float(36864,9216:32,96,1) ***************
VERBOSE: --------------- Timing Runner: self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 (CaskConvolution)
VERBOSE: self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: 394365917225754726
VERBOSE: Tactic: 394365917225754726 Time: 0.10608
VERBOSE: self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 924563784895318224
VERBOSE: Tactic: 924563784895318224 Time: 0.078496
VERBOSE: self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: 3451841782205411203
VERBOSE: Tactic: 3451841782205411203 Time: 0.080192
VERBOSE: self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 6096719469361499298
VERBOSE: Tactic: 6096719469361499298 Time: 0.077952
VERBOSE: self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: 7785217228143857868
VERBOSE: Tactic: 7785217228143857868 Time: 0.092
VERBOSE: self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: 8315790488934712458
VERBOSE: Tactic: 8315790488934712458 Time: 0.078592
VERBOSE: self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: 9080823067063042887
VERBOSE: Tactic: 9080823067063042887 Time: 0.113952
VERBOSE: self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -8462194455331556195
VERBOSE: Tactic: -8462194455331556195 Time: 0.079328
VERBOSE: self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: -8165074686865110847
VERBOSE: Tactic: -8165074686865110847 Time: 0.078304
VERBOSE: self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -7638944668269666085
VERBOSE: Tactic: -7638944668269666085 Time: 0.092512
VERBOSE: self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: -7185527339793611699
VERBOSE: Tactic: -7185527339793611699 Time: 0.079456
VERBOSE: self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -5979101256828290173
VERBOSE: Tactic: -5979101256828290173 Time: 0.115392
VERBOSE: self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -5763174003249488100
VERBOSE: Tactic: -5763174003249488100 Time: 0.07984
VERBOSE: self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: -5612459945002849429
VERBOSE: Tactic: -5612459945002849429 Time: 0.113664
VERBOSE: self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -5250790590226149674
VERBOSE: Tactic: -5250790590226149674 Time: 0.09216
VERBOSE: self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -4911193113143178408
VERBOSE: Tactic: -4911193113143178408 Time: 0.094208
VERBOSE: self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -4831366370915083630
VERBOSE: Tactic: -4831366370915083630 Time: 0.077952
VERBOSE: self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -4563432698383308679
VERBOSE: Tactic: -4563432698383308679 Time: 0.092384
VERBOSE: self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -4067022988143035981
VERBOSE: Tactic: -4067022988143035981 Time: 0.081184
VERBOSE: self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -3936136542475827126
VERBOSE: Tactic: -3936136542475827126 Time: 0.108864
VERBOSE: self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -3784829056659735491
VERBOSE: Tactic: -3784829056659735491 Time: 0.077536
VERBOSE: self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -3720553804749394441
VERBOSE: Tactic: -3720553804749394441 Time: 0.092864
VERBOSE: self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -2697681528489059028
VERBOSE: Tactic: -2697681528489059028 Time: 0.113248
VERBOSE: self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: -733152064595858464
VERBOSE: Tactic: -733152064595858464 Time: 0.078784
VERBOSE: Fastest Tactic: -3784829056659735491 Time: 0.077536
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -3784829056659735491
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(294912,576,24,1) -> Int8(294912,576,24,1) ***************
VERBOSE: --------------- Timing Runner: base_level5_tree1_relu_output_quant.1TensorQ (Scale)
VERBOSE: Tactic: 0 Time: 0.006496
VERBOSE: Fastest Tactic: 0 Time: 0.006496
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: *************** Autotuning format combination: Float(294912,576,24,1) -> Int8(73728,576:4,24,1) ***************
VERBOSE: --------------- Timing Runner: base_level5_tree1_relu_output_quant.1TensorQ (Scale)
VERBOSE: Tactic: 0 Time: 0.00768
VERBOSE: Fastest Tactic: 0 Time: 0.00768
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: *************** Autotuning format combination: Float(294912,576,24,1) -> Int8(9216,576:32,24,1) ***************
VERBOSE: --------------- Timing Runner: base_level5_tree1_relu_output_quant.1TensorQ (Scale)
VERBOSE: Tactic: 0 Time: 0.007616
VERBOSE: Fastest Tactic: 0 Time: 0.007616
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(1179648,9216,96,1) -> Int8(1179648,9216,96,1) ***************
VERBOSE: *************** Autotuning format combination: Float(1179648,9216,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: *************** Autotuning format combination: Float(1179648,9216,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(73728,576:4,24,1) -> Int8(73728,576:4,24,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 (FusedConvActConvolution)
VERBOSE: Tactic: 524287 Time: 0.16368
VERBOSE: Tactic: 720895 Time: 0.113248
VERBOSE: Tactic: 983039 Time: 0.120448
VERBOSE: Tactic: 1048575 Time: 0.145248
VERBOSE: Tactic: 1703935 Time: 0.108224
VERBOSE: Tactic: 1769471 Time: 0.129792
VERBOSE: Tactic: 2424831 Time: 0.117472
VERBOSE: Tactic: 2621439 Time: 0.109888
VERBOSE: Tactic: 3014655 Time: 0.123936
VERBOSE: Tactic: 3604479 Time: 0.123616
VERBOSE: Tactic: 5046271 Time: 0.133152
VERBOSE: Tactic: 6488063 Time: 0.175776
VERBOSE: Tactic: 7274495 Time: 0.128512
VERBOSE: Tactic: 7864319 Time: 0.131008
VERBOSE: Tactic: 8847359 Time: 0.148416
VERBOSE: Tactic: 9043967 Time: 0.149088
VERBOSE: Tactic: 9961471 Time: 0.135168
VERBOSE: Tactic: 10027007 Time: 0.130752
VERBOSE: Tactic: 10485759 Time: 0.125088
VERBOSE: Tactic: 10682367 Time: 0.109472
VERBOSE: Tactic: 10813439 Time: 0.110976
VERBOSE: Fastest Tactic: 1703935 Time: 0.108224
VERBOSE: --------------- Timing Runner: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 (CaskConvolution)
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 4438325421691896755
VERBOSE: Tactic: 4438325421691896755 Time: 0.137792
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 4581732244273465060
VERBOSE: Tactic: 4581732244273465060 Time: 0.11952
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 4934335053031119367
VERBOSE: Tactic: 4934335053031119367 Time: 0.211776
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 6797040896965118050
VERBOSE: Tactic: 6797040896965118050 Time: 0.201504
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 8006952294591770973
VERBOSE: Tactic: 8006952294591770973 Time: 0.246752
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -7210942453088153035
VERBOSE: Tactic: -7210942453088153035 Time: 0.195264
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -6282183216199417697
VERBOSE: Tactic: -6282183216199417697 Time: 0.22672
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: -5026383765466876607
VERBOSE: Tactic: -5026383765466876607 Time: 0.20272
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: -5016725782072253841
VERBOSE: Tactic: -5016725782072253841 Time: 0.206784
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: -1370999262391786833
VERBOSE: Tactic: -1370999262391786833 Time: 0.129184
VERBOSE: Fastest Tactic: 4581732244273465060 Time: 0.11952
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: FusedConvActConvolution Tactic: 1703935
VERBOSE: *************** Autotuning format combination: Int8(73728,576:4,24,1) -> Int8(9216,576:32,24,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 (CaskConvolution)
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: 1213457772632185722
VERBOSE: Tactic: 1213457772632185722 Time: 0.211424
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 1713441381477652893
VERBOSE: Tactic: 1713441381477652893 Time: 0.245952
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_xregs_large_c32_nn_v1 Tactic: 7125598890155666458
VERBOSE: Tactic: 7125598890155666458 Time: 0.128768
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 8047041638267142825
VERBOSE: Tactic: 8047041638267142825 Time: 0.226464
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: -7846982807478255793
VERBOSE: Tactic: -7846982807478255793 Time: 0.119584
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: -6459719113600909000
VERBOSE: Tactic: -6459719113600909000 Time: 0.205312
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: -4573925292554651334
VERBOSE: Tactic: -4573925292554651334 Time: 0.195424
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: -3566249366964946311
VERBOSE: Tactic: -3566249366964946311 Time: 0.137536
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: -2002418013575043687
VERBOSE: Tactic: -2002418013575043687 Time: 0.201344
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_xregs_large_c32_nn_v1 Tactic: -1659631603542281459
VERBOSE: Tactic: -1659631603542281459 Time: 0.20288
VERBOSE: Fastest Tactic: -7846982807478255793 Time: 0.119584
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -7846982807478255793
VERBOSE: *************** Autotuning format combination: Int8(9216,576:32,24,1) -> Int8(9216,576:32,24,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 (CudaGroupConvolution)
VERBOSE: CudaGroupConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 (FusedConvActConvolution)
VERBOSE: Tactic: 524287 Time: 0.16416
VERBOSE: Tactic: 720895 Time: 0.11248
VERBOSE: Tactic: 983039 Time: 0.120928
VERBOSE: Tactic: 1048575 Time: 0.148544
VERBOSE: Tactic: 1703935 Time: 0.110208
VERBOSE: Tactic: 1769471 Time: 0.127712
VERBOSE: Tactic: 2424831 Time: 0.131072
VERBOSE: Tactic: 2621439 Time: 0.110656
VERBOSE: Tactic: 3014655 Time: 0.125792
VERBOSE: Tactic: 3604479 Time: 0.125792
VERBOSE: Tactic: 5046271 Time: 0.156256
VERBOSE: Tactic: 6488063 Time: 0.178496
VERBOSE: Tactic: 7274495 Time: 0.127264
VERBOSE: Tactic: 7864319 Time: 0.131936
VERBOSE: Tactic: 8847359 Time: 0.149088
VERBOSE: Tactic: 9043967 Time: 0.151456
VERBOSE: Tactic: 9961471 Time: 0.144768
VERBOSE: Tactic: 10027007 Time: 0.133792
VERBOSE: Tactic: 10485759 Time: 0.126048
VERBOSE: Tactic: 10682367 Time: 0.109984
VERBOSE: Tactic: 10813439 Time: 0.112256
VERBOSE: Fastest Tactic: 10682367 Time: 0.109984
VERBOSE: --------------- Timing Runner: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 (CaskConvolution)
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 289888059097454627
VERBOSE: Tactic: 289888059097454627 Time: 0.106592
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 680740992583869928
VERBOSE: Tactic: 680740992583869928 Time: 0.052064
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 1230105269624924765
VERBOSE: Tactic: 1230105269624924765 Time: 0.070304
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 1388866374720163187
VERBOSE: Tactic: 1388866374720163187 Time: 0.070592
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 2133329569091732311
VERBOSE: Tactic: 2133329569091732311 Time: 0.067104
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 2579824863892891529
VERBOSE: Tactic: 2579824863892891529 Time: 0.083328
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: 4099749238296892232
VERBOSE: Tactic: 4099749238296892232 Time: 0.071456
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 4214794893922618058
VERBOSE: Tactic: 4214794893922618058 Time: 0.066656
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 4384868749799132354
VERBOSE: Tactic: 4384868749799132354 Time: 0.082656
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 4871133328510103657
VERBOSE: Tactic: 4871133328510103657 Time: 0.06912
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 4931167631624420067
VERBOSE: Tactic: 4931167631624420067 Time: 0.103968
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 5641967928706599451
VERBOSE: Tactic: 5641967928706599451 Time: 0.094464
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 6394572396369862482
VERBOSE: Tactic: 6394572396369862482 Time: 0.097536
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 7048234086361926570
VERBOSE: Tactic: 7048234086361926570 Time: 0.05728
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -9138158634332993439
VERBOSE: Tactic: -9138158634332993439 Time: 0.106496
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -9108166971364503411
VERBOSE: Tactic: -9108166971364503411 Time: 0.0576
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: -8861822316054763526
VERBOSE: Tactic: -8861822316054763526 Time: 0.107232
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: -8475551154769412306
VERBOSE: Tactic: -8475551154769412306 Time: 0.080992
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -8431788508843860955
VERBOSE: Tactic: -8431788508843860955 Time: 0.06144
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: -6371781333659293809
VERBOSE: Tactic: -6371781333659293809 Time: 0.048992
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -5932545395595141539
VERBOSE: Tactic: -5932545395595141539 Time: 0.0752
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: -5180570335464125033
VERBOSE: Tactic: -5180570335464125033 Time: 0.07712
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: -2965659229411050116
VERBOSE: Tactic: -2965659229411050116 Time: 0.079392
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -2499089240293650188
VERBOSE: Tactic: -2499089240293650188 Time: 0.052608
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -2328318099174473157
VERBOSE: Tactic: -2328318099174473157 Time: 0.04896
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -1841683966837205309
VERBOSE: Tactic: -1841683966837205309 Time: 0.10816
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -1405083410359886949
VERBOSE: Tactic: -1405083410359886949 Time: 0.076832
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -1044618751912149244
VERBOSE: Tactic: -1044618751912149244 Time: 0.068064
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -496455309852654971
VERBOSE: Tactic: -496455309852654971 Time: 0.065184
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -409197672346571968
VERBOSE: Tactic: -409197672346571968 Time: 0.106304
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: -366411318217594794
VERBOSE: Tactic: -366411318217594794 Time: 0.070944
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: -351548418071036983
VERBOSE: Tactic: -351548418071036983 Time: 0.104896
VERBOSE: Fastest Tactic: -2328318099174473157 Time: 0.04896
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -2328318099174473157
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(294912,9216:4,96,1) -> Float(589824,9216,96,1) ***************
VERBOSE: *************** Autotuning format combination: Int8(36864,9216:32,96,1) -> Float(589824,9216,96,1) ***************
VERBOSE: *************** Autotuning format combination: Int8(36864,9216:32,96,1) -> Float(18432,9216:32,96,1) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(589824,9216,96,1) -> Float(2359296,36864,192,1) ***************
VERBOSE: *************** Autotuning format combination: Float(589824,1,6144,64) -> Float(2359296,1,12288,64) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(2359296,36864,192,1) -> Int8(4718592,36864,192,1) ***************
VERBOSE: *************** Autotuning format combination: Float(2359296,36864,192,1) -> Int8(1179648,36864:4,192,1) ***************
VERBOSE: *************** Autotuning format combination: Float(2359296,36864,192,1) -> Int8(147456,36864:32,192,1) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(1179648,36864:4,192,1) -> Int8(1179648,36864:4,192,1) ***************
VERBOSE: *************** Autotuning format combination: Int8(1179648,36864:4,192,1) -> Int8(147456,36864:32,192,1) ***************
VERBOSE: *************** Autotuning format combination: Int8(147456,36864:32,192,1) -> Int8(147456,36864:32,192,1) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(147456,576,24,1) -> Int8(737280,576,24,1) ***************
VERBOSE: --------------- Timing Runner: quant_input.78TensorQ_clone_2 (Scale)
VERBOSE: Tactic: 0 Time: 0.006464
VERBOSE: Fastest Tactic: 0 Time: 0.006464
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: *************** Autotuning format combination: Float(147456,576,24,1) -> Int8(184320,576:4,24,1) ***************
VERBOSE: --------------- Timing Runner: quant_input.78TensorQ_clone_2 (Scale)
VERBOSE: Tactic: 0 Time: 0.007072
VERBOSE: Fastest Tactic: 0 Time: 0.007072
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: *************** Autotuning format combination: Float(147456,576,24,1) -> Int8(23040,576:32,24,1) ***************
VERBOSE: --------------- Timing Runner: quant_input.78TensorQ_clone_2 (Scale)
VERBOSE: Tactic: 0 Time: 0.006944
VERBOSE: Fastest Tactic: 0 Time: 0.006944
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(294912,576,24,1) -> Int8(737280,576,24,1) ***************
VERBOSE: --------------- Timing Runner: quant_input.78TensorQ_clone_1 (Scale)
VERBOSE: Tactic: 0 Time: 0.006848
VERBOSE: Fastest Tactic: 0 Time: 0.006848
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: *************** Autotuning format combination: Float(294912,576,24,1) -> Int8(184320,576:4,24,1) ***************
VERBOSE: --------------- Timing Runner: quant_input.78TensorQ_clone_1 (Scale)
VERBOSE: Tactic: 0 Time: 0.007616
VERBOSE: Fastest Tactic: 0 Time: 0.007616
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: *************** Autotuning format combination: Float(294912,576,24,1) -> Int8(23040,576:32,24,1) ***************
VERBOSE: --------------- Timing Runner: quant_input.78TensorQ_clone_1 (Scale)
VERBOSE: Tactic: 0 Time: 0.007648
VERBOSE: Fastest Tactic: 0 Time: 0.007648
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Scale Tactic: 0
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(73728,576:4,24,1), Int8(73728,576:4,24,1) -> Int8(73728,576:4,24,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 (FusedConvActConvolution)
VERBOSE: FusedConvActConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 (CaskConvolution)
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 4438325421691896755
VERBOSE: Tactic: 4438325421691896755 Time: 0.139328
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 4581732244273465060
VERBOSE: Tactic: 4581732244273465060 Time: 0.121248
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 4934335053031119367
VERBOSE: Tactic: 4934335053031119367 Time: 0.214528
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 6797040896965118050
VERBOSE: Tactic: 6797040896965118050 Time: 0.202976
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 8006952294591770973
VERBOSE: Tactic: 8006952294591770973 Time: 0.248128
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -7210942453088153035
VERBOSE: Tactic: -7210942453088153035 Time: 0.197088
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -6282183216199417697
VERBOSE: Tactic: -6282183216199417697 Time: 0.228064
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: -5026383765466876607
VERBOSE: Tactic: -5026383765466876607 Time: 0.204384
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: -5016725782072253841
VERBOSE: Tactic: -5016725782072253841 Time: 0.207328
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: -1370999262391786833
VERBOSE: Tactic: -1370999262391786833 Time: 0.130976
VERBOSE: Fastest Tactic: 4581732244273465060 Time: 0.121248
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 4581732244273465060
VERBOSE: *************** Autotuning format combination: Int8(73728,576:4,24,1), Int8(9216,576:32,24,1) -> Int8(9216,576:32,24,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 (CaskConvolution)
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: 1213457772632185722
VERBOSE: Tactic: 1213457772632185722 Time: 0.212832
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 1713441381477652893
VERBOSE: Tactic: 1713441381477652893 Time: 0.247456
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_xregs_large_c32_nn_v1 Tactic: 7125598890155666458
VERBOSE: Tactic: 7125598890155666458 Time: 0.130816
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 8047041638267142825
VERBOSE: Tactic: 8047041638267142825 Time: 0.228704
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: -7846982807478255793
VERBOSE: Tactic: -7846982807478255793 Time: 0.121728
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: -6459719113600909000
VERBOSE: Tactic: -6459719113600909000 Time: 0.2072
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: -4573925292554651334
VERBOSE: Tactic: -4573925292554651334 Time: 0.197664
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: -3566249366964946311
VERBOSE: Tactic: -3566249366964946311 Time: 0.139616
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: -2002418013575043687
VERBOSE: Tactic: -2002418013575043687 Time: 0.203584
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_xregs_large_c32_nn_v1 Tactic: -1659631603542281459
VERBOSE: Tactic: -1659631603542281459 Time: 0.20496
VERBOSE: Fastest Tactic: -7846982807478255793 Time: 0.121728
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -7846982807478255793
VERBOSE: *************** Autotuning format combination: Int8(9216,576:32,24,1), Int8(9216,576:32,24,1) -> Int8(9216,576:32,24,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 (CudaGroupConvolution)
VERBOSE: CudaGroupConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 (FusedConvActConvolution)
VERBOSE: FusedConvActConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 (CaskConvolution)
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 289888059097454627
VERBOSE: Tactic: 289888059097454627 Time: 0.107744
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 680740992583869928
VERBOSE: Tactic: 680740992583869928 Time: 0.053056
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 1230105269624924765
VERBOSE: Tactic: 1230105269624924765 Time: 0.07168
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 1388866374720163187
VERBOSE: Tactic: 1388866374720163187 Time: 0.070944
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 2133329569091732311
VERBOSE: Tactic: 2133329569091732311 Time: 0.069312
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 2579824863892891529
VERBOSE: Tactic: 2579824863892891529 Time: 0.083456
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: 4099749238296892232
VERBOSE: Tactic: 4099749238296892232 Time: 0.072448
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 4214794893922618058
VERBOSE: Tactic: 4214794893922618058 Time: 0.068288
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 4384868749799132354
VERBOSE: Tactic: 4384868749799132354 Time: 0.082912
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 4871133328510103657
VERBOSE: Tactic: 4871133328510103657 Time: 0.069856
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 4931167631624420067
VERBOSE: Tactic: 4931167631624420067 Time: 0.105728
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 5641967928706599451
VERBOSE: Tactic: 5641967928706599451 Time: 0.097024
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 6394572396369862482
VERBOSE: Tactic: 6394572396369862482 Time: 0.09824
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 7048234086361926570
VERBOSE: Tactic: 7048234086361926570 Time: 0.057696
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -9138158634332993439
VERBOSE: Tactic: -9138158634332993439 Time: 0.108224
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -9108166971364503411
VERBOSE: Tactic: -9108166971364503411 Time: 0.058208
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: -8861822316054763526
VERBOSE: Tactic: -8861822316054763526 Time: 0.109408
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: -8475551154769412306
VERBOSE: Tactic: -8475551154769412306 Time: 0.082688
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -8431788508843860955
VERBOSE: Tactic: -8431788508843860955 Time: 0.062816
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: -6371781333659293809
VERBOSE: Tactic: -6371781333659293809 Time: 0.049952
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -5932545395595141539
VERBOSE: Tactic: -5932545395595141539 Time: 0.07632
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: -5180570335464125033
VERBOSE: Tactic: -5180570335464125033 Time: 0.077952
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: -2965659229411050116
VERBOSE: Tactic: -2965659229411050116 Time: 0.080864
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -2499089240293650188
VERBOSE: Tactic: -2499089240293650188 Time: 0.053312
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -2328318099174473157
VERBOSE: Tactic: -2328318099174473157 Time: 0.050144
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -1841683966837205309
VERBOSE: Tactic: -1841683966837205309 Time: 0.10992
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -1405083410359886949
VERBOSE: Tactic: -1405083410359886949 Time: 0.077984
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -1044618751912149244
VERBOSE: Tactic: -1044618751912149244 Time: 0.069248
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -496455309852654971
VERBOSE: Tactic: -496455309852654971 Time: 0.066368
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -409197672346571968
VERBOSE: Tactic: -409197672346571968 Time: 0.107968
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: -366411318217594794
VERBOSE: Tactic: -366411318217594794 Time: 0.071392
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: -351548418071036983
VERBOSE: Tactic: -351548418071036983 Time: 0.106816
VERBOSE: Fastest Tactic: -6371781333659293809 Time: 0.049952
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -6371781333659293809
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(184320,576:4,24,1) -> Int8(73728,576:4,24,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 (FusedConvActConvolution)
VERBOSE: Tactic: 589823 Time: 0.09952
VERBOSE: Tactic: 655359 Time: 0.067584
VERBOSE: Tactic: 786431 Time: 0.090976
VERBOSE: Tactic: 851967 Time: 0.050976
VERBOSE: Tactic: 1179647 Time: 0.053088
VERBOSE: Tactic: 1638399 Time: 0.088896
VERBOSE: Tactic: 1835007 Time: 0.097696
VERBOSE: Tactic: 2097151 Time: 0.084256
VERBOSE: Tactic: 3080191 Time: 0.05792
VERBOSE: Tactic: 3538943 Time: 0.055072
VERBOSE: Tactic: 3997695 Time: 0.079264
VERBOSE: Tactic: 4063231 Time: 0.06224
VERBOSE: Tactic: 4259839 Time: 0.073216
VERBOSE: Tactic: 4325375 Time: 0.086368
VERBOSE: Tactic: 4587519 Time: 0.079744
VERBOSE: Tactic: 4653055 Time: 0.053248
VERBOSE: Tactic: 4915199 Time: 0.082112
VERBOSE: Tactic: 5177343 Time: 0.05744
VERBOSE: Tactic: 5373951 Time: 0.066976
VERBOSE: Tactic: 5636095 Time: 0.06208
VERBOSE: Tactic: 5898239 Time: 0.055008
VERBOSE: Tactic: 6225919 Time: 0.059936
VERBOSE: Tactic: 6291455 Time: 0.053472
VERBOSE: Tactic: 6422527 Time: 0.052704
VERBOSE: Tactic: 6750207 Time: 0.095616
VERBOSE: Tactic: 7012351 Time: 0.084224
VERBOSE: Tactic: 7077887 Time: 0.049888
VERBOSE: Tactic: 7340031 Time: 0.057664
VERBOSE: Tactic: 7405567 Time: 0.063232
VERBOSE: Tactic: 7602175 Time: 0.08
VERBOSE: Tactic: 7733247 Time: 0.053376
VERBOSE: Tactic: 8191999 Time: 0.096544
VERBOSE: Tactic: 8257535 Time: 0.084576
VERBOSE: Tactic: 8323071 Time: 0.097728
VERBOSE: Tactic: 8650751 Time: 0.196992
VERBOSE: Tactic: 9109503 Time: 0.194688
VERBOSE: Tactic: 9568255 Time: 0.184544
VERBOSE: Tactic: 10354687 Time: 0.166432
VERBOSE: Tactic: 10747903 Time: 0.11536
VERBOSE: Tactic: 10944511 Time: 0.209408
VERBOSE: Fastest Tactic: 7077887 Time: 0.049888
VERBOSE: --------------- Timing Runner: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 (CaskConvolution)
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 3145259992339075399
VERBOSE: Tactic: 3145259992339075399 Time: 0.080512
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_interior_nn_v1 Tactic: 4000990898022781625
VERBOSE: Tactic: 4000990898022781625 Time: 0.13504
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 4438325421691896755
VERBOSE: Tactic: 4438325421691896755 Time: 0.088928
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 4581732244273465060
VERBOSE: Tactic: 4581732244273465060 Time: 0.085536
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 4934335053031119367
VERBOSE: Tactic: 4934335053031119367 Time: 0.142944
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 6797040896965118050
VERBOSE: Tactic: 6797040896965118050 Time: 0.13616
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 8006952294591770973
VERBOSE: Tactic: 8006952294591770973 Time: 0.159104
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 8097855305881829878
VERBOSE: Tactic: 8097855305881829878 Time: 0.140416
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -7210942453088153035
VERBOSE: Tactic: -7210942453088153035 Time: 0.134976
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -6282183216199417697
VERBOSE: Tactic: -6282183216199417697 Time: 0.154496
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: -5016725782072253841
VERBOSE: Tactic: -5016725782072253841 Time: 0.142048
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_interior_nn_v1 Tactic: -1543391652455542154
VERBOSE: Tactic: -1543391652455542154 Time: 0.14048
VERBOSE: Fastest Tactic: 3145259992339075399 Time: 0.080512
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: FusedConvActConvolution Tactic: 7077887
VERBOSE: *************** Autotuning format combination: Int8(184320,576:4,24,1) -> Int8(9216,576:32,24,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 (CaskConvolution)
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_interior_c32_nn_v1 Tactic: 1025026069226666066
VERBOSE: Tactic: 1025026069226666066 Time: 0.135104
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: 1213457772632185722
VERBOSE: Tactic: 1213457772632185722 Time: 0.141504
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 1713441381477652893
VERBOSE: Tactic: 1713441381477652893 Time: 0.158496
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_interior_c32_nn_v1 Tactic: 2339361327868109050
VERBOSE: Tactic: 2339361327868109050 Time: 0.080576
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 8047041638267142825
VERBOSE: Tactic: 8047041638267142825 Time: 0.153376
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: -7846982807478255793
VERBOSE: Tactic: -7846982807478255793 Time: 0.085536
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_interior_c32_nn_v1 Tactic: -7686150779628967382
VERBOSE: Tactic: -7686150779628967382 Time: 0.140288
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: -6459719113600909000
VERBOSE: Tactic: -6459719113600909000 Time: 0.140928
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: -4573925292554651334
VERBOSE: Tactic: -4573925292554651334 Time: 0.135232
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_interior_c32_nn_v1 Tactic: -4208188808979933945
VERBOSE: Tactic: -4208188808979933945 Time: 0.14048
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: -3566249366964946311
VERBOSE: Tactic: -3566249366964946311 Time: 0.08896
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: -2002418013575043687
VERBOSE: Tactic: -2002418013575043687 Time: 0.136128
VERBOSE: Fastest Tactic: 2339361327868109050 Time: 0.080576
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 2339361327868109050
VERBOSE: *************** Autotuning format combination: Int8(23040,576:32,24,1) -> Int8(9216,576:32,24,1) ***************
VERBOSE: --------------- Timing Runner: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 (CudaGroupConvolution)
VERBOSE: CudaGroupConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 (FusedConvActConvolution)
VERBOSE: Tactic: 589823 Time: 0.237952
VERBOSE: Tactic: 655359 Time: 0.15328
VERBOSE: Tactic: 786431 Time: 0.261152
VERBOSE: Tactic: 851967 Time: 0.112256
VERBOSE: Tactic: 1179647 Time: 0.154688
VERBOSE: Tactic: 1638399 Time: 0.25216
VERBOSE: Tactic: 1835007 Time: 0.297376
VERBOSE: Tactic: 2097151 Time: 0.246208
VERBOSE: Tactic: 3080191 Time: 0.126432
VERBOSE: Tactic: 3538943 Time: 0.172512
VERBOSE: Tactic: 3997695 Time: 0.273344
VERBOSE: Tactic: 4063231 Time: 0.14064
VERBOSE: Tactic: 4259839 Time: 0.216192
VERBOSE: Tactic: 4325375 Time: 0.263424
VERBOSE: Tactic: 4587519 Time: 0.262016
VERBOSE: Tactic: 4653055 Time: 0.151264
VERBOSE: Tactic: 4915199 Time: 0.258688
VERBOSE: Tactic: 5177343 Time: 0.162944
VERBOSE: Tactic: 5373951 Time: 0.201792
VERBOSE: Tactic: 5636095 Time: 0.14112
VERBOSE: Tactic: 5898239 Time: 0.156896
VERBOSE: Tactic: 6225919 Time: 0.206432
VERBOSE: Tactic: 6291455 Time: 0.164992
VERBOSE: Tactic: 6422527 Time: 0.132256
VERBOSE: Tactic: 6750207 Time: 0.349728
VERBOSE: Tactic: 7012351 Time: 0.262112
VERBOSE: Tactic: 7077887 Time: 0.167136
VERBOSE: Tactic: 7340031 Time: 0.150368
VERBOSE: Tactic: 7405567 Time: 0.178464
VERBOSE: Tactic: 7602175 Time: 0.281632
VERBOSE: Tactic: 7733247 Time: 0.1512
VERBOSE: Tactic: 8191999 Time: 0.408448
VERBOSE: Tactic: 8257535 Time: 0.23824
VERBOSE: Tactic: 8323071 Time: 0.357088
VERBOSE: Tactic: 8650751 Time: 0.275008
VERBOSE: Tactic: 9109503 Time: 0.242816
VERBOSE: Tactic: 9568255 Time: 0.25808
VERBOSE: Tactic: 10354687 Time: 0.217408
VERBOSE: Tactic: 10747903 Time: 0.16704
VERBOSE: Tactic: 10944511 Time: 0.290944
VERBOSE: Fastest Tactic: 851967 Time: 0.112256
VERBOSE: --------------- Timing Runner: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 (CaskConvolution)
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 68468667201176803
VERBOSE: Tactic: 68468667201176803 Time: 0.061664
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 1230105269624924765
VERBOSE: Tactic: 1230105269624924765 Time: 0.0576
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 2548946449357458230
VERBOSE: Tactic: 2548946449357458230 Time: 0.0536
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 3242897809704328258
VERBOSE: Tactic: 3242897809704328258 Time: 0.045312
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 3312456766204252694
VERBOSE: Tactic: 3312456766204252694 Time: 0.06256
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 3541919052468401776
VERBOSE: Tactic: 3541919052468401776 Time: 0.05168
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 3927509214678622419
VERBOSE: Tactic: 3927509214678622419 Time: 0.04448
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 4871133328510103657
VERBOSE: Tactic: 4871133328510103657 Time: 0.058432
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 8684013308930763400
VERBOSE: Tactic: 8684013308930763400 Time: 0.061152
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -9138158634332993439
VERBOSE: Tactic: -9138158634332993439 Time: 0.088064
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: -8943710627305202139
VERBOSE: Tactic: -8943710627305202139 Time: 0.046112
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: -8859846367886814331
VERBOSE: Tactic: -8859846367886814331 Time: 0.045696
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -8431788508843860955
VERBOSE: Tactic: -8431788508843860955 Time: 0.053728
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: -8382298409581540699
VERBOSE: Tactic: -8382298409581540699 Time: 0.072416
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r1s1 Tactic: -8172318747337038866
VERBOSE: Tactic: -8172318747337038866 Time: 0.08496
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: -7361755530333096258
VERBOSE: Tactic: -7361755530333096258 Time: 0.057376
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: -7106539943789766885
VERBOSE: Tactic: -7106539943789766885 Time: 0.084448
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: -6969478418607271266
VERBOSE: Tactic: -6969478418607271266 Time: 0.083936
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: -6346247605026339453
VERBOSE: Tactic: -6346247605026339453 Time: 0.045504
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -5932545395595141539
VERBOSE: Tactic: -5932545395595141539 Time: 0.062688
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: -5697614955743334137
VERBOSE: Tactic: -5697614955743334137 Time: 0.052608
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_interior_nt_v1 Tactic: -5428851623690564527
VERBOSE: Tactic: -5428851623690564527 Time: 0.063648
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: -5311474420963248369
VERBOSE: Tactic: -5311474420963248369 Time: 0.063904
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: -4260476497340370474
VERBOSE: Tactic: -4260476497340370474 Time: 0.07344
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r1s1 Tactic: -3531681826488401618
VERBOSE: Tactic: -3531681826488401618 Time: 0.085056
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_interior_nt_v1 Tactic: -3065443564418447033
VERBOSE: Tactic: -3065443564418447033 Time: 0.055104
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -1841683966837205309
VERBOSE: Tactic: -1841683966837205309 Time: 0.087808
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: -1494157908358500249
VERBOSE: Tactic: -1494157908358500249 Time: 0.04656
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -1405083410359886949
VERBOSE: Tactic: -1405083410359886949 Time: 0.066688
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_interior_nt_v1 Tactic: -870280893819213650
VERBOSE: Tactic: -870280893819213650 Time: 0.08736
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_interior_nt_v1 Tactic: -545024964146453661
VERBOSE: Tactic: -545024964146453661 Time: 0.060128
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -496455309852654971
VERBOSE: Tactic: -496455309852654971 Time: 0.054464
VERBOSE: Fastest Tactic: 3927509214678622419 Time: 0.04448
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 3927509214678622419
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(73728,576:4,24,1) -> Float(147456,576,24,1) ***************
VERBOSE: --------------- Timing Runner: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 (CaskConvolution)
VERBOSE: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 892787096507693407
VERBOSE: Tactic: 892787096507693407 Time: 0.07648
VERBOSE: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 1204440019753223942
VERBOSE: Tactic: 1204440019753223942 Time: 0.05584
VERBOSE: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 1659301557717208403
VERBOSE: Tactic: 1659301557717208403 Time: 0.056928
VERBOSE: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 2057291331119027912
VERBOSE: Tactic: 2057291331119027912 Time: 0.052992
VERBOSE: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 3275977259705528576
VERBOSE: Tactic: 3275977259705528576 Time: 0.053792
VERBOSE: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 5623454780463195174
VERBOSE: Tactic: 5623454780463195174 Time: 0.06176
VERBOSE: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: -9204333525109552344
VERBOSE: Tactic: -9204333525109552344 Time: 0.051264
VERBOSE: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: -7924103240988931433
VERBOSE: Tactic: -7924103240988931433 Time: 0.05024
VERBOSE: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_xregs_interior_nn_v1 Tactic: -7489650117016530013
VERBOSE: Tactic: -7489650117016530013 Time: 0.05168
VERBOSE: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -4973811344878172338
VERBOSE: Tactic: -4973811344878172338 Time: 0.075616
VERBOSE: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: -3908975881807046106
VERBOSE: Tactic: -3908975881807046106 Time: 0.05616
VERBOSE: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_interior_nn_v1 Tactic: -1765942417666394360
VERBOSE: Tactic: -1765942417666394360 Time: 0.07568
VERBOSE: Fastest Tactic: -7924103240988931433 Time: 0.05024
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -7924103240988931433
VERBOSE: *************** Autotuning format combination: Int8(9216,576:32,24,1) -> Float(147456,576,24,1) ***************
VERBOSE: --------------- Timing Runner: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 (CaskConvolution)
VERBOSE: CaskConvolution has no valid tactics for this config, skipping
VERBOSE: *************** Autotuning format combination: Int8(9216,576:32,24,1) -> Float(4608,576:32,24,1) ***************
VERBOSE: --------------- Timing Runner: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 (CaskConvolution)
VERBOSE: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: 394365917225754726
VERBOSE: Tactic: 394365917225754726 Time: 0.034304
VERBOSE: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 924563784895318224
VERBOSE: Tactic: 924563784895318224 Time: 0.033472
VERBOSE: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_interior_nt_v1 Tactic: 4531028070024747144
VERBOSE: Tactic: 4531028070024747144 Time: 0.035232
VERBOSE: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_interior_nt_v1 Tactic: 4697540470896098800
VERBOSE: Tactic: 4697540470896098800 Time: 0.032416
VERBOSE: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 6096719469361499298
VERBOSE: Tactic: 6096719469361499298 Time: 0.033184
VERBOSE: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_interior_nt_v1 Tactic: 7469355608320515097
VERBOSE: Tactic: 7469355608320515097 Time: 0.055392
VERBOSE: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: 7785217228143857868
VERBOSE: Tactic: 7785217228143857868 Time: 0.055296
VERBOSE: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: 8315790488934712458
VERBOSE: Tactic: 8315790488934712458 Time: 0.032032
VERBOSE: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_interior_nt_v1 Tactic: 9221575372280690678
VERBOSE: Tactic: 9221575372280690678 Time: 0.03472
VERBOSE: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -8462194455331556195
VERBOSE: Tactic: -8462194455331556195 Time: 0.032064
VERBOSE: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -7638944668269666085
VERBOSE: Tactic: -7638944668269666085 Time: 0.055264
VERBOSE: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: -7185527339793611699
VERBOSE: Tactic: -7185527339793611699 Time: 0.033472
VERBOSE: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -5979101256828290173
VERBOSE: Tactic: -5979101256828290173 Time: 0.036288
VERBOSE: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -5250790590226149674
VERBOSE: Tactic: -5250790590226149674 Time: 0.055488
VERBOSE: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -4831366370915083630
VERBOSE: Tactic: -4831366370915083630 Time: 0.031456
VERBOSE: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -4563432698383308679
VERBOSE: Tactic: -4563432698383308679 Time: 0.055552
VERBOSE: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -3936136542475827126
VERBOSE: Tactic: -3936136542475827126 Time: 0.033728
VERBOSE: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -3784829056659735491
VERBOSE: Tactic: -3784829056659735491 Time: 0.031456
VERBOSE: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -2697681528489059028
VERBOSE: Tactic: -2697681528489059028 Time: 0.034912
VERBOSE: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_interior_nt_v1 Tactic: -2621193268472024213
VERBOSE: Tactic: -2621193268472024213 Time: 0.032704
VERBOSE: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_interior_nt_v1 Tactic: -2120707675073643088
VERBOSE: Tactic: -2120707675073643088 Time: 0.032704
VERBOSE: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: -733152064595858464
VERBOSE: Tactic: -733152064595858464 Time: 0.033152
VERBOSE: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_interior_nt_v1 Tactic: -706197824303187656
VERBOSE: Tactic: -706197824303187656 Time: 0.033024
VERBOSE: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_interior_nt_v1 Tactic: -214244313010793854
VERBOSE: Tactic: -214244313010793854 Time: 0.056192
VERBOSE: Fastest Tactic: -4831366370915083630 Time: 0.031456
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -4831366370915083630
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(147456,576,24,1) -> Float(589824,2304,48,1) ***************
VERBOSE: --------------- Timing Runner: dla_up_ida_0_up_1.1 (CudnnDeconvolution)
VERBOSE: Tactic: 0 Time: 0.162976
VERBOSE: Tactic: 1 Time: 0.162944
VERBOSE: Tactic: 3 Time: 23.0131
VERBOSE: Tactic: 24 Time: 3.66605
VERBOSE: Tactic: 25 Time: 13.9502
VERBOSE: Tactic: 27 Time: 20.6694
VERBOSE: Fastest Tactic: 1 Time: 0.162944
VERBOSE: --------------- Timing Runner: dla_up_ida_0_up_1.1 (GemmDeconvolution)
VERBOSE: Tactic: 0 Time: 4.14973
VERBOSE: Fastest Tactic: 0 Time: 4.14973
VERBOSE: --------------- Timing Runner: dla_up_ida_0_up_1.1 (CaskDeconvolution)
VERBOSE: CaskDeconvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: dla_up_ida_0_up_1.1 (CaskDeconvolutionV2)
VERBOSE: dla_up_ida_0_up_1.1 Set Tactic Name: sm50_xmma_deconv_generic_f32f32_f32_f32_nchwkcrs_nchw Tactic: 1108745107350470483
VERBOSE: Tactic: 1108745107350470483 Time: 10000
VERBOSE: Fastest Tactic: 1108745107350470483 Time: 10000
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CudnnDeconvolution Tactic: 1
VERBOSE: *************** Autotuning format combination: Float(147456,1,6144,256) -> Float(589824,1,12288,256) ***************
VERBOSE: --------------- Timing Runner: dla_up_ida_0_up_1.1 (CudnnDeconvolution)
VERBOSE: CudnnDeconvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: dla_up_ida_0_up_1.1 (GemmDeconvolution)
VERBOSE: GemmDeconvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: dla_up_ida_0_up_1.1 (CaskDeconvolution)
VERBOSE: CaskDeconvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: dla_up_ida_0_up_1.1 (CaskDeconvolutionV2)
VERBOSE: dla_up_ida_0_up_1.1 Set Tactic Name: sm70_xmma_deconv_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x32x8_stage1_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: -6927323623127951136
VERBOSE: Tactic: -6927323623127951136 Time: 7.42371
VERBOSE: dla_up_ida_0_up_1.1 Set Tactic Name: sm70_xmma_deconv_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize256x128x8_stage1_warpsize2x4x1_g1_ffma_aligna4_alignc4 Tactic: -5811489873080635378
VERBOSE: Tactic: -5811489873080635378 Time: 20.9067
VERBOSE: dla_up_ida_0_up_1.1 Set Tactic Name: sm70_xmma_deconv_implicit_gemm_f32f32_f32f32_f32_nhwckrsc_nhwc_tilesize32x64x8_stage1_warpsize1x2x1_g1_ffma_aligna4_alignc4 Tactic: -500373307302467644
VERBOSE: Tactic: -500373307302467644 Time: 5.91408
VERBOSE: Fastest Tactic: -500373307302467644 Time: 5.91408
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskDeconvolutionV2 Tactic: -500373307302467644
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(589824,2304,48,1) -> Int8(1179648,2304,48,1) ***************
VERBOSE: *************** Autotuning format combination: Float(589824,2304,48,1) -> Int8(294912,2304:4,48,1) ***************
VERBOSE: *************** Autotuning format combination: Float(589824,2304,48,1) -> Int8(36864,2304:32,48,1) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(589824,2304,48,1) -> Int8(1179648,2304,48,1) ***************
VERBOSE: *************** Autotuning format combination: Float(589824,2304,48,1) -> Int8(294912,2304:4,48,1) ***************
VERBOSE: *************** Autotuning format combination: Float(589824,2304,48,1) -> Int8(36864,2304:32,48,1) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(294912,2304:4,48,1) -> Int8(147456,2304:4,48,1) ***************
VERBOSE: --------------- Timing Runner: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 (FusedConvActConvolution)
VERBOSE: Tactic: 524287 Time: 0.32272
VERBOSE: Tactic: 720895 Time: 0.262208
VERBOSE: Tactic: 983039 Time: 0.262624
VERBOSE: Tactic: 1048575 Time: 0.268576
VERBOSE: Tactic: 1703935 Time: 0.260992
VERBOSE: Tactic: 1769471 Time: 0.2848
VERBOSE: Tactic: 2424831 Time: 0.288064
VERBOSE: Tactic: 2621439 Time: 0.269472
VERBOSE: Tactic: 3014655 Time: 0.270048
VERBOSE: Tactic: 3604479 Time: 0.269536
VERBOSE: Tactic: 5046271 Time: 0.28624
VERBOSE: Tactic: 6488063 Time: 0.32176
VERBOSE: Tactic: 7274495 Time: 0.274624
VERBOSE: Tactic: 7864319 Time: 0.279808
VERBOSE: Tactic: 8847359 Time: 0.268512
VERBOSE: Tactic: 9043967 Time: 0.265824
VERBOSE: Tactic: 9961471 Time: 0.299808
VERBOSE: Tactic: 10027007 Time: 0.30816
VERBOSE: Tactic: 10485759 Time: 0.268128
VERBOSE: Tactic: 10682367 Time: 0.2672
VERBOSE: Tactic: 10813439 Time: 0.265184
VERBOSE: Fastest Tactic: 1703935 Time: 0.260992
VERBOSE: --------------- Timing Runner: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 (CaskConvolution)
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 4438325421691896755
VERBOSE: Tactic: 4438325421691896755 Time: 0.2272
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 4581732244273465060
VERBOSE: Tactic: 4581732244273465060 Time: 0.21616
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 4934335053031119367
VERBOSE: Tactic: 4934335053031119367 Time: 0.316704
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 6797040896965118050
VERBOSE: Tactic: 6797040896965118050 Time: 0.201984
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 8006952294591770973
VERBOSE: Tactic: 8006952294591770973 Time: 0.263776
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -7210942453088153035
VERBOSE: Tactic: -7210942453088153035 Time: 0.197952
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -6282183216199417697
VERBOSE: Tactic: -6282183216199417697 Time: 0.25376
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: -5026383765466876607
VERBOSE: Tactic: -5026383765466876607 Time: 0.203264
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: -5016725782072253841
VERBOSE: Tactic: -5016725782072253841 Time: 0.310016
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: -1370999262391786833
VERBOSE: Tactic: -1370999262391786833 Time: 0.210816
VERBOSE: Fastest Tactic: -7210942453088153035 Time: 0.197952
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -7210942453088153035
VERBOSE: *************** Autotuning format combination: Int8(294912,2304:4,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: --------------- Timing Runner: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 (CaskConvolution)
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: 1213457772632185722
VERBOSE: Tactic: 1213457772632185722 Time: 0.314048
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 1713441381477652893
VERBOSE: Tactic: 1713441381477652893 Time: 0.264416
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_xregs_large_c32_nn_v1 Tactic: 7125598890155666458
VERBOSE: Tactic: 7125598890155666458 Time: 0.210752
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 8047041638267142825
VERBOSE: Tactic: 8047041638267142825 Time: 0.252896
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: -7846982807478255793
VERBOSE: Tactic: -7846982807478255793 Time: 0.216032
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: -6459719113600909000
VERBOSE: Tactic: -6459719113600909000 Time: 0.21792
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: -4573925292554651334
VERBOSE: Tactic: -4573925292554651334 Time: 0.19824
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: -3566249366964946311
VERBOSE: Tactic: -3566249366964946311 Time: 0.227008
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: -2002418013575043687
VERBOSE: Tactic: -2002418013575043687 Time: 0.200896
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_xregs_large_c32_nn_v1 Tactic: -1659631603542281459
VERBOSE: Tactic: -1659631603542281459 Time: 0.202176
VERBOSE: Fastest Tactic: -4573925292554651334 Time: 0.19824
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -4573925292554651334
VERBOSE: *************** Autotuning format combination: Int8(36864,2304:32,48,1) -> Int8(18432,2304:32,48,1) ***************
VERBOSE: --------------- Timing Runner: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 (CudaGroupConvolution)
VERBOSE: CudaGroupConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 (FusedConvActConvolution)
VERBOSE: Tactic: 524287 Time: 0.2584
VERBOSE: Tactic: 720895 Time: 0.204672
VERBOSE: Tactic: 983039 Time: 0.2072
VERBOSE: Tactic: 1048575 Time: 0.248032
VERBOSE: Tactic: 1703935 Time: 0.20912
VERBOSE: Tactic: 1769471 Time: 0.220512
VERBOSE: Tactic: 2424831 Time: 0.25424
VERBOSE: Tactic: 2621439 Time: 0.213696
VERBOSE: Tactic: 3014655 Time: 0.216064
VERBOSE: Tactic: 3604479 Time: 0.215872
VERBOSE: Tactic: 5046271 Time: 0.22656
VERBOSE: Tactic: 6488063 Time: 0.258656
VERBOSE: Tactic: 7274495 Time: 0.212864
VERBOSE: Tactic: 7864319 Time: 0.222496
VERBOSE: Tactic: 8847359 Time: 0.212416
VERBOSE: Tactic: 9043967 Time: 0.21328
VERBOSE: Tactic: 9961471 Time: 0.252736
VERBOSE: Tactic: 10027007 Time: 0.247008
VERBOSE: Tactic: 10485759 Time: 0.212
VERBOSE: Tactic: 10682367 Time: 0.212384
VERBOSE: Tactic: 10813439 Time: 0.210752
VERBOSE: Fastest Tactic: 720895 Time: 0.204672
VERBOSE: --------------- Timing Runner: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 (CaskConvolution)
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 289888059097454627
VERBOSE: Tactic: 289888059097454627 Time: 0.106944
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 680740992583869928
VERBOSE: Tactic: 680740992583869928 Time: 0.0848
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 1230105269624924765
VERBOSE: Tactic: 1230105269624924765 Time: 0.0712
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 1388866374720163187
VERBOSE: Tactic: 1388866374720163187 Time: 0.133408
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 2133329569091732311
VERBOSE: Tactic: 2133329569091732311 Time: 0.076128
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 2579824863892891529
VERBOSE: Tactic: 2579824863892891529 Time: 0.162496
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: 4099749238296892232
VERBOSE: Tactic: 4099749238296892232 Time: 0.07216
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 4214794893922618058
VERBOSE: Tactic: 4214794893922618058 Time: 0.076704
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 4384868749799132354
VERBOSE: Tactic: 4384868749799132354 Time: 0.156
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 4871133328510103657
VERBOSE: Tactic: 4871133328510103657 Time: 0.069664
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 4931167631624420067
VERBOSE: Tactic: 4931167631624420067 Time: 0.104288
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 5641967928706599451
VERBOSE: Tactic: 5641967928706599451 Time: 0.195616
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 6394572396369862482
VERBOSE: Tactic: 6394572396369862482 Time: 0.195232
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 7048234086361926570
VERBOSE: Tactic: 7048234086361926570 Time: 0.107584
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -9138158634332993439
VERBOSE: Tactic: -9138158634332993439 Time: 0.107008
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -9108166971364503411
VERBOSE: Tactic: -9108166971364503411 Time: 0.108064
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: -8861822316054763526
VERBOSE: Tactic: -8861822316054763526 Time: 0.107648
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: -8475551154769412306
VERBOSE: Tactic: -8475551154769412306 Time: 0.08576
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -8431788508843860955
VERBOSE: Tactic: -8431788508843860955 Time: 0.06608
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: -6371781333659293809
VERBOSE: Tactic: -6371781333659293809 Time: 0.083232
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -5932545395595141539
VERBOSE: Tactic: -5932545395595141539 Time: 0.07664
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: -5180570335464125033
VERBOSE: Tactic: -5180570335464125033 Time: 0.085152
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: -2965659229411050116
VERBOSE: Tactic: -2965659229411050116 Time: 0.081376
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -2499089240293650188
VERBOSE: Tactic: -2499089240293650188 Time: 0.085376
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -2328318099174473157
VERBOSE: Tactic: -2328318099174473157 Time: 0.08336
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -1841683966837205309
VERBOSE: Tactic: -1841683966837205309 Time: 0.108
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -1405083410359886949
VERBOSE: Tactic: -1405083410359886949 Time: 0.080768
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -1044618751912149244
VERBOSE: Tactic: -1044618751912149244 Time: 0.071584
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -496455309852654971
VERBOSE: Tactic: -496455309852654971 Time: 0.065376
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -409197672346571968
VERBOSE: Tactic: -409197672346571968 Time: 0.106112
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: -366411318217594794
VERBOSE: Tactic: -366411318217594794 Time: 0.133568
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: -351548418071036983
VERBOSE: Tactic: -351548418071036983 Time: 0.10544
VERBOSE: Fastest Tactic: -496455309852654971 Time: 0.065376
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -496455309852654971
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(147456,2304:4,48,1) -> Float(294912,2304,48,1) ***************
VERBOSE: *************** Autotuning format combination: Int8(18432,2304:32,48,1) -> Float(294912,2304,48,1) ***************
VERBOSE: *************** Autotuning format combination: Int8(18432,2304:32,48,1) -> Float(9216,2304:32,48,1) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(294912,2304,48,1) -> Float(1179648,9216,96,1) ***************
VERBOSE: *************** Autotuning format combination: Float(294912,1,6144,128) -> Float(1179648,1,12288,128) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(1179648,9216,96,1) -> Int8(2359296,9216,96,1) ***************
VERBOSE: *************** Autotuning format combination: Float(1179648,9216,96,1) -> Int8(589824,9216:4,96,1) ***************
VERBOSE: *************** Autotuning format combination: Float(1179648,9216,96,1) -> Int8(73728,9216:32,96,1) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(1179648,9216,96,1) -> Int8(2359296,9216,96,1) ***************
VERBOSE: *************** Autotuning format combination: Float(1179648,9216,96,1) -> Int8(589824,9216:4,96,1) ***************
VERBOSE: *************** Autotuning format combination: Float(1179648,9216,96,1) -> Int8(73728,9216:32,96,1) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(589824,9216:4,96,1) -> Int8(294912,9216:4,96,1) ***************
VERBOSE: --------------- Timing Runner: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 (FusedConvActConvolution)
VERBOSE: Tactic: 524287 Time: 0.223904
VERBOSE: Tactic: 720895 Time: 0.202656
VERBOSE: Tactic: 983039 Time: 0.195776
VERBOSE: Tactic: 1048575 Time: 0.210432
VERBOSE: Tactic: 1703935 Time: 0.20704
VERBOSE: Tactic: 1769471 Time: 0.247584
VERBOSE: Tactic: 2424831 Time: 0.254048
VERBOSE: Tactic: 2621439 Time: 0.222496
VERBOSE: Tactic: 3014655 Time: 0.201952
VERBOSE: Tactic: 3604479 Time: 0.2016
VERBOSE: Tactic: 5046271 Time: 0.211904
VERBOSE: Tactic: 6488063 Time: 0.213664
VERBOSE: Tactic: 7274495 Time: 0.2192
VERBOSE: Tactic: 7864319 Time: 0.218912
VERBOSE: Tactic: 8847359 Time: 0.223072
VERBOSE: Tactic: 9043967 Time: 0.209728
VERBOSE: Tactic: 9961471 Time: 0.25312
VERBOSE: Tactic: 10027007 Time: 0.213376
VERBOSE: Tactic: 10485759 Time: 0.211712
VERBOSE: Tactic: 10682367 Time: 0.219776
VERBOSE: Tactic: 10813439 Time: 0.196736
VERBOSE: Fastest Tactic: 983039 Time: 0.195776
VERBOSE: --------------- Timing Runner: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 (CaskConvolution)
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 4438325421691896755
VERBOSE: Tactic: 4438325421691896755 Time: 0.214976
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 4581732244273465060
VERBOSE: Tactic: 4581732244273465060 Time: 0.210624
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 4934335053031119367
VERBOSE: Tactic: 4934335053031119367 Time: 0.236544
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 6797040896965118050
VERBOSE: Tactic: 6797040896965118050 Time: 0.199904
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 8006952294591770973
VERBOSE: Tactic: 8006952294591770973 Time: 0.24544
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -7210942453088153035
VERBOSE: Tactic: -7210942453088153035 Time: 0.198304
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -6282183216199417697
VERBOSE: Tactic: -6282183216199417697 Time: 0.240032
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: -5026383765466876607
VERBOSE: Tactic: -5026383765466876607 Time: 0.205472
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: -5016725782072253841
VERBOSE: Tactic: -5016725782072253841 Time: 0.226176
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: -1370999262391786833
VERBOSE: Tactic: -1370999262391786833 Time: 0.208096
VERBOSE: Fastest Tactic: -7210942453088153035 Time: 0.198304
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: FusedConvActConvolution Tactic: 983039
VERBOSE: *************** Autotuning format combination: Int8(589824,9216:4,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: --------------- Timing Runner: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 (CaskConvolution)
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: 1213457772632185722
VERBOSE: Tactic: 1213457772632185722 Time: 0.236064
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 1713441381477652893
VERBOSE: Tactic: 1713441381477652893 Time: 0.244896
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_xregs_large_c32_nn_v1 Tactic: 7125598890155666458
VERBOSE: Tactic: 7125598890155666458 Time: 0.208096
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 8047041638267142825
VERBOSE: Tactic: 8047041638267142825 Time: 0.238656
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: -7846982807478255793
VERBOSE: Tactic: -7846982807478255793 Time: 0.210752
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: -6459719113600909000
VERBOSE: Tactic: -6459719113600909000 Time: 0.225984
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: -4573925292554651334
VERBOSE: Tactic: -4573925292554651334 Time: 0.198944
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: -3566249366964946311
VERBOSE: Tactic: -3566249366964946311 Time: 0.2152
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: -2002418013575043687
VERBOSE: Tactic: -2002418013575043687 Time: 0.20032
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_xregs_large_c32_nn_v1 Tactic: -1659631603542281459
VERBOSE: Tactic: -1659631603542281459 Time: 0.205984
VERBOSE: Fastest Tactic: -4573925292554651334 Time: 0.198944
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -4573925292554651334
VERBOSE: *************** Autotuning format combination: Int8(73728,9216:32,96,1) -> Int8(36864,9216:32,96,1) ***************
VERBOSE: --------------- Timing Runner: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 (CudaGroupConvolution)
VERBOSE: CudaGroupConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 (FusedConvActConvolution)
VERBOSE: Tactic: 524287 Time: 0.229312
VERBOSE: Tactic: 720895 Time: 0.200192
VERBOSE: Tactic: 983039 Time: 0.196864
VERBOSE: Tactic: 1048575 Time: 0.230304
VERBOSE: Tactic: 1703935 Time: 0.211648
VERBOSE: Tactic: 1769471 Time: 0.23968
VERBOSE: Tactic: 2424831 Time: 0.288736
VERBOSE: Tactic: 2621439 Time: 0.223232
VERBOSE: Tactic: 3014655 Time: 0.207808
VERBOSE: Tactic: 3604479 Time: 0.20736
VERBOSE: Tactic: 5046271 Time: 0.223104
VERBOSE: Tactic: 6488063 Time: 0.234912
VERBOSE: Tactic: 7274495 Time: 0.21568
VERBOSE: Tactic: 7864319 Time: 0.22832
VERBOSE: Tactic: 8847359 Time: 0.223808
VERBOSE: Tactic: 9043967 Time: 0.214368
VERBOSE: Tactic: 9961471 Time: 0.275712
VERBOSE: Tactic: 10027007 Time: 0.216736
VERBOSE: Tactic: 10485759 Time: 0.213248
VERBOSE: Tactic: 10682367 Time: 0.222816
VERBOSE: Tactic: 10813439 Time: 0.198688
VERBOSE: Fastest Tactic: 983039 Time: 0.196864
VERBOSE: --------------- Timing Runner: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 (CaskConvolution)
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 289888059097454627
VERBOSE: Tactic: 289888059097454627 Time: 0.064736
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 680740992583869928
VERBOSE: Tactic: 680740992583869928 Time: 0.08784
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 1230105269624924765
VERBOSE: Tactic: 1230105269624924765 Time: 0.069824
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 1388866374720163187
VERBOSE: Tactic: 1388866374720163187 Time: 0.136288
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 2133329569091732311
VERBOSE: Tactic: 2133329569091732311 Time: 0.070528
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 2579824863892891529
VERBOSE: Tactic: 2579824863892891529 Time: 0.155392
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: 4099749238296892232
VERBOSE: Tactic: 4099749238296892232 Time: 0.069632
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 4214794893922618058
VERBOSE: Tactic: 4214794893922618058 Time: 0.072
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 4384868749799132354
VERBOSE: Tactic: 4384868749799132354 Time: 0.155008
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 4871133328510103657
VERBOSE: Tactic: 4871133328510103657 Time: 0.069408
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 4931167631624420067
VERBOSE: Tactic: 4931167631624420067 Time: 0.120384
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 5641967928706599451
VERBOSE: Tactic: 5641967928706599451 Time: 0.184672
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 6394572396369862482
VERBOSE: Tactic: 6394572396369862482 Time: 0.185376
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 7048234086361926570
VERBOSE: Tactic: 7048234086361926570 Time: 0.104928
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -9138158634332993439
VERBOSE: Tactic: -9138158634332993439 Time: 0.064576
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -9108166971364503411
VERBOSE: Tactic: -9108166971364503411 Time: 0.106144
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: -8861822316054763526
VERBOSE: Tactic: -8861822316054763526 Time: 0.066336
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: -8475551154769412306
VERBOSE: Tactic: -8475551154769412306 Time: 0.07808
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -8431788508843860955
VERBOSE: Tactic: -8431788508843860955 Time: 0.0656
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: -6371781333659293809
VERBOSE: Tactic: -6371781333659293809 Time: 0.084768
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -5932545395595141539
VERBOSE: Tactic: -5932545395595141539 Time: 0.081376
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: -5180570335464125033
VERBOSE: Tactic: -5180570335464125033 Time: 0.077248
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: -2965659229411050116
VERBOSE: Tactic: -2965659229411050116 Time: 0.085536
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -2499089240293650188
VERBOSE: Tactic: -2499089240293650188 Time: 0.0888
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -2328318099174473157
VERBOSE: Tactic: -2328318099174473157 Time: 0.085088
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -1841683966837205309
VERBOSE: Tactic: -1841683966837205309 Time: 0.062016
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -1405083410359886949
VERBOSE: Tactic: -1405083410359886949 Time: 0.083808
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -1044618751912149244
VERBOSE: Tactic: -1044618751912149244 Time: 0.068704
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -496455309852654971
VERBOSE: Tactic: -496455309852654971 Time: 0.065216
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -409197672346571968
VERBOSE: Tactic: -409197672346571968 Time: 0.062976
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: -366411318217594794
VERBOSE: Tactic: -366411318217594794 Time: 0.137696
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: -351548418071036983
VERBOSE: Tactic: -351548418071036983 Time: 0.123232
VERBOSE: Fastest Tactic: -1841683966837205309 Time: 0.062016
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -1841683966837205309
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(294912,9216:4,96,1) -> Float(589824,9216,96,1) ***************
VERBOSE: *************** Autotuning format combination: Int8(36864,9216:32,96,1) -> Float(589824,9216,96,1) ***************
VERBOSE: *************** Autotuning format combination: Int8(36864,9216:32,96,1) -> Float(18432,9216:32,96,1) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(589824,9216,96,1) -> Float(2359296,36864,192,1) ***************
VERBOSE: *************** Autotuning format combination: Float(589824,1,6144,64) -> Float(2359296,1,12288,64) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(2359296,36864,192,1) -> Int8(4718592,36864,192,1) ***************
VERBOSE: *************** Autotuning format combination: Float(2359296,36864,192,1) -> Int8(1179648,36864:4,192,1) ***************
VERBOSE: *************** Autotuning format combination: Float(2359296,36864,192,1) -> Int8(147456,36864:32,192,1) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(1179648,36864:4,192,1) -> Int8(589824,36864:4,192,1) ***************
VERBOSE: --------------- Timing Runner: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 (FusedConvActConvolution)
VERBOSE: Tactic: 524287 Time: 0.22032
VERBOSE: Tactic: 720895 Time: 0.206016
VERBOSE: Tactic: 983039 Time: 0.202912
VERBOSE: Tactic: 1048575 Time: 0.218816
VERBOSE: Tactic: 1703935 Time: 0.216192
VERBOSE: Tactic: 1769471 Time: 0.300224
VERBOSE: Tactic: 2424831 Time: 0.318336
VERBOSE: Tactic: 2621439 Time: 0.23984
VERBOSE: Tactic: 3014655 Time: 0.21216
VERBOSE: Tactic: 3604479 Time: 0.209984
VERBOSE: Tactic: 5046271 Time: 0.221952
VERBOSE: Tactic: 6488063 Time: 0.2224
VERBOSE: Tactic: 7274495 Time: 0.249472
VERBOSE: Tactic: 7864319 Time: 0.237376
VERBOSE: Tactic: 8847359 Time: 0.25184
VERBOSE: Tactic: 9043967 Time: 0.22032
VERBOSE: Tactic: 9961471 Time: 0.316608
VERBOSE: Tactic: 10027007 Time: 0.218272
VERBOSE: Tactic: 10485759 Time: 0.216
VERBOSE: Tactic: 10682367 Time: 0.234496
VERBOSE: Tactic: 10813439 Time: 0.198144
VERBOSE: Fastest Tactic: 10813439 Time: 0.198144
VERBOSE: --------------- Timing Runner: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 (CaskConvolution)
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 4438325421691896755
VERBOSE: Tactic: 4438325421691896755 Time: 0.215488
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 4581732244273465060
VERBOSE: Tactic: 4581732244273465060 Time: 0.212608
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 4934335053031119367
VERBOSE: Tactic: 4934335053031119367 Time: 0.220544
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 6797040896965118050
VERBOSE: Tactic: 6797040896965118050 Time: 0.392416
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 8006952294591770973
VERBOSE: Tactic: 8006952294591770973 Time: 0.245088
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -7210942453088153035
VERBOSE: Tactic: -7210942453088153035 Time: 0.390432
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -6282183216199417697
VERBOSE: Tactic: -6282183216199417697 Time: 0.23984
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: -5026383765466876607
VERBOSE: Tactic: -5026383765466876607 Time: 0.421184
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: -5016725782072253841
VERBOSE: Tactic: -5016725782072253841 Time: 0.218208
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: -1370999262391786833
VERBOSE: Tactic: -1370999262391786833 Time: 0.207936
VERBOSE: Fastest Tactic: -1370999262391786833 Time: 0.207936
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: FusedConvActConvolution Tactic: 10813439
VERBOSE: *************** Autotuning format combination: Int8(1179648,36864:4,192,1) -> Int8(73728,36864:32,192,1) ***************
VERBOSE: --------------- Timing Runner: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 (CaskConvolution)
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: 1213457772632185722
VERBOSE: Tactic: 1213457772632185722 Time: 0.2208
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 1713441381477652893
VERBOSE: Tactic: 1713441381477652893 Time: 0.246592
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_xregs_large_c32_nn_v1 Tactic: 7125598890155666458
VERBOSE: Tactic: 7125598890155666458 Time: 0.208672
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 8047041638267142825
VERBOSE: Tactic: 8047041638267142825 Time: 0.238656
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: -7846982807478255793
VERBOSE: Tactic: -7846982807478255793 Time: 0.2128
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: -6459719113600909000
VERBOSE: Tactic: -6459719113600909000 Time: 0.218368
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: -4573925292554651334
VERBOSE: Tactic: -4573925292554651334 Time: 0.391136
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: -3566249366964946311
VERBOSE: Tactic: -3566249366964946311 Time: 0.215648
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: -2002418013575043687
VERBOSE: Tactic: -2002418013575043687 Time: 0.392576
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_xregs_large_c32_nn_v1 Tactic: -1659631603542281459
VERBOSE: Tactic: -1659631603542281459 Time: 0.420416
VERBOSE: Fastest Tactic: 7125598890155666458 Time: 0.208672
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 7125598890155666458
VERBOSE: *************** Autotuning format combination: Int8(147456,36864:32,192,1) -> Int8(73728,36864:32,192,1) ***************
VERBOSE: --------------- Timing Runner: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 (CudaGroupConvolution)
VERBOSE: CudaGroupConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 (FusedConvActConvolution)
VERBOSE: Tactic: 524287 Time: 0.225376
VERBOSE: Tactic: 720895 Time: 0.2032
VERBOSE: Tactic: 983039 Time: 0.201984
VERBOSE: Tactic: 1048575 Time: 0.222048
VERBOSE: Tactic: 1703935 Time: 0.221504
VERBOSE: Tactic: 1769471 Time: 0.28688
VERBOSE: Tactic: 2424831 Time: 0.372288
VERBOSE: Tactic: 2621439 Time: 0.24112
VERBOSE: Tactic: 3014655 Time: 0.218144
VERBOSE: Tactic: 3604479 Time: 0.218144
VERBOSE: Tactic: 5046271 Time: 0.218912
VERBOSE: Tactic: 6488063 Time: 0.228832
VERBOSE: Tactic: 7274495 Time: 0.240768
VERBOSE: Tactic: 7864319 Time: 0.254784
VERBOSE: Tactic: 8847359 Time: 0.252064
VERBOSE: Tactic: 9043967 Time: 0.221216
VERBOSE: Tactic: 9961471 Time: 0.36336
VERBOSE: Tactic: 10027007 Time: 0.220896
VERBOSE: Tactic: 10485759 Time: 0.217408
VERBOSE: Tactic: 10682367 Time: 0.240672
VERBOSE: Tactic: 10813439 Time: 0.198752
VERBOSE: Fastest Tactic: 10813439 Time: 0.198752
VERBOSE: --------------- Timing Runner: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 (CaskConvolution)
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 289888059097454627
VERBOSE: Tactic: 289888059097454627 Time: 0.137216
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 680740992583869928
VERBOSE: Tactic: 680740992583869928 Time: 0.147328
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 1230105269624924765
VERBOSE: Tactic: 1230105269624924765 Time: 0.076192
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 1388866374720163187
VERBOSE: Tactic: 1388866374720163187 Time: 0.137504
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 2133329569091732311
VERBOSE: Tactic: 2133329569091732311 Time: 0.130336
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 2579824863892891529
VERBOSE: Tactic: 2579824863892891529 Time: 0.156096
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: 4099749238296892232
VERBOSE: Tactic: 4099749238296892232 Time: 0.076704
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 4214794893922618058
VERBOSE: Tactic: 4214794893922618058 Time: 0.12976
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 4384868749799132354
VERBOSE: Tactic: 4384868749799132354 Time: 0.155616
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 4871133328510103657
VERBOSE: Tactic: 4871133328510103657 Time: 0.07568
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 4931167631624420067
VERBOSE: Tactic: 4931167631624420067 Time: 0.259104
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 5641967928706599451
VERBOSE: Tactic: 5641967928706599451 Time: 0.186112
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 6394572396369862482
VERBOSE: Tactic: 6394572396369862482 Time: 0.186272
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 7048234086361926570
VERBOSE: Tactic: 7048234086361926570 Time: 0.105088
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -9138158634332993439
VERBOSE: Tactic: -9138158634332993439 Time: 0.133344
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -9108166971364503411
VERBOSE: Tactic: -9108166971364503411 Time: 0.105888
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: -8861822316054763526
VERBOSE: Tactic: -8861822316054763526 Time: 0.140032
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: -8475551154769412306
VERBOSE: Tactic: -8475551154769412306 Time: 0.084576
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -8431788508843860955
VERBOSE: Tactic: -8431788508843860955 Time: 0.124256
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: -6371781333659293809
VERBOSE: Tactic: -6371781333659293809 Time: 0.089856
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -5932545395595141539
VERBOSE: Tactic: -5932545395595141539 Time: 0.09264
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: -5180570335464125033
VERBOSE: Tactic: -5180570335464125033 Time: 0.084128
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: -2965659229411050116
VERBOSE: Tactic: -2965659229411050116 Time: 0.096928
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -2499089240293650188
VERBOSE: Tactic: -2499089240293650188 Time: 0.148032
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -2328318099174473157
VERBOSE: Tactic: -2328318099174473157 Time: 0.09024
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -1841683966837205309
VERBOSE: Tactic: -1841683966837205309 Time: 0.129312
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -1405083410359886949
VERBOSE: Tactic: -1405083410359886949 Time: 0.095392
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -1044618751912149244
VERBOSE: Tactic: -1044618751912149244 Time: 0.13008
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -496455309852654971
VERBOSE: Tactic: -496455309852654971 Time: 0.124864
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -409197672346571968
VERBOSE: Tactic: -409197672346571968 Time: 0.131936
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: -366411318217594794
VERBOSE: Tactic: -366411318217594794 Time: 0.136704
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: -351548418071036983
VERBOSE: Tactic: -351548418071036983 Time: 0.265888
VERBOSE: Fastest Tactic: 4871133328510103657 Time: 0.07568
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 4871133328510103657
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(589824,36864:4,192,1) -> Int8(2359296,36864:4,192,1) ***************
VERBOSE: --------------- Timing Runner: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 (FusedConvActConvolution)
VERBOSE: FusedConvActConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 (CaskConvolution)
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 4438325421691896755
VERBOSE: Tactic: 4438325421691896755 Time: 0.385792
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 4581732244273465060
VERBOSE: Tactic: 4581732244273465060 Time: 0.382176
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 4934335053031119367
VERBOSE: Tactic: 4934335053031119367 Time: 0.411744
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 6797040896965118050
VERBOSE: Tactic: 6797040896965118050 Time: 0.379968
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 8006952294591770973
VERBOSE: Tactic: 8006952294591770973 Time: 0.43232
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -7210942453088153035
VERBOSE: Tactic: -7210942453088153035 Time: 0.37936
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -6282183216199417697
VERBOSE: Tactic: -6282183216199417697 Time: 0.420736
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: -5026383765466876607
VERBOSE: Tactic: -5026383765466876607 Time: 0.435456
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: -5016725782072253841
VERBOSE: Tactic: -5016725782072253841 Time: 0.403648
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: -1370999262391786833
VERBOSE: Tactic: -1370999262391786833 Time: 0.377312
VERBOSE: Fastest Tactic: -1370999262391786833 Time: 0.377312
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -1370999262391786833
VERBOSE: *************** Autotuning format combination: Int8(589824,36864:4,192,1) -> Int8(294912,36864:32,192,1) ***************
VERBOSE: --------------- Timing Runner: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 (CaskConvolution)
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: 1213457772632185722
VERBOSE: Tactic: 1213457772632185722 Time: 0.410176
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 1713441381477652893
VERBOSE: Tactic: 1713441381477652893 Time: 0.43136
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_xregs_large_c32_nn_v1 Tactic: 7125598890155666458
VERBOSE: Tactic: 7125598890155666458 Time: 0.381728
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 8047041638267142825
VERBOSE: Tactic: 8047041638267142825 Time: 0.421792
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: -7846982807478255793
VERBOSE: Tactic: -7846982807478255793 Time: 0.383744
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: -6459719113600909000
VERBOSE: Tactic: -6459719113600909000 Time: 0.403072
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: -4573925292554651334
VERBOSE: Tactic: -4573925292554651334 Time: 0.40592
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: -3566249366964946311
VERBOSE: Tactic: -3566249366964946311 Time: 0.387488
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: -2002418013575043687
VERBOSE: Tactic: -2002418013575043687 Time: 0.405344
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x128_relu_xregs_large_c32_nn_v1 Tactic: -1659631603542281459
VERBOSE: Tactic: -1659631603542281459 Time: 0.474144
VERBOSE: Fastest Tactic: 7125598890155666458 Time: 0.381728
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 7125598890155666458
VERBOSE: *************** Autotuning format combination: Int8(73728,36864:32,192,1) -> Int8(294912,36864:32,192,1) ***************
VERBOSE: --------------- Timing Runner: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 (CudaGroupConvolution)
VERBOSE: CudaGroupConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 (FusedConvActConvolution)
VERBOSE: FusedConvActConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 (CaskConvolution)
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 289888059097454627
VERBOSE: Tactic: 289888059097454627 Time: 0.163008
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 680740992583869928
VERBOSE: Tactic: 680740992583869928 Time: 0.172064
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 1230105269624924765
VERBOSE: Tactic: 1230105269624924765 Time: 0.14608
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 1388866374720163187
VERBOSE: Tactic: 1388866374720163187 Time: 0.28144
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 2133329569091732311
VERBOSE: Tactic: 2133329569091732311 Time: 0.147392
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 2579824863892891529
VERBOSE: Tactic: 2579824863892891529 Time: 0.326592
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: 4099749238296892232
VERBOSE: Tactic: 4099749238296892232 Time: 0.145632
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 4214794893922618058
VERBOSE: Tactic: 4214794893922618058 Time: 0.1448
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 4384868749799132354
VERBOSE: Tactic: 4384868749799132354 Time: 0.32592
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 4871133328510103657
VERBOSE: Tactic: 4871133328510103657 Time: 0.143872
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 4931167631624420067
VERBOSE: Tactic: 4931167631624420067 Time: 0.158784
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 5641967928706599451
VERBOSE: Tactic: 5641967928706599451 Time: 0.39008
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 6394572396369862482
VERBOSE: Tactic: 6394572396369862482 Time: 0.389952
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 7048234086361926570
VERBOSE: Tactic: 7048234086361926570 Time: 0.210784
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -9138158634332993439
VERBOSE: Tactic: -9138158634332993439 Time: 0.158656
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -9108166971364503411
VERBOSE: Tactic: -9108166971364503411 Time: 0.211584
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: -8861822316054763526
VERBOSE: Tactic: -8861822316054763526 Time: 0.169888
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: -8475551154769412306
VERBOSE: Tactic: -8475551154769412306 Time: 0.162176
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -8431788508843860955
VERBOSE: Tactic: -8431788508843860955 Time: 0.133696
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: -6371781333659293809
VERBOSE: Tactic: -6371781333659293809 Time: 0.17312
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -5932545395595141539
VERBOSE: Tactic: -5932545395595141539 Time: 0.200768
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: -5180570335464125033
VERBOSE: Tactic: -5180570335464125033 Time: 0.161984
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: -2965659229411050116
VERBOSE: Tactic: -2965659229411050116 Time: 0.21104
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -2499089240293650188
VERBOSE: Tactic: -2499089240293650188 Time: 0.171968
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -2328318099174473157
VERBOSE: Tactic: -2328318099174473157 Time: 0.176032
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -1841683966837205309
VERBOSE: Tactic: -1841683966837205309 Time: 0.154944
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -1405083410359886949
VERBOSE: Tactic: -1405083410359886949 Time: 0.204384
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -1044618751912149244
VERBOSE: Tactic: -1044618751912149244 Time: 0.141248
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -496455309852654971
VERBOSE: Tactic: -496455309852654971 Time: 0.13472
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -409197672346571968
VERBOSE: Tactic: -409197672346571968 Time: 0.157312
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: -366411318217594794
VERBOSE: Tactic: -366411318217594794 Time: 0.279424
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: -351548418071036983
VERBOSE: Tactic: -351548418071036983 Time: 0.160288
VERBOSE: Fastest Tactic: -8431788508843860955 Time: 0.133696
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -8431788508843860955
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(589824,36864:4,192,1) -> Int8(2359296,36864:4,192,1) ***************
VERBOSE: *************** Autotuning format combination: Int8(589824,36864:4,192,1) -> Int8(294912,36864:32,192,1) ***************
VERBOSE: *************** Autotuning format combination: Int8(73728,36864:32,192,1) -> Int8(294912,36864:32,192,1) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(589824,36864:4,192,1) -> Int8(2359296,36864:4,192,1) ***************
VERBOSE: *************** Autotuning format combination: Int8(589824,36864:4,192,1) -> Int8(294912,36864:32,192,1) ***************
VERBOSE: *************** Autotuning format combination: Int8(73728,36864:32,192,1) -> Int8(294912,36864:32,192,1) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(2359296,36864:4,192,1) -> Float(4718592,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 (CaskConvolution)
VERBOSE: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 892787096507693407
VERBOSE: Tactic: 892787096507693407 Time: 0.152128
VERBOSE: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 1204440019753223942
VERBOSE: Tactic: 1204440019753223942 Time: 0.250464
VERBOSE: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 1659301557717208403
VERBOSE: Tactic: 1659301557717208403 Time: 0.24656
VERBOSE: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 2057291331119027912
VERBOSE: Tactic: 2057291331119027912 Time: 0.169088
VERBOSE: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 3275977259705528576
VERBOSE: Tactic: 3275977259705528576 Time: 0.251552
VERBOSE: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 5623454780463195174
VERBOSE: Tactic: 5623454780463195174 Time: 0.248288
VERBOSE: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: -9204333525109552344
VERBOSE: Tactic: -9204333525109552344 Time: 0.17232
VERBOSE: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: -7924103240988931433
VERBOSE: Tactic: -7924103240988931433 Time: 0.179424
VERBOSE: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_xregs_interior_nn_v1 Tactic: -7489650117016530013
VERBOSE: Tactic: -7489650117016530013 Time: 0.24944
VERBOSE: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -4973811344878172338
VERBOSE: Tactic: -4973811344878172338 Time: 0.156576
VERBOSE: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: -3908975881807046106
VERBOSE: Tactic: -3908975881807046106 Time: 0.247136
VERBOSE: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_interior_nn_v1 Tactic: -1765942417666394360
VERBOSE: Tactic: -1765942417666394360 Time: 0.15168
VERBOSE: Fastest Tactic: -1765942417666394360 Time: 0.15168
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -1765942417666394360
VERBOSE: *************** Autotuning format combination: Int8(294912,36864:32,192,1) -> Float(4718592,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 (CaskConvolution)
VERBOSE: CaskConvolution has no valid tactics for this config, skipping
VERBOSE: *************** Autotuning format combination: Int8(294912,36864:32,192,1) -> Float(147456,36864:32,192,1) ***************
VERBOSE: --------------- Timing Runner: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 (CaskConvolution)
VERBOSE: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: 394365917225754726
VERBOSE: Tactic: 394365917225754726 Time: 0.18944
VERBOSE: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 924563784895318224
VERBOSE: Tactic: 924563784895318224 Time: 0.190816
VERBOSE: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_interior_nt_v1 Tactic: 4531028070024747144
VERBOSE: Tactic: 4531028070024747144 Time: 0.190208
VERBOSE: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_interior_nt_v1 Tactic: 4697540470896098800
VERBOSE: Tactic: 4697540470896098800 Time: 0.159968
VERBOSE: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 6096719469361499298
VERBOSE: Tactic: 6096719469361499298 Time: 0.190592
VERBOSE: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_interior_nt_v1 Tactic: 7469355608320515097
VERBOSE: Tactic: 7469355608320515097 Time: 0.199232
VERBOSE: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: 7785217228143857868
VERBOSE: Tactic: 7785217228143857868 Time: 0.197952
VERBOSE: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: 8315790488934712458
VERBOSE: Tactic: 8315790488934712458 Time: 0.159136
VERBOSE: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_interior_nt_v1 Tactic: 9221575372280690678
VERBOSE: Tactic: 9221575372280690678 Time: 0.187232
VERBOSE: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -8462194455331556195
VERBOSE: Tactic: -8462194455331556195 Time: 0.160224
VERBOSE: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -7638944668269666085
VERBOSE: Tactic: -7638944668269666085 Time: 0.196896
VERBOSE: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: -7185527339793611699
VERBOSE: Tactic: -7185527339793611699 Time: 0.192352
VERBOSE: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -5979101256828290173
VERBOSE: Tactic: -5979101256828290173 Time: 0.189472
VERBOSE: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -5250790590226149674
VERBOSE: Tactic: -5250790590226149674 Time: 0.197376
VERBOSE: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -4831366370915083630
VERBOSE: Tactic: -4831366370915083630 Time: 0.15936
VERBOSE: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -4563432698383308679
VERBOSE: Tactic: -4563432698383308679 Time: 0.198912
VERBOSE: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -3936136542475827126
VERBOSE: Tactic: -3936136542475827126 Time: 0.190176
VERBOSE: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -3784829056659735491
VERBOSE: Tactic: -3784829056659735491 Time: 0.1608
VERBOSE: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -2697681528489059028
VERBOSE: Tactic: -2697681528489059028 Time: 0.188608
VERBOSE: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_interior_nt_v1 Tactic: -2621193268472024213
VERBOSE: Tactic: -2621193268472024213 Time: 0.161024
VERBOSE: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_interior_nt_v1 Tactic: -2120707675073643088
VERBOSE: Tactic: -2120707675073643088 Time: 0.19136
VERBOSE: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: -733152064595858464
VERBOSE: Tactic: -733152064595858464 Time: 0.193056
VERBOSE: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_interior_nt_v1 Tactic: -706197824303187656
VERBOSE: Tactic: -706197824303187656 Time: 0.193472
VERBOSE: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_interior_nt_v1 Tactic: -214244313010793854
VERBOSE: Tactic: -214244313010793854 Time: 0.196576
VERBOSE: Fastest Tactic: 8315790488934712458 Time: 0.159136
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 8315790488934712458
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(2359296,36864:4,192,1) -> Float(9584640,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 (CudaDepthwiseConvolution)
VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
VERBOSE: --------------- Timing Runner: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 (CaskConvolution)
VERBOSE: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 892787096507693407
VERBOSE: Tactic: 892787096507693407 Time: 0.109728
VERBOSE: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 1204440019753223942
VERBOSE: Tactic: 1204440019753223942 Time: 0.057056
VERBOSE: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 1659301557717208403
VERBOSE: Tactic: 1659301557717208403 Time: 0.049888
VERBOSE: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 2057291331119027912
VERBOSE: Tactic: 2057291331119027912 Time: 0.06352
VERBOSE: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 3275977259705528576
VERBOSE: Tactic: 3275977259705528576 Time: 0.056896
VERBOSE: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 5623454780463195174
VERBOSE: Tactic: 5623454780463195174 Time: 0.05072
VERBOSE: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: -9204333525109552344
VERBOSE: Tactic: -9204333525109552344 Time: 0.063008
VERBOSE: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: -7924103240988931433
VERBOSE: Tactic: -7924103240988931433 Time: 0.061952
VERBOSE: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_xregs_interior_nn_v1 Tactic: -7489650117016530013
VERBOSE: Tactic: -7489650117016530013 Time: 0.056704
VERBOSE: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -4973811344878172338
VERBOSE: Tactic: -4973811344878172338 Time: 0.110272
VERBOSE: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 Set Tactic Name: volta_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: -3908975881807046106
VERBOSE: Tactic: -3908975881807046106 Time: 0.049984
VERBOSE: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_interior_nn_v1 Tactic: -1765942417666394360
VERBOSE: Tactic: -1765942417666394360 Time: 0.109248
VERBOSE: Fastest Tactic: 1659301557717208403 Time: 0.049888
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 1659301557717208403
VERBOSE: *************** Autotuning format combination: Int8(294912,36864:32,192,1) -> Float(9584640,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 (CaskConvolution)
VERBOSE: CaskConvolution has no valid tactics for this config, skipping
VERBOSE: *************** Autotuning format combination: Int8(294912,36864:32,192,1) -> Float(331776,36864:32,192,1) ***************
VERBOSE: --------------- Timing Runner: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 (CaskConvolution)
VERBOSE: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: 394365917225754726
VERBOSE: Tactic: 394365917225754726 Time: 0.072128
VERBOSE: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 924563784895318224
VERBOSE: Tactic: 924563784895318224 Time: 0.065696
VERBOSE: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_interior_nt_v1 Tactic: 4531028070024747144
VERBOSE: Tactic: 4531028070024747144 Time: 0.071776
VERBOSE: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_interior_nt_v1 Tactic: 4697540470896098800
VERBOSE: Tactic: 4697540470896098800 Time: 0.066784
VERBOSE: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 6096719469361499298
VERBOSE: Tactic: 6096719469361499298 Time: 0.0656
VERBOSE: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_interior_nt_v1 Tactic: 7469355608320515097
VERBOSE: Tactic: 7469355608320515097 Time: 0.072992
VERBOSE: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: 7785217228143857868
VERBOSE: Tactic: 7785217228143857868 Time: 0.072544
VERBOSE: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: 8315790488934712458
VERBOSE: Tactic: 8315790488934712458 Time: 0.066784
VERBOSE: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_interior_nt_v1 Tactic: 9221575372280690678
VERBOSE: Tactic: 9221575372280690678 Time: 0.072
VERBOSE: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -8462194455331556195
VERBOSE: Tactic: -8462194455331556195 Time: 0.06784
VERBOSE: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -7638944668269666085
VERBOSE: Tactic: -7638944668269666085 Time: 0.072704
VERBOSE: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: -7185527339793611699
VERBOSE: Tactic: -7185527339793611699 Time: 0.064608
VERBOSE: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -5979101256828290173
VERBOSE: Tactic: -5979101256828290173 Time: 0.072
VERBOSE: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -5250790590226149674
VERBOSE: Tactic: -5250790590226149674 Time: 0.072064
VERBOSE: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -4831366370915083630
VERBOSE: Tactic: -4831366370915083630 Time: 0.067008
VERBOSE: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -4563432698383308679
VERBOSE: Tactic: -4563432698383308679 Time: 0.071168
VERBOSE: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -3936136542475827126
VERBOSE: Tactic: -3936136542475827126 Time: 0.071552
VERBOSE: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -3784829056659735491
VERBOSE: Tactic: -3784829056659735491 Time: 0.066688
VERBOSE: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -2697681528489059028
VERBOSE: Tactic: -2697681528489059028 Time: 0.072256
VERBOSE: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_interior_nt_v1 Tactic: -2621193268472024213
VERBOSE: Tactic: -2621193268472024213 Time: 0.067008
VERBOSE: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_interior_nt_v1 Tactic: -2120707675073643088
VERBOSE: Tactic: -2120707675073643088 Time: 0.066112
VERBOSE: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: -733152064595858464
VERBOSE: Tactic: -733152064595858464 Time: 0.06608
VERBOSE: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_interior_nt_v1 Tactic: -706197824303187656
VERBOSE: Tactic: -706197824303187656 Time: 0.066144
VERBOSE: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x128_ldg16_relu_interior_nt_v1 Tactic: -214244313010793854
VERBOSE: Tactic: -214244313010793854 Time: 0.073312
VERBOSE: Fastest Tactic: -7185527339793611699 Time: 0.064608
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -7185527339793611699
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Int8(2359296,36864:4,192,1) -> Float(9584640,36864,192,1) ***************
VERBOSE: *************** Autotuning format combination: Int8(294912,36864:32,192,1) -> Float(9584640,36864,192,1) ***************
VERBOSE: *************** Autotuning format combination: Int8(294912,36864:32,192,1) -> Float(331776,36864:32,192,1) ***************
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(4718592,36864,192,1) -> Float(4718592,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: PWN(input11.1) (PointWiseV2)
VERBOSE: Tactic: 0 Time: 0.157472
VERBOSE: Tactic: 1 Time: 0.163808
VERBOSE: Tactic: 2 Time: 0.155168
VERBOSE: Tactic: 3 Time: 0.17568
VERBOSE: Tactic: 4 Time: 0.161472
VERBOSE: Tactic: 5 Time: 0.158528
VERBOSE: Tactic: 6 Time: 0.191936
VERBOSE: Tactic: 7 Time: 0.1688
VERBOSE: Tactic: 8 Time: 0.164256
VERBOSE: Tactic: 9 Time: 0.163456
VERBOSE: Tactic: 28 Time: 0.212352
VERBOSE: Fastest Tactic: 2 Time: 0.155168
VERBOSE: --------------- Timing Runner: PWN(input11.1) (PointWise)
VERBOSE: PointWise has no valid tactics for this config, skipping
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 2
VERBOSE: *************** Autotuning format combination: Float(4718592,1,24576,128) -> Float(4718592,1,24576,128) ***************
VERBOSE: --------------- Timing Runner: PWN(input11.1) (PointWiseV2)
VERBOSE: Tactic: 0 Time: 0.21664
VERBOSE: Tactic: 1 Time: 0.172224
VERBOSE: Tactic: 2 Time: 0.1552
VERBOSE: Tactic: 3 Time: 0.179584
VERBOSE: Tactic: 4 Time: 0.162528
VERBOSE: Tactic: 5 Time: 0.158144
VERBOSE: Tactic: 6 Time: 0.193312
VERBOSE: Tactic: 7 Time: 0.169568
VERBOSE: Tactic: 8 Time: 0.164896
VERBOSE: Tactic: 9 Time: 0.164384
VERBOSE: Tactic: 28 Time: 0.212576
VERBOSE: Fastest Tactic: 2 Time: 0.1552
VERBOSE: --------------- Timing Runner: PWN(input11.1) (PointWise)
VERBOSE: PointWise has no valid tactics for this config, skipping
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 2
VERBOSE: *************** Autotuning format combination: Float(147456,36864:32,192,1) -> Float(147456,36864:32,192,1) ***************
VERBOSE: --------------- Timing Runner: PWN(input11.1) (PointWiseV2)
VERBOSE: Tactic: 24 Time: 0.158048
VERBOSE: Tactic: 25 Time: 0.163584
VERBOSE: Tactic: 26 Time: 0.1672
VERBOSE: Tactic: 27 Time: 0.172
VERBOSE: Tactic: 31 Time: 0.157408
VERBOSE: Fastest Tactic: 31 Time: 0.157408
VERBOSE: --------------- Timing Runner: PWN(input11.1) (PointWise)
VERBOSE: PointWise has no valid tactics for this config, skipping
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 31
VERBOSE: *************** Autotuning format combination: Float(1:4,36864,192,1) -> Float(1:4,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: PWN(input11.1) (PointWiseV2)
VERBOSE: Tactic: 0 Time: 9.21882
VERBOSE: Tactic: 1 Time: 10.8324
VERBOSE: Tactic: 2 Time: 13.0118
VERBOSE: Tactic: 3 Time: 12.7472
VERBOSE: Tactic: 4 Time: 18.1396
VERBOSE: Tactic: 5 Time: 17.9712
VERBOSE: Tactic: 6 Time: 16.6401
VERBOSE: Tactic: 7 Time: 23.8709
VERBOSE: Tactic: 8 Time: 23.0674
VERBOSE: Tactic: 9 Time: 16.9795
VERBOSE: Tactic: 10 Time: 8.34208
VERBOSE: Tactic: 11 Time: 11.1104
VERBOSE: Tactic: 12 Time: 9.79302
VERBOSE: Tactic: 13 Time: 14.2485
VERBOSE: Tactic: 14 Time: 13.2309
VERBOSE: Tactic: 15 Time: 10.9471
VERBOSE: Tactic: 16 Time: 20.6589
VERBOSE: Tactic: 17 Time: 17.6981
VERBOSE: Tactic: 18 Time: 15.1908
VERBOSE: Tactic: 19 Time: 13.7132
VERBOSE: Tactic: 20 Time: 7.2008
VERBOSE: Tactic: 21 Time: 9.67174
VERBOSE: Tactic: 22 Time: 13.0212
VERBOSE: Tactic: 23 Time: 19.9384
VERBOSE: Tactic: 28 Time: 0.61808
VERBOSE: Tactic: 29 Time: 0.608672
VERBOSE: Tactic: 30 Time: 0.97456
VERBOSE: Fastest Tactic: 29 Time: 0.608672
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 29
VERBOSE: =============== Computing costs for 
VERBOSE: *************** Autotuning format combination: Float(4718592,36864,192,1) -> Float(9584640,36864,192,1) ***************
VERBOSE: --------------- Timing Runner: max_pool2d.1 (TiledPooling)
VERBOSE: Tactic: 2752769 Time: 1.29923
VERBOSE: Tactic: 2818305 Time: 0.868288
VERBOSE: Tactic: 2883841 Time: 0.699296
VERBOSE: Tactic: 2949377 Time: 0.6
VERBOSE: Tactic: 3014913 Time: 0.694048
VERBOSE: Tactic: 3080449 Time: 0.502688
VERBOSE: Tactic: 3145985 Time: 0.528256
VERBOSE: Tactic: 3211521 Time: 1.3096
VERBOSE: Tactic: 3277057 Time: 0.877632
VERBOSE: Tactic: 3342593 Time: 0.713984
VERBOSE: Tactic: 3408129 Time: 0.391648
VERBOSE: Tactic: 3473665 Time: 0.431392
VERBOSE: Tactic: 3539201 Time: 0.428608
VERBOSE: Tactic: 3604737 Time: 0.429312
VERBOSE: Tactic: 3670273 Time: 1.30842
VERBOSE: Tactic: 3735809 Time: 0.868352
VERBOSE: Tactic: 3801345 Time: 0.71872
VERBOSE: Tactic: 3866881 Time: 0.341184
VERBOSE: Tactic: 3932417 Time: 0.360896
VERBOSE: Tactic: 3997953 Time: 0.426112
VERBOSE: Tactic: 4063489 Time: 0.42944
VERBOSE: Tactic: 4129025 Time: 1.3105
VERBOSE: Tactic: 4194561 Time: 0.872896
VERBOSE: Tactic: 4260097 Time: 0.719904
VERBOSE: Tactic: 4325633 Time: 0.322592
VERBOSE: Tactic: 4391169 Time: 0.329248
VERBOSE: Tactic: 4456705 Time: 0.429632
VERBOSE: Tactic: 4522241 Time: 0.432768
VERBOSE: Tactic: 4587777 Time: 1.31107
VERBOSE: Tactic: 4653313 Time: 0.865888
VERBOSE: Tactic: 4718849 Time: 0.719488
VERBOSE: Tactic: 4784385 Time: 0.320032
VERBOSE: Tactic: 4849921 Time: 0.314976
VERBOSE: Tactic: 4915457 Time: 0.428512
VERBOSE: Tactic: 4980993 Time: 0.43392
VERBOSE: Tactic: 5046529 Time: 1.30576
VERBOSE: Tactic: 5112065 Time: 0.870496
VERBOSE: Tactic: 5177601 Time: 0.720128
VERBOSE: Tactic: 5243137 Time: 0.318944
VERBOSE: Tactic: 5308673 Time: 0.311072
VERBOSE: Tactic: 5374209 Time: 0.431776
VERBOSE: Tactic: 5439745 Time: 0.440864
VERBOSE: Tactic: 6553857 Time: 0.200864
VERBOSE: Tactic: 6750465 Time: 0.26832
VERBOSE: Fastest Tactic: 6553857 Time: 0.200864
VERBOSE: --------------- Timing Runner: max_pool2d.1 (CudnnPooling)
VERBOSE: Tactic: -1 Time: 0.511008
VERBOSE: Fastest Tactic: -1 Time: 0.511008
VERBOSE: --------------- Timing Runner: max_pool2d.1 (CaskPooling)
VERBOSE: max_pool2d.1 Set Tactic Name: sm50_xmma_pooling_fw_4d_FP32FP32NCHW_Max Tactic: -5359392427276731246
VERBOSE: Tactic: -5359392427276731246 Time: 0.516
VERBOSE: Fastest Tactic: -5359392427276731246 Time: 0.516
VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: TiledPooling Tactic: 6553857
VERBOSE: Adding reformat layer: Reformatted Input Tensor 0 to self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 (quant_input.26TensorQ) from Int8(36864,9216:32,96,1) to Int8(294912,9216:4,96,1)
VERBOSE: Adding reformat layer: Reformatted Input Tensor 0 to self.base.level3.tree2.tree1.conv2.weight + quant_weight.36ChannelQ + input.59 + input3.1 + 494 (quant_input.36TensorQ) from Int8(36864,9216:32,96,1) to Int8(294912,9216:4,96,1)
VERBOSE: Adding reformat layer: Reformatted Input Tensor 0 to self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 (inputs.17) from Int8(36864,9216:32,96,1) to Int8(294912,9216:4,96,1)
VERBOSE: Adding reformat layer: Reformatted Input Tensor 0 to self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 (quant_input.50TensorQ) from Int8(18432,2304:32,48,1) to Int8(147456,2304:4,48,1)
VERBOSE: Adding reformat layer: Reformatted Input Tensor 0 to self.base.level4.tree2.tree1.conv2.weight + quant_weight.60ChannelQ + input.95 + input7.1 + 818 (quant_input.60TensorQ) from Int8(18432,2304:32,48,1) to Int8(147456,2304:4,48,1)
VERBOSE: Adding reformat layer: Reformatted Input Tensor 0 to self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 (quant_input.66TensorQ) from Int8(64512,2304:32,48,1) to Int8(516096,2304:4,48,1)
VERBOSE: Adding reformat layer: Reformatted Input Tensor 0 to self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 (quant_input.70TensorQ) from Int8(18432,2304:32,48,1) to Int8(147456,2304:4,48,1)
VERBOSE: Adding reformat layer: Reformatted Input Tensor 1 to self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 (base_level5_project_1_output_quant.1) from Float(294912,576,24,1) to Float(9216,576:32,24,1)
VERBOSE: Adding reformat layer: Reformatted Output Tensor 0 to self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 (993) from Float(9216,576:32,24,1) to Float(294912,576,24,1)
VERBOSE: Adding reformat layer: Reformatted Output Tensor 0 to self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 (1240) from Float(36864,9216:32,96,1) to Float(1179648,9216,96,1)
VERBOSE: Adding reformat layer: Reformatted Input Tensor 0 to dla_up_ida_0_up_1.1 (1105) from Float(4608,576:32,24,1) to Float(147456,576,24,1)
VERBOSE: Adding reformat layer: Reformatted Input Tensor 0 to self.dla_up.ida_1.proj_2.0.weight + quant_weight.86ChannelQ + input.137 (quant_input.86TensorQ) from Int8(18432,2304:32,48,1) to Int8(147456,2304:4,48,1)
VERBOSE: Adding reformat layer: Reformatted Input Tensor 0 to self.dla_up.ida_2.proj_3.0.weight + quant_weight.96ChannelQ + input.157 (quant_input.96TensorQ) from Int8(36864,9216:32,96,1) to Int8(294912,9216:4,96,1)
VERBOSE: Adding reformat layer: Reformatted Input Tensor 0 to self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 (quant_input.106TensorQ) from Int8(294912,36864:32,192,1) to Int8(2359296,36864:4,192,1)
VERBOSE: Adding reformat layer: Reformatted Output Tensor 0 to self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 (wh_2.1) from Float(331776,36864:32,192,1) to Float(9584640,36864,192,1)
VERBOSE: Adding reformat layer: Reformatted Output Tensor 0 to self.reg.2.weight + quant_weight.1ChannelQ + reg_2.1 (reg_2.1) from Float(331776,36864:32,192,1) to Float(9584640,36864,192,1)
VERBOSE: Formats and tactics selection completed in 23.2716 seconds.
VERBOSE: After reformat layers: 133 layers
VERBOSE: Pre-optimized block assignment.
VERBOSE: Block size 2359296
VERBOSE: Block size 9437184
VERBOSE: Block size 9437184
VERBOSE: Block size 4718592
VERBOSE: Block size 1179648
VERBOSE: Block size 2359296
VERBOSE: Block size 9437184
VERBOSE: Block size 2359296
VERBOSE: Block size 9437184
VERBOSE: Block size 2359296
VERBOSE: Block size 2359296
VERBOSE: Block size 4718592
VERBOSE: Block size 9437184
VERBOSE: Block size 2359296
VERBOSE: Block size 2359296
VERBOSE: Block size 589824
VERBOSE: Block size 1179648
VERBOSE: Block size 4718592
VERBOSE: Block size 1179648
VERBOSE: Block size 4718592
VERBOSE: Block size 1179648
VERBOSE: Block size 1179648
VERBOSE: Block size 2359296
VERBOSE: Block size 4718592
VERBOSE: Block size 1179648
VERBOSE: Block size 4718592
VERBOSE: Block size 1179648
VERBOSE: Block size 4718592
VERBOSE: Block size 1179648
VERBOSE: Block size 1179648
VERBOSE: Block size 4128768
VERBOSE: Block size 4718592
VERBOSE: Block size 1179648
VERBOSE: Block size 1179648
VERBOSE: Block size 294912
VERBOSE: Block size 589824
VERBOSE: Block size 2359296
VERBOSE: Block size 589824
VERBOSE: Block size 2359296
VERBOSE: Block size 589824
VERBOSE: Block size 589824
VERBOSE: Block size 1179648
VERBOSE: Block size 2359296
VERBOSE: Block size 589824
VERBOSE: Block size 2359296
VERBOSE: Block size 589824
VERBOSE: Block size 2359296
VERBOSE: Block size 589824
VERBOSE: Block size 589824
VERBOSE: Block size 2064384
VERBOSE: Block size 2359296
VERBOSE: Block size 589824
VERBOSE: Block size 147456
VERBOSE: Block size 294912
VERBOSE: Block size 1179648
VERBOSE: Block size 589824
VERBOSE: Block size 294912
VERBOSE: Block size 1179648
VERBOSE: Block size 294912
VERBOSE: Block size 294912
VERBOSE: Block size 737280
VERBOSE: Block size 294912
VERBOSE: Block size 589824
VERBOSE: Block size 2359296
VERBOSE: Block size 1179648
VERBOSE: Block size 1179648
VERBOSE: Block size 4718592
VERBOSE: Block size 589824
VERBOSE: Block size 1179648
VERBOSE: Block size 4718592
VERBOSE: Block size 2359296
VERBOSE: Block size 4718592
VERBOSE: Block size 2359296
VERBOSE: Block size 2359296
VERBOSE: Block size 9437184
VERBOSE: Block size 1179648
VERBOSE: Block size 2359296
VERBOSE: Block size 9437184
VERBOSE: Block size 1179648
VERBOSE: Block size 2359296
VERBOSE: Block size 9437184
VERBOSE: Block size 4718592
VERBOSE: Block size 4718592
VERBOSE: Block size 4718592
VERBOSE: Block size 2359296
VERBOSE: Block size 9437184
VERBOSE: Block size 18874368
VERBOSE: Block size 18874368
VERBOSE: Block size 9437184
VERBOSE: Block size 9437184
VERBOSE: Block size 2359296
VERBOSE: Block size 1179648
VERBOSE: Block size 1179648
VERBOSE: Block size 589824
VERBOSE: Block size 589824
VERBOSE: Block size 294912
VERBOSE: Block size 1179648
VERBOSE: Block size 1179648
VERBOSE: Block size 1179648
VERBOSE: Block size 589824
VERBOSE: Block size 589824
VERBOSE: Block size 2064384
VERBOSE: Block size 589824
VERBOSE: Block size 1179648
VERBOSE: Block size 1179648
VERBOSE: Block size 4718592
VERBOSE: Block size 589824
VERBOSE: Block size 589824
VERBOSE: Block size 1179648
VERBOSE: Block size 9437184
VERBOSE: Block size 4718592
VERBOSE: Block size 4718592
VERBOSE: Block size 4294967296
VERBOSE: Total Activation Memory: 4632936448
Detected 1 inputs and 5 output network tensors.
VERBOSE: self.base.base_layer.0.weight + quant_weight.2ChannelQ + input.5 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 8006952294591770973
VERBOSE: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -6282183216199417697
VERBOSE: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -6282183216199417697
VERBOSE: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 4581732244273465060
VERBOSE: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Set Tactic Name: volta_int8x4_icudnn_int8x4_128x64_relu_interior_c32_nn_v1 Tactic: 2339361327868109050
VERBOSE: self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: -9204333525109552344
VERBOSE: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: 1230105269624924765
VERBOSE: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 4871133328510103657
VERBOSE: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: -7924103240988931433
VERBOSE: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -1841683966837205309
VERBOSE: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 3541919052468401776
VERBOSE: self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: -9204333525109552344
VERBOSE: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -1841683966837205309
VERBOSE: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -1841683966837205309
VERBOSE: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 892787096507693407
VERBOSE: self.base.level3.tree2.tree1.conv1.weight + quant_weight.34ChannelQ + input.55 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -1841683966837205309
VERBOSE: self.base.level3.tree2.tree1.conv2.weight + quant_weight.36ChannelQ + input.59 + input3.1 + 494 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: -9204333525109552344
VERBOSE: self.base.level3.tree2.tree2.conv1.weight + quant_weight.38ChannelQ + input.61 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -1841683966837205309
VERBOSE: self.base.level3.tree2.tree2.conv2.weight + quant_weight.40ChannelQ + input.65 + input4.1 + 552 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -1841683966837205309
VERBOSE: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_interior_nn_v1 Tactic: -1765942417666394360
VERBOSE: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -8431788508843860955
VERBOSE: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: -7924103240988931433
VERBOSE: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 3927509214678622419
VERBOSE: self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -4973811344878172338
VERBOSE: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 4871133328510103657
VERBOSE: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -496455309852654971
VERBOSE: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -496455309852654971
VERBOSE: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_interior_nn_v1 Tactic: -1765942417666394360
VERBOSE: self.base.level4.tree2.tree1.conv1.weight + quant_weight.58ChannelQ + input.91 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -496455309852654971
VERBOSE: self.base.level4.tree2.tree1.conv2.weight + quant_weight.60ChannelQ + input.95 + input7.1 + 818 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -4973811344878172338
VERBOSE: self.base.level4.tree2.tree2.conv1.weight + quant_weight.62ChannelQ + input.97 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -496455309852654971
VERBOSE: self.base.level4.tree2.tree2.conv2.weight + quant_weight.64ChannelQ + input.101 + input8.1 + 876 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -496455309852654971
VERBOSE: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_interior_nn_v1 Tactic: -1765942417666394360
VERBOSE: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: -6371781333659293809
VERBOSE: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: -7924103240988931433
VERBOSE: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: -8859846367886814331
VERBOSE: self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 Set Tactic Name: turing_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 6096719469361499298
VERBOSE: self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 Set Tactic Name: volta_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -3784829056659735491
VERBOSE: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: -2328318099174473157
VERBOSE: self.dla_up.ida_2.proj_2.0.weight + quant_weight.94ChannelQ + input.153 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: -7924103240988931433
VERBOSE: self.dla_up.ida_2.node_2.0.weight + quant_weight.100ChannelQ + input.165 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 4871133328510103657
VERBOSE: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: -6371781333659293809
VERBOSE: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Set Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 3927509214678622419
VERBOSE: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 Set Tactic Name: turing_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -4831366370915083630
VERBOSE: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: -496455309852654971
VERBOSE: self.dla_up.ida_1.proj_2.0.weight + quant_weight.86ChannelQ + input.137 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: -7924103240988931433
VERBOSE: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Set Tactic Name: turing_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: -1841683966837205309
VERBOSE: self.dla_up.ida_2.proj_3.0.weight + quant_weight.96ChannelQ + input.157 Set Tactic Name: volta_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: -7924103240988931433
VERBOSE: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Set Tactic Name: turing_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: 4871133328510103657
VERBOSE: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -8431788508843860955
VERBOSE: self.wh.0.weight + quant_weight.108ChannelQ + input.175 + 1512 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -8431788508843860955
VERBOSE: self.reg.0.weight + quant_weight.112ChannelQ + input.1 + 1545 Set Tactic Name: turing_int8_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -8431788508843860955
VERBOSE: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 Set Tactic Name: volta_fp32_icudnn_int8x4_128x128_relu_interior_nn_v1 Tactic: -1765942417666394360
VERBOSE: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: -7185527339793611699
VERBOSE: self.reg.2.weight + quant_weight.1ChannelQ + reg_2.1 Set Tactic Name: volta_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: -7185527339793611699
VERBOSE: Layer: quant_input.2TensorQ Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.base.base_layer.0.weight + quant_weight.2ChannelQ + input.5 Host Persistent: 2176 Device Persistent: 3539456 Scratch Memory: 0
VERBOSE: Layer: self.base.level0.0.weight + quant_weight.4ChannelQ + input.9 Host Persistent: 1664 Device Persistent: 3539456 Scratch Memory: 0
VERBOSE: Layer: self.base.level1.0.weight + quant_weight.6ChannelQ + input.13 Host Persistent: 1664 Device Persistent: 885248 Scratch Memory: 0
VERBOSE: Layer: inputs.5 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19 Host Persistent: 1664 Device Persistent: 222208 Scratch Memory: 0
VERBOSE: Layer: self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17 Host Persistent: 3200 Device Persistent: 222208 Scratch Memory: 0
VERBOSE: Layer: base_level2_project_1_output_quant.1 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170 Host Persistent: 1664 Device Persistent: 222208 Scratch Memory: 0
VERBOSE: Layer: base_level2_tree1_relu_output_quant.1TensorQ Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25 Host Persistent: 2176 Device Persistent: 259072 Scratch Memory: 0
VERBOSE: Layer: quant_input.18TensorQ_clone_1 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228 Host Persistent: 1664 Device Persistent: 259072 Scratch Memory: 0
VERBOSE: Layer: 228 copy Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31 Host Persistent: 3200 Device Persistent: 222208 Scratch Memory: 0
VERBOSE: Layer: quant_input.42TensorQ_clone_2 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: quant_input.22TensorQ Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: inputs.11 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39 Host Persistent: 2176 Device Persistent: 130560 Scratch Memory: 0
VERBOSE: Layer: self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37 Host Persistent: 2240 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: base_level3_tree1_project_1_output_quant.1 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: Reformatting CopyNode for Input Tensor 0 to self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350 Host Persistent: 1664 Device Persistent: 56832 Scratch Memory: 0
VERBOSE: Layer: base_level3_tree1_tree1_relu_output_quant.1TensorQ Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45 Host Persistent: 2176 Device Persistent: 204288 Scratch Memory: 0
VERBOSE: Layer: quant_input.32TensorQ_clone_1 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408 Host Persistent: 2176 Device Persistent: 204288 Scratch Memory: 0
VERBOSE: Layer: 408 copy Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51 Host Persistent: 2176 Device Persistent: 56832 Scratch Memory: 0
VERBOSE: Layer: base_level3_tree1_root_relu_output_quant.1TensorQ Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: base_level3_tree1_root_relu_output_quant.1 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.base.level3.tree2.tree1.conv1.weight + quant_weight.34ChannelQ + input.55 Host Persistent: 2176 Device Persistent: 204288 Scratch Memory: 0
VERBOSE: Layer: Reformatting CopyNode for Input Tensor 0 to self.base.level3.tree2.tree1.conv2.weight + quant_weight.36ChannelQ + input.59 + input3.1 + 494 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.base.level3.tree2.tree1.conv2.weight + quant_weight.36ChannelQ + input.59 + input3.1 + 494 Host Persistent: 1664 Device Persistent: 56832 Scratch Memory: 0
VERBOSE: Layer: base_level3_tree2_tree1_relu_output_quant.1TensorQ Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.base.level3.tree2.tree2.conv1.weight + quant_weight.38ChannelQ + input.61 Host Persistent: 2176 Device Persistent: 204288 Scratch Memory: 0
VERBOSE: Layer: quant_input.42TensorQ_clone_3 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: inputs.9 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: quant_input.42TensorQ_clone_1 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.base.level3.tree2.tree2.conv2.weight + quant_weight.40ChannelQ + input.65 + input4.1 + 552 Host Persistent: 2176 Device Persistent: 204288 Scratch Memory: 0
VERBOSE: Layer: 552 copy Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67 Host Persistent: 3200 Device Persistent: 56832 Scratch Memory: 0
VERBOSE: Layer: quant_input.66TensorQ_clone_2 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: quant_input.46TensorQ Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: inputs.17 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75 Host Persistent: 1664 Device Persistent: 311296 Scratch Memory: 0
VERBOSE: Layer: Reformatting CopyNode for Input Tensor 0 to self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149 Host Persistent: 3200 Device Persistent: 56320 Scratch Memory: 0
VERBOSE: Layer: self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73 Host Persistent: 2240 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: dla_up_ida_2_up_1.1 Host Persistent: 24 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: base_level4_tree1_project_1_output_quant.1 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: quant_input.98TensorQ_clone_1 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: quant_input.98TensorQ_clone_0 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: Reformatting CopyNode for Input Tensor 0 to self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674 Host Persistent: 1664 Device Persistent: 16384 Scratch Memory: 0
VERBOSE: Layer: self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161 Host Persistent: 1664 Device Persistent: 295936 Scratch Memory: 0
VERBOSE: Layer: base_level4_tree1_tree1_relu_output_quant.1TensorQ Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81 Host Persistent: 2176 Device Persistent: 606208 Scratch Memory: 0
VERBOSE: Layer: quant_input.56TensorQ_clone_1 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732 Host Persistent: 2176 Device Persistent: 606208 Scratch Memory: 0
VERBOSE: Layer: 732 copy Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87 Host Persistent: 3200 Device Persistent: 16384 Scratch Memory: 0
VERBOSE: Layer: base_level4_tree1_root_relu_output_quant.1TensorQ Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: base_level4_tree1_root_relu_output_quant.1 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.base.level4.tree2.tree1.conv1.weight + quant_weight.58ChannelQ + input.91 Host Persistent: 2176 Device Persistent: 606208 Scratch Memory: 0
VERBOSE: Layer: Reformatting CopyNode for Input Tensor 0 to self.base.level4.tree2.tree1.conv2.weight + quant_weight.60ChannelQ + input.95 + input7.1 + 818 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.base.level4.tree2.tree1.conv2.weight + quant_weight.60ChannelQ + input.95 + input7.1 + 818 Host Persistent: 1664 Device Persistent: 16384 Scratch Memory: 0
VERBOSE: Layer: base_level4_tree2_tree1_relu_output_quant.1TensorQ Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.base.level4.tree2.tree2.conv1.weight + quant_weight.62ChannelQ + input.97 Host Persistent: 2176 Device Persistent: 606208 Scratch Memory: 0
VERBOSE: Layer: quant_input.66TensorQ_clone_3 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: inputs.15 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: quant_input.66TensorQ_clone_1 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.base.level4.tree2.tree2.conv2.weight + quant_weight.64ChannelQ + input.101 + input8.1 + 876 Host Persistent: 2176 Device Persistent: 606208 Scratch Memory: 0
VERBOSE: Layer: 876 copy Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: Reformatting CopyNode for Input Tensor 0 to self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103 Host Persistent: 3200 Device Persistent: 16384 Scratch Memory: 0
VERBOSE: Layer: inputs.21 Host Persistent: 48 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: quant_input.70TensorQ Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: quant_input.68TensorQ Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109 Host Persistent: 2240 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: Reformatting CopyNode for Input Tensor 0 to self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133 Host Persistent: 3200 Device Persistent: 15360 Scratch Memory: 0
VERBOSE: Layer: self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107 Host Persistent: 2240 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: dla_up_ida_1_up_1.1 Host Persistent: 24 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: base_level5_project_1_output_quant.1 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: quant_input.88TensorQ_clone_1 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: quant_input.88TensorQ_clone_0 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: Reformatting CopyNode for Input Tensor 1 to self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 Host Persistent: 1664 Device Persistent: 2366976 Scratch Memory: 0
VERBOSE: Layer: Reformatting CopyNode for Output Tensor 0 to self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 Host Persistent: 1664 Device Persistent: 351744 Scratch Memory: 0
VERBOSE: Layer: Reformatting CopyNode for Output Tensor 0 to self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: base_level5_tree1_relu_output_quant.1TensorQ Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: quant_input.94TensorQ Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115 Host Persistent: 2240 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.dla_up.ida_2.proj_2.0.weight + quant_weight.94ChannelQ + input.153 Host Persistent: 3200 Device Persistent: 56320 Scratch Memory: 0
VERBOSE: Layer: dla_up_ida_2_up_2.1 Host Persistent: 24 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: quant_input.100TensorQ_clone_1 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.dla_up.ida_2.node_2.0.weight + quant_weight.100ChannelQ + input.165 Host Persistent: 1664 Device Persistent: 295936 Scratch Memory: 0
VERBOSE: Layer: quant_input.78TensorQ_clone_2 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: quant_input.78TensorQ_clone_1 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051 Host Persistent: 2240 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: 1051 copy Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121 Host Persistent: 2240 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125 Host Persistent: 1664 Device Persistent: 136704 Scratch Memory: 0
VERBOSE: Layer: Reformatting CopyNode for Input Tensor 0 to dla_up_ida_0_up_1.1 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: dla_up_ida_0_up_1.1 Host Persistent: 24 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: quant_input.82TensorQ_clone_1 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: quant_input.82TensorQ_clone_0 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129 Host Persistent: 2176 Device Persistent: 1196032 Scratch Memory: 0
VERBOSE: Layer: Reformatting CopyNode for Input Tensor 0 to self.dla_up.ida_1.proj_2.0.weight + quant_weight.86ChannelQ + input.137 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.dla_up.ida_1.proj_2.0.weight + quant_weight.86ChannelQ + input.137 Host Persistent: 3200 Device Persistent: 15360 Scratch Memory: 0
VERBOSE: Layer: dla_up_ida_1_up_2.1 Host Persistent: 24 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: quant_input.90TensorQ_clone_1 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: quant_input.90TensorQ_clone_0 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145 Host Persistent: 2176 Device Persistent: 351744 Scratch Memory: 0
VERBOSE: Layer: Reformatting CopyNode for Input Tensor 0 to self.dla_up.ida_2.proj_3.0.weight + quant_weight.96ChannelQ + input.157 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.dla_up.ida_2.proj_3.0.weight + quant_weight.96ChannelQ + input.157 Host Persistent: 3200 Device Persistent: 56320 Scratch Memory: 0
VERBOSE: Layer: dla_up_ida_2_up_3.1 Host Persistent: 24 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: quant_input.102TensorQ_clone_1 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169 Host Persistent: 1664 Device Persistent: 295936 Scratch Memory: 0
VERBOSE: Layer: self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474 Host Persistent: 1664 Device Persistent: 371200 Scratch Memory: 0
VERBOSE: Layer: self.wh.0.weight + quant_weight.108ChannelQ + input.175 + 1512 Host Persistent: 1664 Device Persistent: 371200 Scratch Memory: 0
VERBOSE: Layer: self.reg.0.weight + quant_weight.112ChannelQ + input.1 + 1545 Host Persistent: 1664 Device Persistent: 371200 Scratch Memory: 0
VERBOSE: Layer: Reformatting CopyNode for Input Tensor 0 to self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1 Host Persistent: 3200 Device Persistent: 222720 Scratch Memory: 0
VERBOSE: Layer: self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 Host Persistent: 2176 Device Persistent: 230400 Scratch Memory: 0
VERBOSE: Layer: Reformatting CopyNode for Output Tensor 0 to self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: self.reg.2.weight + quant_weight.1ChannelQ + reg_2.1 Host Persistent: 2176 Device Persistent: 230400 Scratch Memory: 0
VERBOSE: Layer: Reformatting CopyNode for Output Tensor 0 to self.reg.2.weight + quant_weight.1ChannelQ + reg_2.1 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: PWN(input11.1) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: max_pool2d.1 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
VERBOSE: Layer: input11.1 copy Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0
Total Host Persistent Memory: 122160
Total Device Persistent Memory: 21446144
Total Scratch Memory: 0
[MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 87 MiB, GPU 188 MiB
[BlockAssignment] Algorithm ShiftNTopDown took 19.6116ms to assign 10 blocks to 112 nodes requiring 65912832 bytes.
VERBOSE: Optimized block assignment.
VERBOSE: Block size 18874368
VERBOSE: Block size 18874368
VERBOSE: Block size 9437184
VERBOSE: Block size 9437184
VERBOSE: Block size 4718592
VERBOSE: Block size 2359296
VERBOSE: Block size 1179648
VERBOSE: Block size 589824
VERBOSE: Block size 294912
VERBOSE: Block size 147456
Total Activation Memory: 65912832
VERBOSE: Disabling unused tactic source: CUBLAS, CUBLASLT
VERBOSE: Using cuDNN as a tactic source
[MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 344, GPU 2358 (MiB)
WARNING: TensorRT was linked against cuDNN 8.3.2 but loaded cuDNN 8.2.4
VERBOSE: Engine generation completed in 24.21 seconds.
VERBOSE: Deleting timing cache: 428 entries, served 489 hits since creation.
VERBOSE: Engine Layer Information:
Layer(Scale): quant_input.2TensorQ, Tactic: 0, x.1[Float(1,3,768,768)] -> quant_input.2TensorQ[Int8(1,3,768,768)]
Layer(CaskConvolution): self.base.base_layer.0.weight + quant_weight.2ChannelQ + input.5, Tactic: 8006952294591770973, quant_input.2TensorQ[Int8(1,3,768,768)] -> quant_input.4TensorQ[Int8(1,16,768,768)]
Layer(CaskConvolution): self.base.level0.0.weight + quant_weight.4ChannelQ + input.9, Tactic: -6282183216199417697, quant_input.4TensorQ[Int8(1,16,768,768)] -> quant_input.6TensorQ[Int8(1,16,768,768)]
Layer(CaskConvolution): self.base.level1.0.weight + quant_weight.6ChannelQ + input.13, Tactic: -6282183216199417697, quant_input.6TensorQ[Int8(1,16,768,768)] -> inputs.5[Int8(1,32,384,384)]
Layer(CudaPooling): inputs.5, Tactic: -3, inputs.5[Int8(1,32,384,384)] -> quant_input.8TensorQ[Int8(1,32,192,192)]
Layer(CaskConvolution): self.base.level2.tree1.conv1.weight + quant_weight.10ChannelQ + input.19, Tactic: 4581732244273465060, inputs.5[Int8(1,32,384,384)] -> quant_input.12TensorQ[Int8(1,64,192,192)]
Layer(CaskConvolution): self.base.level2.project.0.weight + quant_weight.8ChannelQ + input.17, Tactic: 2339361327868109050, quant_input.8TensorQ[Int8(1,32,192,192)] -> base_level2_project_1_output_quant.1TensorQ[Int8(1,64,192,192)]
Layer(Scale): base_level2_project_1_output_quant.1, Tactic: 0, base_level2_project_1_output_quant.1TensorQ[Int8(1,64,192,192)] -> base_level2_project_1_output_quant.1[Float(1,64,192,192)]
Layer(CaskConvolution): self.base.level2.tree1.conv2.weight + quant_weight.12ChannelQ + input.23 + input.3 + 170, Tactic: -9204333525109552344, quant_input.12TensorQ[Int8(1,64,192,192)], base_level2_project_1_output_quant.1[Float(1,64,192,192)] -> 170[Float(1,64,192,192)]
Layer(Scale): base_level2_tree1_relu_output_quant.1TensorQ, Tactic: 0, 170[Float(1,64,192,192)] -> base_level2_tree1_relu_output_quant.1TensorQ[Int8(1,64,192,192)]
Layer(CaskConvolution): self.base.level2.tree2.conv1.weight + quant_weight.14ChannelQ + input.25, Tactic: 1230105269624924765, base_level2_tree1_relu_output_quant.1TensorQ[Int8(1,64,192,192)] -> quant_input.16TensorQ[Int8(1,64,192,192)]
Layer(Scale): quant_input.18TensorQ_clone_1, Tactic: 0, 170[Float(1,64,192,192)] -> quant_input.18TensorQ[Int8(1,64,192,192)]
Layer(CaskConvolution): self.base.level2.tree2.conv2.weight + quant_weight.16ChannelQ + input.29 + input0.1 + 228, Tactic: 4871133328510103657, quant_input.16TensorQ[Int8(1,64,192,192)], base_level2_tree1_relu_output_quant.1TensorQ[Int8(1,64,192,192)] -> 228_clone_0[Int8(1,64,192,192)]
Layer(Reformat): 228 copy, Tactic: 0, 228_clone_0[Int8(1,64,192,192)] -> quant_input.18TensorQ[Int8(1,64,192,192)]
Layer(CaskConvolution): self.base.level2.root.conv.weight + quant_weight.18ChannelQ + input.31, Tactic: -7924103240988931433, quant_input.18TensorQ[Int8(1,128,192,192)] -> 256[Float(1,64,192,192)]
Layer(Scale): quant_input.42TensorQ_clone_2, Tactic: 0, 256[Float(1,64,192,192)] -> inputs.9[Int8(1,64,192,192)]
Layer(Scale): quant_input.22TensorQ, Tactic: 0, 256[Float(1,64,192,192)] -> inputs.11[Int8(1,64,192,192)]
Layer(CudaPooling): inputs.11, Tactic: -4, inputs.11[Int8(1,64,192,192)] -> quant_input.22TensorQ[Int8(1,64,96,96)]
Layer(CaskConvolution): self.base.level3.tree1.tree1.conv1.weight + quant_weight.24ChannelQ + input.39, Tactic: -1841683966837205309, inputs.11[Int8(1,64,192,192)] -> quant_input.26TensorQ[Int8(1,128,96,96)]
Layer(CaskConvolution): self.base.level3.tree1.project.0.weight + quant_weight.22ChannelQ + input.37, Tactic: 3541919052468401776, quant_input.22TensorQ[Int8(1,64,96,96)] -> base_level3_tree1_project_1_output_quant.1TensorQ[Int8(1,128,96,96)]
Layer(Scale): base_level3_tree1_project_1_output_quant.1, Tactic: 0, base_level3_tree1_project_1_output_quant.1TensorQ[Int8(1,128,96,96)] -> base_level3_tree1_project_1_output_quant.1[Float(1,128,96,96)]
Layer(Reformat): Reformatting CopyNode for Input Tensor 0 to self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350, Tactic: 0, quant_input.26TensorQ[Int8(1,128,96,96)] -> Reformatted Input Tensor 0 to self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350[Int8(1,128,96,96)]
Layer(CaskConvolution): self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350, Tactic: -9204333525109552344, Reformatted Input Tensor 0 to self.base.level3.tree1.tree1.conv2.weight + quant_weight.26ChannelQ + input.43 + input1.1 + 350[Int8(1,128,96,96)], base_level3_tree1_project_1_output_quant.1[Float(1,128,96,96)] -> 350[Float(1,128,96,96)]
Layer(Scale): base_level3_tree1_tree1_relu_output_quant.1TensorQ, Tactic: 0, 350[Float(1,128,96,96)] -> base_level3_tree1_tree1_relu_output_quant.1TensorQ[Int8(1,128,96,96)]
Layer(CaskConvolution): self.base.level3.tree1.tree2.conv1.weight + quant_weight.28ChannelQ + input.45, Tactic: -1841683966837205309, base_level3_tree1_tree1_relu_output_quant.1TensorQ[Int8(1,128,96,96)] -> quant_input.30TensorQ[Int8(1,128,96,96)]
Layer(Scale): quant_input.32TensorQ_clone_1, Tactic: 0, 350[Float(1,128,96,96)] -> quant_input.32TensorQ[Int8(1,128,96,96)]
Layer(CaskConvolution): self.base.level3.tree1.tree2.conv2.weight + quant_weight.30ChannelQ + input.49 + input2.1 + 408, Tactic: -1841683966837205309, quant_input.30TensorQ[Int8(1,128,96,96)], base_level3_tree1_tree1_relu_output_quant.1TensorQ[Int8(1,128,96,96)] -> 408_clone_0[Int8(1,128,96,96)]
Layer(Reformat): 408 copy, Tactic: 0, 408_clone_0[Int8(1,128,96,96)] -> quant_input.32TensorQ[Int8(1,128,96,96)]
Layer(CaskConvolution): self.base.level3.tree1.root.conv.weight + quant_weight.32ChannelQ + input.51, Tactic: 892787096507693407, quant_input.32TensorQ[Int8(1,256,96,96)] -> 436[Float(1,128,96,96)]
Layer(Scale): base_level3_tree1_root_relu_output_quant.1TensorQ, Tactic: 0, 436[Float(1,128,96,96)] -> base_level3_tree1_root_relu_output_quant.1TensorQ[Int8(1,128,96,96)]
Layer(Scale): base_level3_tree1_root_relu_output_quant.1, Tactic: 0, base_level3_tree1_root_relu_output_quant.1TensorQ[Int8(1,128,96,96)] -> base_level3_tree1_root_relu_output_quant.1[Float(1,128,96,96)]
Layer(CaskConvolution): self.base.level3.tree2.tree1.conv1.weight + quant_weight.34ChannelQ + input.55, Tactic: -1841683966837205309, base_level3_tree1_root_relu_output_quant.1TensorQ[Int8(1,128,96,96)] -> quant_input.36TensorQ[Int8(1,128,96,96)]
Layer(Reformat): Reformatting CopyNode for Input Tensor 0 to self.base.level3.tree2.tree1.conv2.weight + quant_weight.36ChannelQ + input.59 + input3.1 + 494, Tactic: 0, quant_input.36TensorQ[Int8(1,128,96,96)] -> Reformatted Input Tensor 0 to self.base.level3.tree2.tree1.conv2.weight + quant_weight.36ChannelQ + input.59 + input3.1 + 494[Int8(1,128,96,96)]
Layer(CaskConvolution): self.base.level3.tree2.tree1.conv2.weight + quant_weight.36ChannelQ + input.59 + input3.1 + 494, Tactic: -9204333525109552344, Reformatted Input Tensor 0 to self.base.level3.tree2.tree1.conv2.weight + quant_weight.36ChannelQ + input.59 + input3.1 + 494[Int8(1,128,96,96)], base_level3_tree1_root_relu_output_quant.1[Float(1,128,96,96)] -> 494[Float(1,128,96,96)]
Layer(Scale): base_level3_tree2_tree1_relu_output_quant.1TensorQ, Tactic: 0, 494[Float(1,128,96,96)] -> base_level3_tree2_tree1_relu_output_quant.1TensorQ[Int8(1,128,96,96)]
Layer(CaskConvolution): self.base.level3.tree2.tree2.conv1.weight + quant_weight.38ChannelQ + input.61, Tactic: -1841683966837205309, base_level3_tree2_tree1_relu_output_quant.1TensorQ[Int8(1,128,96,96)] -> quant_input.40TensorQ[Int8(1,128,96,96)]
Layer(Scale): quant_input.42TensorQ_clone_3, Tactic: 0, 436[Float(1,128,96,96)] -> quant_input.42TensorQ[Int8(1,128,96,96)]
Layer(CudaPooling): inputs.9, Tactic: -3, inputs.9[Int8(1,64,192,192)] -> quant_input.42TensorQ[Int8(1,64,96,96)]
Layer(Scale): quant_input.42TensorQ_clone_1, Tactic: 0, 494[Float(1,128,96,96)] -> quant_input.42TensorQ[Int8(1,128,96,96)]
Layer(CaskConvolution): self.base.level3.tree2.tree2.conv2.weight + quant_weight.40ChannelQ + input.65 + input4.1 + 552, Tactic: -1841683966837205309, quant_input.40TensorQ[Int8(1,128,96,96)], base_level3_tree2_tree1_relu_output_quant.1TensorQ[Int8(1,128,96,96)] -> 552_clone_0[Int8(1,128,96,96)]
Layer(Reformat): 552 copy, Tactic: 0, 552_clone_0[Int8(1,128,96,96)] -> quant_input.42TensorQ[Int8(1,128,96,96)]
Layer(CaskConvolution): self.base.level3.tree2.root.conv.weight + quant_weight.42ChannelQ + input.67, Tactic: -1765942417666394360, quant_input.42TensorQ[Int8(1,448,96,96)] -> 580[Float(1,128,96,96)]
Layer(Scale): quant_input.66TensorQ_clone_2, Tactic: 0, 580[Float(1,128,96,96)] -> inputs.15[Int8(1,128,96,96)]
Layer(Scale): quant_input.46TensorQ, Tactic: 0, 580[Float(1,128,96,96)] -> inputs.17[Int8(1,128,96,96)]
Layer(CudaPooling): inputs.17, Tactic: -4, inputs.17[Int8(1,128,96,96)] -> quant_input.46TensorQ[Int8(1,128,48,48)]
Layer(CaskConvolution): self.base.level4.tree1.tree1.conv1.weight + quant_weight.48ChannelQ + input.75, Tactic: -8431788508843860955, inputs.17[Int8(1,128,96,96)] -> quant_input.50TensorQ[Int8(1,256,48,48)]
Layer(Reformat): Reformatting CopyNode for Input Tensor 0 to self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149, Tactic: 0, inputs.17[Int8(1,128,96,96)] -> Reformatted Input Tensor 0 to self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149[Int8(1,128,96,96)]
Layer(CaskConvolution): self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149, Tactic: -7924103240988931433, Reformatted Input Tensor 0 to self.dla_up.ida_2.proj_1.0.weight + quant_weight.92ChannelQ + input.149[Int8(1,128,96,96)] -> 1294[Float(1,64,96,96)]
Layer(CaskConvolution): self.base.level4.tree1.project.0.weight + quant_weight.46ChannelQ + input.73, Tactic: 3927509214678622419, quant_input.46TensorQ[Int8(1,128,48,48)] -> base_level4_tree1_project_1_output_quant.1TensorQ[Int8(1,256,48,48)]
Layer(CudnnDeconvolution): dla_up_ida_2_up_1.1, Tactic: 0, 1294[Float(1,64,96,96)] -> dla_up_ida_2_up_1.1[Float(1,64,192,192)]
Layer(Scale): base_level4_tree1_project_1_output_quant.1, Tactic: 0, base_level4_tree1_project_1_output_quant.1TensorQ[Int8(1,256,48,48)] -> base_level4_tree1_project_1_output_quant.1[Float(1,256,48,48)]
Layer(Scale): quant_input.98TensorQ_clone_1, Tactic: 0, dla_up_ida_2_up_1.1[Float(1,64,192,192)] -> quant_input.98TensorQ[Int8(1,64,192,192)]
Layer(Scale): quant_input.98TensorQ_clone_0, Tactic: 0, 256[Float(1,64,192,192)] -> quant_input.98TensorQ[Int8(1,64,192,192)]
Layer(Reformat): Reformatting CopyNode for Input Tensor 0 to self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674, Tactic: 0, quant_input.50TensorQ[Int8(1,256,48,48)] -> Reformatted Input Tensor 0 to self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674[Int8(1,256,48,48)]
Layer(CaskConvolution): self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674, Tactic: -4973811344878172338, Reformatted Input Tensor 0 to self.base.level4.tree1.tree1.conv2.weight + quant_weight.50ChannelQ + input.79 + input5.1 + 674[Int8(1,256,48,48)], base_level4_tree1_project_1_output_quant.1[Float(1,256,48,48)] -> 674[Float(1,256,48,48)]
Layer(CaskConvolution): self.dla_up.ida_2.node_1.0.weight + quant_weight.98ChannelQ + input.161, Tactic: 4871133328510103657, quant_input.98TensorQ[Int8(1,128,192,192)] -> quant_input.100TensorQ[Int8(1,64,192,192)]
Layer(Scale): base_level4_tree1_tree1_relu_output_quant.1TensorQ, Tactic: 0, 674[Float(1,256,48,48)] -> base_level4_tree1_tree1_relu_output_quant.1TensorQ[Int8(1,256,48,48)]
Layer(CaskConvolution): self.base.level4.tree1.tree2.conv1.weight + quant_weight.52ChannelQ + input.81, Tactic: -496455309852654971, base_level4_tree1_tree1_relu_output_quant.1TensorQ[Int8(1,256,48,48)] -> quant_input.54TensorQ[Int8(1,256,48,48)]
Layer(Scale): quant_input.56TensorQ_clone_1, Tactic: 0, 674[Float(1,256,48,48)] -> quant_input.56TensorQ[Int8(1,256,48,48)]
Layer(CaskConvolution): self.base.level4.tree1.tree2.conv2.weight + quant_weight.54ChannelQ + input.85 + input6.1 + 732, Tactic: -496455309852654971, quant_input.54TensorQ[Int8(1,256,48,48)], base_level4_tree1_tree1_relu_output_quant.1TensorQ[Int8(1,256,48,48)] -> 732_clone_0[Int8(1,256,48,48)]
Layer(Reformat): 732 copy, Tactic: 0, 732_clone_0[Int8(1,256,48,48)] -> quant_input.56TensorQ[Int8(1,256,48,48)]
Layer(CaskConvolution): self.base.level4.tree1.root.conv.weight + quant_weight.56ChannelQ + input.87, Tactic: -1765942417666394360, quant_input.56TensorQ[Int8(1,512,48,48)] -> 760[Float(1,256,48,48)]
Layer(Scale): base_level4_tree1_root_relu_output_quant.1TensorQ, Tactic: 0, 760[Float(1,256,48,48)] -> base_level4_tree1_root_relu_output_quant.1TensorQ[Int8(1,256,48,48)]
Layer(Scale): base_level4_tree1_root_relu_output_quant.1, Tactic: 0, base_level4_tree1_root_relu_output_quant.1TensorQ[Int8(1,256,48,48)] -> base_level4_tree1_root_relu_output_quant.1[Float(1,256,48,48)]
Layer(CaskConvolution): self.base.level4.tree2.tree1.conv1.weight + quant_weight.58ChannelQ + input.91, Tactic: -496455309852654971, base_level4_tree1_root_relu_output_quant.1TensorQ[Int8(1,256,48,48)] -> quant_input.60TensorQ[Int8(1,256,48,48)]
Layer(Reformat): Reformatting CopyNode for Input Tensor 0 to self.base.level4.tree2.tree1.conv2.weight + quant_weight.60ChannelQ + input.95 + input7.1 + 818, Tactic: 0, quant_input.60TensorQ[Int8(1,256,48,48)] -> Reformatted Input Tensor 0 to self.base.level4.tree2.tree1.conv2.weight + quant_weight.60ChannelQ + input.95 + input7.1 + 818[Int8(1,256,48,48)]
Layer(CaskConvolution): self.base.level4.tree2.tree1.conv2.weight + quant_weight.60ChannelQ + input.95 + input7.1 + 818, Tactic: -4973811344878172338, Reformatted Input Tensor 0 to self.base.level4.tree2.tree1.conv2.weight + quant_weight.60ChannelQ + input.95 + input7.1 + 818[Int8(1,256,48,48)], base_level4_tree1_root_relu_output_quant.1[Float(1,256,48,48)] -> 818[Float(1,256,48,48)]
Layer(Scale): base_level4_tree2_tree1_relu_output_quant.1TensorQ, Tactic: 0, 818[Float(1,256,48,48)] -> base_level4_tree2_tree1_relu_output_quant.1TensorQ[Int8(1,256,48,48)]
Layer(CaskConvolution): self.base.level4.tree2.tree2.conv1.weight + quant_weight.62ChannelQ + input.97, Tactic: -496455309852654971, base_level4_tree2_tree1_relu_output_quant.1TensorQ[Int8(1,256,48,48)] -> quant_input.64TensorQ[Int8(1,256,48,48)]
Layer(Scale): quant_input.66TensorQ_clone_3, Tactic: 0, 760[Float(1,256,48,48)] -> quant_input.66TensorQ[Int8(1,256,48,48)]
Layer(CudaPooling): inputs.15, Tactic: -4, inputs.15[Int8(1,128,96,96)] -> quant_input.66TensorQ[Int8(1,128,48,48)]
Layer(Scale): quant_input.66TensorQ_clone_1, Tactic: 0, 818[Float(1,256,48,48)] -> quant_input.66TensorQ[Int8(1,256,48,48)]
Layer(CaskConvolution): self.base.level4.tree2.tree2.conv2.weight + quant_weight.64ChannelQ + input.101 + input8.1 + 876, Tactic: -496455309852654971, quant_input.64TensorQ[Int8(1,256,48,48)], base_level4_tree2_tree1_relu_output_quant.1TensorQ[Int8(1,256,48,48)] -> 876_clone_0[Int8(1,256,48,48)]
Layer(Reformat): 876 copy, Tactic: 0, 876_clone_0[Int8(1,256,48,48)] -> quant_input.66TensorQ[Int8(1,256,48,48)]
Layer(Reformat): Reformatting CopyNode for Input Tensor 0 to self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103, Tactic: 0, quant_input.66TensorQ[Int8(1,896,48,48)] -> Reformatted Input Tensor 0 to self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103[Int8(1,896,48,48)]
Layer(CaskConvolution): self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103, Tactic: -1765942417666394360, Reformatted Input Tensor 0 to self.base.level4.tree2.root.conv.weight + quant_weight.66ChannelQ + input.103[Int8(1,896,48,48)] -> 904[Float(1,256,48,48)]
Layer(CudnnPooling): inputs.21, Tactic: -1, 904[Float(1,256,48,48)] -> inputs.21[Float(1,256,24,24)]
Layer(Scale): quant_input.70TensorQ, Tactic: 0, 904[Float(1,256,48,48)] -> quant_input.70TensorQ[Int8(1,256,48,48)]
Layer(Scale): quant_input.68TensorQ, Tactic: 0, inputs.21[Float(1,256,24,24)] -> quant_input.68TensorQ[Int8(1,256,24,24)]
Layer(CaskConvolution): self.base.level5.tree1.conv1.weight + quant_weight.70ChannelQ + input.109, Tactic: -6371781333659293809, quant_input.70TensorQ[Int8(1,256,48,48)] -> quant_input.72TensorQ[Int8(1,512,24,24)]
Layer(Reformat): Reformatting CopyNode for Input Tensor 0 to self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133, Tactic: 0, quant_input.70TensorQ[Int8(1,256,48,48)] -> Reformatted Input Tensor 0 to self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133[Int8(1,256,48,48)]
Layer(CaskConvolution): self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133, Tactic: -7924103240988931433, Reformatted Input Tensor 0 to self.dla_up.ida_1.proj_1.0.weight + quant_weight.84ChannelQ + input.133[Int8(1,256,48,48)] -> 1168[Float(1,128,48,48)]
Layer(CaskConvolution): self.base.level5.project.0.weight + quant_weight.68ChannelQ + input.107, Tactic: -8859846367886814331, quant_input.68TensorQ[Int8(1,256,24,24)] -> base_level5_project_1_output_quant.1TensorQ[Int8(1,512,24,24)]
Layer(CudnnDeconvolution): dla_up_ida_1_up_1.1, Tactic: 0, 1168[Float(1,128,48,48)] -> dla_up_ida_1_up_1.1[Float(1,128,96,96)]
Layer(Scale): base_level5_project_1_output_quant.1, Tactic: 0, base_level5_project_1_output_quant.1TensorQ[Int8(1,512,24,24)] -> base_level5_project_1_output_quant.1[Float(1,512,24,24)]
Layer(Scale): quant_input.88TensorQ_clone_1, Tactic: 0, dla_up_ida_1_up_1.1[Float(1,128,96,96)] -> quant_input.88TensorQ[Int8(1,128,96,96)]
Layer(Scale): quant_input.88TensorQ_clone_0, Tactic: 0, 580[Float(1,128,96,96)] -> quant_input.88TensorQ[Int8(1,128,96,96)]
Layer(Reformat): Reformatting CopyNode for Input Tensor 1 to self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993, Tactic: 1002, base_level5_project_1_output_quant.1[Float(1,512,24,24)] -> Reformatted Input Tensor 1 to self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993[Float(1,512,24,24)]
Layer(CaskConvolution): self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993, Tactic: 6096719469361499298, quant_input.72TensorQ[Int8(1,512,24,24)], Reformatted Input Tensor 1 to self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993[Float(1,512,24,24)] -> Reformatted Output Tensor 0 to self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993[Float(1,512,24,24)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993, Tactic: 1002, Reformatted Output Tensor 0 to self.base.level5.tree1.conv2.weight + quant_weight.72ChannelQ + input.113 + input9.1 + 993[Float(1,512,24,24)] -> 993[Float(1,512,24,24)]
Layer(CaskConvolution): self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141, Tactic: -3784829056659735491, quant_input.88TensorQ[Int8(1,256,96,96)] -> Reformatted Output Tensor 0 to self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141[Float(1,128,96,96)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141, Tactic: 1002, Reformatted Output Tensor 0 to self.dla_up.ida_1.node_1.0.weight + quant_weight.88ChannelQ + input.141[Float(1,128,96,96)] -> 1240[Float(1,128,96,96)]
Layer(Scale): base_level5_tree1_relu_output_quant.1TensorQ, Tactic: 0, 993[Float(1,512,24,24)] -> base_level5_tree1_relu_output_quant.1TensorQ[Int8(1,512,24,24)]
Layer(Scale): quant_input.94TensorQ, Tactic: 0, 1240[Float(1,128,96,96)] -> quant_input.94TensorQ[Int8(1,128,96,96)]
Layer(CaskConvolution): self.base.level5.tree2.conv1.weight + quant_weight.74ChannelQ + input.115, Tactic: -2328318099174473157, base_level5_tree1_relu_output_quant.1TensorQ[Int8(1,512,24,24)] -> quant_input.76TensorQ[Int8(1,512,24,24)]
Layer(CaskConvolution): self.dla_up.ida_2.proj_2.0.weight + quant_weight.94ChannelQ + input.153, Tactic: -7924103240988931433, quant_input.94TensorQ[Int8(1,128,96,96)] -> 1329[Float(1,64,96,96)]
Layer(CudnnDeconvolution): dla_up_ida_2_up_2.1, Tactic: 0, 1329[Float(1,64,96,96)] -> dla_up_ida_2_up_2.1[Float(1,64,192,192)]
Layer(Scale): quant_input.100TensorQ_clone_1, Tactic: 0, dla_up_ida_2_up_2.1[Float(1,64,192,192)] -> quant_input.100TensorQ[Int8(1,64,192,192)]
Layer(CaskConvolution): self.dla_up.ida_2.node_2.0.weight + quant_weight.100ChannelQ + input.165, Tactic: 4871133328510103657, quant_input.100TensorQ[Int8(1,128,192,192)] -> quant_input.102TensorQ[Int8(1,64,192,192)]
Layer(Scale): quant_input.78TensorQ_clone_2, Tactic: 0, inputs.21[Float(1,256,24,24)] -> quant_input.78TensorQ[Int8(1,256,24,24)]
Layer(Scale): quant_input.78TensorQ_clone_1, Tactic: 0, 993[Float(1,512,24,24)] -> quant_input.78TensorQ[Int8(1,512,24,24)]
Layer(CaskConvolution): self.base.level5.tree2.conv2.weight + quant_weight.76ChannelQ + input.119 + input10.1 + 1051, Tactic: -6371781333659293809, quant_input.76TensorQ[Int8(1,512,24,24)], base_level5_tree1_relu_output_quant.1TensorQ[Int8(1,512,24,24)] -> 1051_clone_0[Int8(1,512,24,24)]
Layer(Reformat): 1051 copy, Tactic: 0, 1051_clone_0[Int8(1,512,24,24)] -> quant_input.78TensorQ[Int8(1,512,24,24)]
Layer(CaskConvolution): self.base.level5.root.conv.weight + quant_weight.78ChannelQ + input.121, Tactic: 3927509214678622419, quant_input.78TensorQ[Int8(1,1280,24,24)] -> quant_input.80TensorQ[Int8(1,512,24,24)]
Layer(CaskConvolution): self.dla_up.ida_0.proj_1.0.weight + quant_weight.80ChannelQ + input.125, Tactic: -4831366370915083630, quant_input.80TensorQ[Int8(1,512,24,24)] -> 1105[Float(1,256,24,24)]
Layer(Reformat): Reformatting CopyNode for Input Tensor 0 to dla_up_ida_0_up_1.1, Tactic: 0, 1105[Float(1,256,24,24)] -> Reformatted Input Tensor 0 to dla_up_ida_0_up_1.1[Float(1,256,24,24)]
Layer(CudnnDeconvolution): dla_up_ida_0_up_1.1, Tactic: 1, Reformatted Input Tensor 0 to dla_up_ida_0_up_1.1[Float(1,256,24,24)] -> dla_up_ida_0_up_1.1[Float(1,256,48,48)]
Layer(Scale): quant_input.82TensorQ_clone_1, Tactic: 0, dla_up_ida_0_up_1.1[Float(1,256,48,48)] -> quant_input.82TensorQ[Int8(1,256,48,48)]
Layer(Scale): quant_input.82TensorQ_clone_0, Tactic: 0, 904[Float(1,256,48,48)] -> quant_input.82TensorQ[Int8(1,256,48,48)]
Layer(CaskConvolution): self.dla_up.ida_0.node_1.0.weight + quant_weight.82ChannelQ + input.129, Tactic: -496455309852654971, quant_input.82TensorQ[Int8(1,512,48,48)] -> quant_input.86TensorQ[Int8(1,256,48,48)]
Layer(Reformat): Reformatting CopyNode for Input Tensor 0 to self.dla_up.ida_1.proj_2.0.weight + quant_weight.86ChannelQ + input.137, Tactic: 0, quant_input.86TensorQ[Int8(1,256,48,48)] -> Reformatted Input Tensor 0 to self.dla_up.ida_1.proj_2.0.weight + quant_weight.86ChannelQ + input.137[Int8(1,256,48,48)]
Layer(CaskConvolution): self.dla_up.ida_1.proj_2.0.weight + quant_weight.86ChannelQ + input.137, Tactic: -7924103240988931433, Reformatted Input Tensor 0 to self.dla_up.ida_1.proj_2.0.weight + quant_weight.86ChannelQ + input.137[Int8(1,256,48,48)] -> 1203[Float(1,128,48,48)]
Layer(CudnnDeconvolution): dla_up_ida_1_up_2.1, Tactic: 0, 1203[Float(1,128,48,48)] -> dla_up_ida_1_up_2.1[Float(1,128,96,96)]
Layer(Scale): quant_input.90TensorQ_clone_1, Tactic: 0, dla_up_ida_1_up_2.1[Float(1,128,96,96)] -> quant_input.90TensorQ[Int8(1,128,96,96)]
Layer(Scale): quant_input.90TensorQ_clone_0, Tactic: 0, 1240[Float(1,128,96,96)] -> quant_input.90TensorQ[Int8(1,128,96,96)]
Layer(CaskConvolution): self.dla_up.ida_1.node_2.0.weight + quant_weight.90ChannelQ + input.145, Tactic: -1841683966837205309, quant_input.90TensorQ[Int8(1,256,96,96)] -> quant_input.96TensorQ[Int8(1,128,96,96)]
Layer(Reformat): Reformatting CopyNode for Input Tensor 0 to self.dla_up.ida_2.proj_3.0.weight + quant_weight.96ChannelQ + input.157, Tactic: 0, quant_input.96TensorQ[Int8(1,128,96,96)] -> Reformatted Input Tensor 0 to self.dla_up.ida_2.proj_3.0.weight + quant_weight.96ChannelQ + input.157[Int8(1,128,96,96)]
Layer(CaskConvolution): self.dla_up.ida_2.proj_3.0.weight + quant_weight.96ChannelQ + input.157, Tactic: -7924103240988931433, Reformatted Input Tensor 0 to self.dla_up.ida_2.proj_3.0.weight + quant_weight.96ChannelQ + input.157[Int8(1,128,96,96)] -> 1364[Float(1,64,96,96)]
Layer(CudnnDeconvolution): dla_up_ida_2_up_3.1, Tactic: 0, 1364[Float(1,64,96,96)] -> dla_up_ida_2_up_3.1[Float(1,64,192,192)]
Layer(Scale): quant_input.102TensorQ_clone_1, Tactic: 0, dla_up_ida_2_up_3.1[Float(1,64,192,192)] -> quant_input.102TensorQ[Int8(1,64,192,192)]
Layer(CaskConvolution): self.dla_up.ida_2.node_3.0.weight + quant_weight.102ChannelQ + input.169, Tactic: 4871133328510103657, quant_input.102TensorQ[Int8(1,128,192,192)] -> quant_input.104TensorQ[Int8(1,64,192,192)]
Layer(CaskConvolution): self.hm.0.weight + quant_weight.104ChannelQ + input.173 + 1474, Tactic: -8431788508843860955, quant_input.104TensorQ[Int8(1,64,192,192)] -> quant_input.106TensorQ[Int8(1,256,192,192)]
Layer(CaskConvolution): self.wh.0.weight + quant_weight.108ChannelQ + input.175 + 1512, Tactic: -8431788508843860955, quant_input.104TensorQ[Int8(1,64,192,192)] -> quant_input.110TensorQ[Int8(1,256,192,192)]
Layer(CaskConvolution): self.reg.0.weight + quant_weight.112ChannelQ + input.1 + 1545, Tactic: -8431788508843860955, quant_input.104TensorQ[Int8(1,64,192,192)] -> quant_input.1TensorQ[Int8(1,256,192,192)]
Layer(Reformat): Reformatting CopyNode for Input Tensor 0 to self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1, Tactic: 0, quant_input.106TensorQ[Int8(1,256,192,192)] -> Reformatted Input Tensor 0 to self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1[Int8(1,256,192,192)]
Layer(CaskConvolution): self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1, Tactic: -1765942417666394360, Reformatted Input Tensor 0 to self.hm.2.weight + quant_weight.106ChannelQ + hm_2.1[Int8(1,256,192,192)] -> hm_2.1[Float(1,128,192,192)]
Layer(CaskConvolution): self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1, Tactic: -7185527339793611699, quant_input.110TensorQ[Int8(1,256,192,192)] -> Reformatted Output Tensor 0 to self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1[Float(1,2,192,192)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1, Tactic: 0, Reformatted Output Tensor 0 to self.wh.2.weight + quant_weight.110ChannelQ + wh_2.1[Float(1,2,192,192)] -> 1563[Float(1,2,192,192)]
Layer(CaskConvolution): self.reg.2.weight + quant_weight.1ChannelQ + reg_2.1, Tactic: -7185527339793611699, quant_input.1TensorQ[Int8(1,256,192,192)] -> Reformatted Output Tensor 0 to self.reg.2.weight + quant_weight.1ChannelQ + reg_2.1[Float(1,2,192,192)]
Layer(Reformat): Reformatting CopyNode for Output Tensor 0 to self.reg.2.weight + quant_weight.1ChannelQ + reg_2.1, Tactic: 0, Reformatted Output Tensor 0 to self.reg.2.weight + quant_weight.1ChannelQ + reg_2.1[Float(1,2,192,192)] -> 1563[Float(1,2,192,192)]
Layer(PointWiseV2): PWN(input11.1), Tactic: 2, hm_2.1[Float(1,128,192,192)] -> input11.1[Float(1,128,192,192)]
Layer(TiledPooling): max_pool2d.1, Tactic: 6553857, input11.1[Float(1,128,192,192)] -> 1563[Float(1,128,192,192)]
Layer(Reformat): input11.1 copy, Tactic: 0, input11.1[Float(1,128,192,192)] -> 1563[Float(1,128,192,192)]
[MemUsageChange] TensorRT-managed allocation in building engine: CPU +17, GPU +38, now: CPU 17, GPU 38 (MiB)
[MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 66, GPU 2286 (MiB)
Loaded engine size: 18 MiB
VERBOSE: Using cuDNN as a tactic source
[MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 67, GPU 2312 (MiB)
WARNING: TensorRT was linked against cuDNN 8.3.2 but loaded cuDNN 8.2.4
VERBOSE: Deserialization required 11780 microseconds.
[MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +38, now: CPU 17, GPU 76 (MiB)
VERBOSE: Using cuDNN as a tactic source
[MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 50, GPU 2334 (MiB)
WARNING: TensorRT was linked against cuDNN 8.3.2 but loaded cuDNN 8.2.4
VERBOSE: Total per-runner device persistent memory is 21446144
VERBOSE: Total per-runner host persistent memory is 122160
[MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +20, now: CPU 17, GPU 96 (MiB)
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] create instance success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertFromMat success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] Forward success! 
D/tnn: CheckResult [File /data/TNN-host/test/test.cc][Line 393] ConvertToMat success! 
I/tnn: Print [File /data/TNN-host/test/timer.cc][Line 60] model_20.pt - CUDA                            TNN Benchmark time cost: min =  6.907   ms  |  max =  7.094   ms  |  avg =  6.960   ms 
Unexpected Internal Error: [virtualMemoryBuffer.cpp::~StdVirtualMemoryBufferImpl::121] Error Code 1: Cuda Runtime (driver shutting down)
